{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Chunking Strategies\n",
    "\n",
    "## Welcome!\n",
    "In Basic RAG, we used simple character-based chunking. But chunking is one of the most impactful\n",
    "factors in RAG quality. Bad chunking = bad retrieval = bad answers!\n",
    "\n",
    "## What You Will Learn\n",
    "1. **Why chunking matters** - The impact on retrieval quality\n",
    "2. **Fixed-size chunking** - What we did before (baseline)\n",
    "3. **Semantic chunking** - Split based on meaning, not characters\n",
    "4. **Parent-Child chunking** - Small chunks for search, big chunks for context\n",
    "5. **Document-aware chunking** - Respect document structure (headers, sections)\n",
    "\n",
    "## The Problem with Basic Chunking\n",
    "```\n",
    "Basic chunking (500 chars) might split like this:\n",
    "\n",
    "Chunk 1: \"...LoRA (Low-Rank Adaptation) is a method for fine-tuning large\"\n",
    "Chunk 2: \"language models efficiently. It works by...\"\n",
    "\n",
    "The concept is split! Neither chunk has the complete information.\n",
    "```\n",
    "\n",
    "Let us learn better approaches!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment loaded!\n",
      "OpenAI API Key found: False\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "print(\"Environment loaded!\")\n",
    "print(f\"OpenAI API Key found: {'OPENAI_API_KEY' in os.environ}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install langchain-experimental sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Sample Document\n",
    "\n",
    "We will use the same PDF you have been working with for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lapde\\anaconda3\\envs\\MLNotebook\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8 pages\n",
      "Total characters: 15540\n",
      "\n",
      "First 500 characters:\n",
      "@genieincodebottle \n",
      "Instagram | GitHub | Medium | YouTube \n",
      "How to Be Better Than Most in GenAI \n",
      " \n",
      "Contents \n",
      " \n",
      "Core LLM Building Blocks ....................................................................................................................... 2 \n",
      "Advanced Architectures ......................................................................................................................... 3 \n",
      "Training & Tuning ............................................................................\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load our familiar PDF\n",
    "pdf_path = \"llm_fundamentals.pdf\"\n",
    "\n",
    "# Check if file exists, if not try alternate path\n",
    "if not os.path.exists(pdf_path):\n",
    "    pdf_path = \"../RAG/llm_fundamentals.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "documents = loader.load()\n",
    "\n",
    "# Combine all pages into one text for demonstration\n",
    "full_text = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "\n",
    "print(f\"Loaded {len(documents)} pages\")\n",
    "print(f\"Total characters: {len(full_text)}\")\n",
    "print(f\"\\nFirst 500 characters:\")\n",
    "print(full_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Strategy 1: Fixed-Size Chunking (Baseline)\n",
    "\n",
    "This is what we have been doing. Let us see its limitations.\n",
    "\n",
    "**How it works:**\n",
    "- Split every N characters\n",
    "- Add overlap to avoid cutting mid-sentence\n",
    "\n",
    "**Problem:**\n",
    "- Does not respect meaning or document structure\n",
    "- May split related concepts across chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed-Size Chunking Results:\n",
      "Number of chunks: 37\n",
      "Average chunk size: 419 chars\n",
      "\n",
      "================================================================================\n",
      "Sample chunks (notice how they might cut mid-concept):\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Chunk 1 (404 chars):\n",
      "@genieincodebottle \n",
      "Instagram | GitHub | Medium | YouTube \n",
      "How to Be Better Than Most in GenAI \n",
      " \n",
      "Contents \n",
      " \n",
      "Core LLM Building Blocks ....................................................................\n",
      "----------------------------------------\n",
      "\n",
      "Chunk 2 (452 chars):\n",
      "Training & Tuning .................................................................................................................................... 3 \n",
      "Generation Controls .............................\n",
      "----------------------------------------\n",
      "\n",
      "Chunk 3 (448 chars):\n",
      "Efficiency & Scaling ................................................................................................................................. 5 \n",
      "Data & Preprocessing ............................\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Basic fixed-size chunking (what we did before)\n",
    "basic_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,      # 500 characters per chunk\n",
    "    chunk_overlap=50,    # 50 character overlap\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # Try to split on paragraphs first\n",
    ")\n",
    "\n",
    "basic_chunks = basic_splitter.split_text(full_text)\n",
    "\n",
    "print(f\"Fixed-Size Chunking Results:\")\n",
    "print(f\"Number of chunks: {len(basic_chunks)}\")\n",
    "print(f\"Average chunk size: {sum(len(c) for c in basic_chunks) / len(basic_chunks):.0f} chars\")\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(f\"Sample chunks (notice how they might cut mid-concept):\")\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "\n",
    "for i, chunk in enumerate(basic_chunks[:3], 1):\n",
    "    print(f\"\\nChunk {i} ({len(chunk)} chars):\")\n",
    "    print(f\"{chunk[:200]}...\")\n",
    "    print(\"-\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Strategy 2: Semantic Chunking\n",
    "\n",
    "**The Idea:**\n",
    "Instead of splitting by character count, split when the MEANING changes!\n",
    "\n",
    "**How it works:**\n",
    "1. Split text into sentences\n",
    "2. Create embeddings for each sentence\n",
    "3. Compare embeddings of adjacent sentences\n",
    "4. When similarity drops significantly - that is a chunk boundary!\n",
    "\n",
    "**Think of it like:**\n",
    "Reading a book and noticing when the topic changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating semantic chunks (this may take a moment)...\n",
      "\n",
      "Semantic Chunking Results:\n",
      "Number of chunks: 54\n",
      "Average chunk size: 287 chars\n",
      "\n",
      "================================================================================\n",
      "Sample semantic chunks (notice how each chunk has a coherent topic):\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Chunk 1 (402 chars):\n",
      "@genieincodebottle \n",
      "Instagram | GitHub | Medium | YouTube \n",
      "How to Be Better Than Most in GenAI \n",
      " \n",
      "Contents \n",
      " \n",
      "Core LLM Building Blocks ....................................................................................................................... 2 \n",
      "Advanced Architectures ......................\n",
      "----------------------------------------\n",
      "\n",
      "Chunk 2 (153 chars):\n",
      "3 \n",
      "Training & Tuning ....................................................................................................................................\n",
      "----------------------------------------\n",
      "\n",
      "Chunk 3 (150 chars):\n",
      "3 \n",
      "Generation Controls ...............................................................................................................................\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Initialize embeddings for semantic comparison\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create semantic chunker\n",
    "# breakpoint_threshold_type options:\n",
    "#   - \"percentile\": Split when similarity drops below X percentile\n",
    "#   - \"standard_deviation\": Split when similarity drops by X std deviations\n",
    "#   - \"interquartile\": Split based on IQR\n",
    "\n",
    "semantic_splitter = SemanticChunker(\n",
    "    embeddings=embeddings,\n",
    "    breakpoint_threshold_type=\"percentile\",  # Most intuitive option\n",
    "    breakpoint_threshold_amount=70  # Split at 70th percentile of similarity drops\n",
    ")\n",
    "\n",
    "print(\"Creating semantic chunks (this may take a moment)...\")\n",
    "semantic_chunks = semantic_splitter.split_text(full_text)\n",
    "\n",
    "print(f\"\\nSemantic Chunking Results:\")\n",
    "print(f\"Number of chunks: {len(semantic_chunks)}\")\n",
    "print(f\"Average chunk size: {sum(len(c) for c in semantic_chunks) / len(semantic_chunks):.0f} chars\")\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(f\"Sample semantic chunks (notice how each chunk has a coherent topic):\")\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "\n",
    "for i, chunk in enumerate(semantic_chunks[:3], 1):\n",
    "    print(f\"\\nChunk {i} ({len(chunk)} chars):\")\n",
    "    print(f\"{chunk[:300]}...\" if len(chunk) > 300 else chunk)\n",
    "    print(\"-\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Strategy 3: Parent-Child Chunking\n",
    "\n",
    "**The Problem:**\n",
    "- Small chunks - Better search precision, but less context for LLM\n",
    "- Large chunks - More context, but harder to find relevant parts\n",
    "\n",
    "**The Solution: Have both!**\n",
    "- **Child chunks**: Small (used for searching)\n",
    "- **Parent chunks**: Large (sent to LLM for context)\n",
    "\n",
    "**How it works:**\n",
    "```\n",
    "1. User asks a question\n",
    "2. Search finds relevant CHILD chunk (small, precise)\n",
    "3. Return the PARENT chunk (large, full context) to LLM\n",
    "```\n",
    "\n",
    "**Analogy:**\n",
    "Like finding a specific sentence in a book (child), but reading the whole paragraph (parent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent chunks created: 16\n",
      "Average parent size: 1005 chars\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Step 1: Create PARENT chunks (larger, for context)\n",
    "parent_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,     # Large chunks\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "parent_chunks = parent_splitter.split_text(full_text)\n",
    "print(f\"Parent chunks created: {len(parent_chunks)}\")\n",
    "print(f\"Average parent size: {sum(len(c) for c in parent_chunks) / len(parent_chunks):.0f} chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Child chunks created: 69\n",
      "Average child size: 234 chars\n",
      "\n",
      "Ratio: ~4.3 children per parent\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Create CHILD chunks from each parent\n",
    "child_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,      # Small chunks for precise search\n",
    "    chunk_overlap=30\n",
    ")\n",
    "\n",
    "# Store parent-child relationships\n",
    "parent_child_map = {}  # child_chunk -> parent_chunk\n",
    "all_child_chunks = []\n",
    "\n",
    "for parent_idx, parent_chunk in enumerate(parent_chunks):\n",
    "    # Split this parent into children\n",
    "    children = child_splitter.split_text(parent_chunk)\n",
    "    \n",
    "    for child in children:\n",
    "        all_child_chunks.append(child)\n",
    "        # Map each child back to its parent\n",
    "        parent_child_map[child] = {\n",
    "            \"parent_idx\": parent_idx,\n",
    "            \"parent_text\": parent_chunk\n",
    "        }\n",
    "\n",
    "print(f\"\\nChild chunks created: {len(all_child_chunks)}\")\n",
    "print(f\"Average child size: {sum(len(c) for c in all_child_chunks) / len(all_child_chunks):.0f} chars\")\n",
    "print(f\"\\nRatio: ~{len(all_child_chunks) / len(parent_chunks):.1f} children per parent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Parent-Child Relationship:\n",
      "================================================================================\n",
      "\n",
      "CHILD CHUNK (what we SEARCH with):\n",
      "Length: 294 chars\n",
      "Content: Data & Preprocessing ............................................................................................................................. 6 \n",
      "Evaluation & Benchmarks ...................................................................................................................... 6\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "PARENT CHUNK (what we SEND TO LLM):\n",
      "Length: 1466 chars\n",
      "Content: @genieincodebottle \n",
      "Instagram | GitHub | Medium | YouTube \n",
      "How to Be Better Than Most in GenAI \n",
      " \n",
      "Contents \n",
      " \n",
      "Core LLM Building Blocks ....................................................................................................................... 2 \n",
      "Advanced Architectures ......................................................................................................................... 3 \n",
      "Training & Tuning ...............................................................................\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Demonstrate the parent-child relationship\n",
    "print(\"Example Parent-Child Relationship:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Pick a child chunk\n",
    "sample_child = all_child_chunks[5]\n",
    "sample_parent_info = parent_child_map[sample_child]\n",
    "\n",
    "print(f\"\\nCHILD CHUNK (what we SEARCH with):\")\n",
    "print(f\"Length: {len(sample_child)} chars\")\n",
    "print(f\"Content: {sample_child}\")\n",
    "print(f\"\\n\" + \"-\"*80)\n",
    "\n",
    "print(f\"\\nPARENT CHUNK (what we SEND TO LLM):\")\n",
    "print(f\"Length: {len(sample_parent_info['parent_text'])} chars\")\n",
    "print(f\"Content: {sample_parent_info['parent_text'][:500]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created with 69 child chunks\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Create a simple parent-child retriever\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Create documents with parent reference in metadata\n",
    "child_documents = []\n",
    "for i, child in enumerate(all_child_chunks):\n",
    "    parent_info = parent_child_map[child]\n",
    "    doc = Document(\n",
    "        page_content=child,\n",
    "        metadata={\n",
    "            \"child_id\": i,\n",
    "            \"parent_idx\": parent_info[\"parent_idx\"],\n",
    "            \"is_child\": True\n",
    "        }\n",
    "    )\n",
    "    child_documents.append(doc)\n",
    "\n",
    "# Create vector store with CHILD chunks (for searching)\n",
    "child_vectorstore = Chroma.from_documents(\n",
    "    documents=child_documents,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"child_chunks\"\n",
    ")\n",
    "\n",
    "print(f\"Vector store created with {len(child_documents)} child chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is LoRA?\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Result 1:\n",
      "Matched CHILD (search hit): 9. QLoRA → LoRA + quantization, enabling fine-tuning of huge models on modest hardware \n",
      "10. PEFT → F...\n",
      "\n",
      "Returned PARENT (full context): @genieincodebottle \n",
      "Advanced Architectures \n",
      "1. Diffusion Models → Generate images/video by learning to reverse noise process (DALL-E, \n",
      "Midjourney, Stable Diffusion) \n",
      "2. VAEs (Variational Autoencoders) → Probabilistic generative models with latent spaces \n",
      "3. GANs (Generative Adversarial Networks) → G...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Result 2:\n",
      "Matched CHILD (search hit): @genieincodebottle \n",
      "Instagram | GitHub | Medium | YouTube \n",
      "How to Be Better Than Most in GenAI \n",
      " \n",
      "Co...\n",
      "\n",
      "Returned PARENT (full context): @genieincodebottle \n",
      "Instagram | GitHub | Medium | YouTube \n",
      "How to Be Better Than Most in GenAI \n",
      " \n",
      "Contents \n",
      " \n",
      "Core LLM Building Blocks ....................................................................................................................... 2 \n",
      "Advanced Architectures ......................\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Result 3:\n",
      "Matched CHILD (search hit): behaviour \n",
      "4. Multi-Agent Systems → Teams of LLMs with specialized roles for complex tasks \n",
      "5. Tool ...\n",
      "\n",
      "Returned PARENT (full context): @genieincodebottle \n",
      "6. Consistency / Contradiction Metrics → Check if model outputs are logically consistent \n",
      "across queries \n",
      "7. Bias & Fairness Metrics → Quantify demographic or cultural biases in outputs \n",
      "8. Adversarial Robustness → Test resilience to malicious or tricky prompts \n",
      "9. Knowledge Prob...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def parent_child_retrieve(query: str, k: int = 3):\n",
    "    \"\"\"\n",
    "    Search using child chunks, but return parent chunks for context.\n",
    "    \n",
    "    This gives you:\n",
    "    - Precise search (small child chunks)\n",
    "    - Rich context (large parent chunks)\n",
    "    \"\"\"\n",
    "    # Step 1: Search child chunks\n",
    "    child_results = child_vectorstore.similarity_search(query, k=k)\n",
    "    \n",
    "    # Step 2: Get unique parent chunks\n",
    "    seen_parents = set()\n",
    "    parent_results = []\n",
    "    \n",
    "    for child_doc in child_results:\n",
    "        parent_idx = child_doc.metadata[\"parent_idx\"]\n",
    "        \n",
    "        if parent_idx not in seen_parents:\n",
    "            seen_parents.add(parent_idx)\n",
    "            parent_results.append({\n",
    "                \"parent_idx\": parent_idx,\n",
    "                \"parent_text\": parent_chunks[parent_idx],\n",
    "                \"matched_child\": child_doc.page_content\n",
    "            })\n",
    "    \n",
    "    return parent_results\n",
    "\n",
    "# Test it!\n",
    "query = \"What is LoRA?\"\n",
    "results = parent_child_retrieve(query, k=3)\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"\\nResult {i}:\")\n",
    "    print(f\"Matched CHILD (search hit): {result['matched_child'][:100]}...\")\n",
    "    print(f\"\\nReturned PARENT (full context): {result['parent_text'][:300]}...\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Strategy 4: Document-Aware Chunking\n",
    "\n",
    "**The Idea:**\n",
    "Respect the document's natural structure (headers, sections, lists).\n",
    "\n",
    "**Best for:**\n",
    "- Technical documentation\n",
    "- Legal documents with sections\n",
    "- Markdown files\n",
    "- HTML content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-Aware Chunking Results:\n",
      "Number of chunks: 5\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Chunk 1:\n",
      "Headers: {'h1': 'LLM Fundamentals', 'h2': 'Core Concepts'}\n",
      "Content: Large Language Models are neural networks trained on vast amounts of text data.\n",
      "They learn patterns in language and can generate human-like text.\n",
      "----------------------------------------\n",
      "\n",
      "Chunk 2:\n",
      "Headers: {'h1': 'LLM Fundamentals', 'h2': 'Core Concepts', 'h3': 'Transformer Architecture'}\n",
      "Content: The transformer is the backbone of modern LLMs. It uses self-attention\n",
      "to process sequences in parallel, making it efficient and powerful.\n",
      "----------------------------------------\n",
      "\n",
      "Chunk 3:\n",
      "Headers: {'h1': 'LLM Fundamentals', 'h2': 'Core Concepts', 'h3': 'Attention Mechanism'}\n",
      "Content: Attention allows the model to focus on relevant parts of the input\n",
      "when generating each output token.\n",
      "----------------------------------------\n",
      "\n",
      "Chunk 4:\n",
      "Headers: {'h1': 'LLM Fundamentals', 'h2': 'Fine-tuning Techniques', 'h3': 'LoRA'}\n",
      "Content: Low-Rank Adaptation (LoRA) is an efficient fine-tuning method.\n",
      "It adds small trainable matrices to the model instead of updating all weights.\n",
      "----------------------------------------\n",
      "\n",
      "Chunk 5:\n",
      "Headers: {'h1': 'LLM Fundamentals', 'h2': 'Fine-tuning Techniques', 'h3': 'QLoRA'}\n",
      "Content: QLoRA combines LoRA with quantization for even more efficient fine-tuning.\n",
      "It allows fine-tuning large models on consumer hardware.\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "# Sample markdown document (simulating structured content)\n",
    "sample_markdown = \"\"\"\n",
    "# LLM Fundamentals\n",
    "\n",
    "## Core Concepts\n",
    "\n",
    "Large Language Models are neural networks trained on vast amounts of text data.\n",
    "They learn patterns in language and can generate human-like text.\n",
    "\n",
    "### Transformer Architecture\n",
    "\n",
    "The transformer is the backbone of modern LLMs. It uses self-attention\n",
    "to process sequences in parallel, making it efficient and powerful.\n",
    "\n",
    "### Attention Mechanism\n",
    "\n",
    "Attention allows the model to focus on relevant parts of the input\n",
    "when generating each output token.\n",
    "\n",
    "## Fine-tuning Techniques\n",
    "\n",
    "### LoRA\n",
    "\n",
    "Low-Rank Adaptation (LoRA) is an efficient fine-tuning method.\n",
    "It adds small trainable matrices to the model instead of updating all weights.\n",
    "\n",
    "### QLoRA\n",
    "\n",
    "QLoRA combines LoRA with quantization for even more efficient fine-tuning.\n",
    "It allows fine-tuning large models on consumer hardware.\n",
    "\"\"\"\n",
    "\n",
    "# Define headers to split on\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"h1\"),      # Main title\n",
    "    (\"##\", \"h2\"),     # Section\n",
    "    (\"###\", \"h3\"),    # Subsection\n",
    "]\n",
    "\n",
    "# Create markdown-aware splitter\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on\n",
    ")\n",
    "\n",
    "# Split the document\n",
    "md_chunks = markdown_splitter.split_text(sample_markdown)\n",
    "\n",
    "print(f\"Document-Aware Chunking Results:\")\n",
    "print(f\"Number of chunks: {len(md_chunks)}\")\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "\n",
    "for i, chunk in enumerate(md_chunks, 1):\n",
    "    print(f\"\\nChunk {i}:\")\n",
    "    print(f\"Headers: {chunk.metadata}\")  # Shows the section hierarchy!\n",
    "    print(f\"Content: {chunk.page_content}\")\n",
    "    print(\"-\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Comparison: Which Strategy to Use?\n",
    "\n",
    "| Strategy | Best For | Pros | Cons |\n",
    "|----------|----------|------|------|\n",
    "| **Fixed-Size** | Quick prototypes | Fast, simple | May split concepts |\n",
    "| **Semantic** | General documents | Respects meaning | Slower, variable sizes |\n",
    "| **Parent-Child** | Precise search + context | Best of both worlds | More complex setup |\n",
    "| **Document-Aware** | Structured docs | Preserves structure | Needs structured input |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking Strategy Comparison:\n",
      "================================================================================\n",
      "\n",
      "Strategy             Chunks     Avg Size        Notes\n",
      "--------------------------------------------------------------------------------\n",
      "Fixed-Size           37         419             Fast, simple\n",
      "Semantic             54         287             Meaning-based\n",
      "Parent-Child         69         234             Search + context\n",
      "(Parents)            16         1005            Full context\n"
     ]
    }
   ],
   "source": [
    "# Summary comparison\n",
    "print(\"Chunking Strategy Comparison:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n{'Strategy':<20} {'Chunks':<10} {'Avg Size':<15} {'Notes'}\")\n",
    "print(\"-\"*80)\n",
    "print(f\"{'Fixed-Size':<20} {len(basic_chunks):<10} {sum(len(c) for c in basic_chunks) / len(basic_chunks):<15.0f} {'Fast, simple'}\")\n",
    "print(f\"{'Semantic':<20} {len(semantic_chunks):<10} {sum(len(c) for c in semantic_chunks) / len(semantic_chunks):<15.0f} {'Meaning-based'}\")\n",
    "print(f\"{'Parent-Child':<20} {len(all_child_chunks):<10} {sum(len(c) for c in all_child_chunks) / len(all_child_chunks):<15.0f} {'Search + context'}\")\n",
    "print(f\"{'(Parents)':<20} {len(parent_chunks):<10} {sum(len(c) for c in parent_chunks) / len(parent_chunks):<15.0f} {'Full context'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Best Practices for Chunking\n",
    "\n",
    "### 1. Data Cleaning (Pre-processing)\n",
    "Clean your data BEFORE chunking!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: This   is    messy    text|with OCR   errors.   Page 1 of 10\n",
      "After:  This is messy textwith OCR errors.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean text before chunking for better quality.\n",
    "    \n",
    "    Good chunking starts with clean data!\n",
    "    \"\"\"\n",
    "    # Remove excessive whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove special characters that do not add meaning\n",
    "    text = re.sub(r'[^\\w\\s.,!?;:\\-\\(\\)\\[\\]\\'\\\"]+', '', text)\n",
    "    \n",
    "    # Fix common OCR errors (if from scanned PDFs)\n",
    "    text = text.replace('|', 'I')  # Common OCR mistake\n",
    "    \n",
    "    # Remove page numbers, headers, footers (customize based on your docs)\n",
    "    text = re.sub(r'Page \\d+ of \\d+', '', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Example\n",
    "dirty_text = \"This   is    messy    text|with OCR   errors.   Page 1 of 10\"\n",
    "clean = clean_text(dirty_text)\n",
    "print(f\"Before: {dirty_text}\")\n",
    "print(f\"After:  {clean}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Chunk Size Guidelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk Size Guidelines:\n",
      "======================\n",
      "\n",
      "Small (200-500 chars):\n",
      "  - Best for: Precise Q&A, factual lookups\n",
      "  - Example: \"What year was X founded?\"\n",
      "  - Trade-off: May lack context\n",
      "\n",
      "Medium (500-1000 chars):\n",
      "  - Best for: General RAG applications\n",
      "  - Example: \"Explain how X works\"\n",
      "  - Trade-off: Good balance (recommended default)\n",
      "\n",
      "Large (1000-2000 chars):\n",
      "  - Best for: Complex reasoning, summaries\n",
      "  - Example: \"Compare X and Y\"\n",
      "  - Trade-off: Less precise retrieval\n",
      "\n",
      "Overlap Guidelines:\n",
      "  - 10-20% of chunk size\n",
      "  - Ensures context is not lost at boundaries\n",
      "  - Example: chunk_size=500, overlap=50-100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Recommended chunk sizes based on use case\n",
    "chunk_size_guide = \"\"\"\n",
    "Chunk Size Guidelines:\n",
    "======================\n",
    "\n",
    "Small (200-500 chars):\n",
    "  - Best for: Precise Q&A, factual lookups\n",
    "  - Example: \"What year was X founded?\"\n",
    "  - Trade-off: May lack context\n",
    "\n",
    "Medium (500-1000 chars):\n",
    "  - Best for: General RAG applications\n",
    "  - Example: \"Explain how X works\"\n",
    "  - Trade-off: Good balance (recommended default)\n",
    "\n",
    "Large (1000-2000 chars):\n",
    "  - Best for: Complex reasoning, summaries\n",
    "  - Example: \"Compare X and Y\"\n",
    "  - Trade-off: Less precise retrieval\n",
    "\n",
    "Overlap Guidelines:\n",
    "  - 10-20% of chunk size\n",
    "  - Ensures context is not lost at boundaries\n",
    "  - Example: chunk_size=500, overlap=50-100\n",
    "\"\"\"\n",
    "print(chunk_size_guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### What You Have Learned:\n",
    "1. **Fixed-Size**: Quick but may split concepts (our baseline)\n",
    "2. **Semantic**: Splits based on meaning changes (smarter)\n",
    "3. **Parent-Child**: Small chunks for search, large for context (best of both)\n",
    "4. **Document-Aware**: Respects headers and structure (for organized docs)\n",
    "\n",
    "### Key Takeaways:\n",
    "- Chunking is ONE OF THE MOST IMPORTANT factors in RAG quality\n",
    "- There is no single best strategy - it depends on your use case\n",
    "- Clean your data before chunking\n",
    "- Test different strategies and measure results (we will learn evaluation later!)\n",
    "\n",
    "### When to Use Each:\n",
    "- **Quick prototype**: Fixed-size\n",
    "- **General RAG**: Semantic or Fixed-size with good overlap\n",
    "- **High-quality production**: Parent-Child\n",
    "- **Technical docs**: Document-Aware\n",
    "\n",
    "### Next Up:\n",
    "**Hybrid Search** - Combining keyword search (BM25) with semantic search for even better retrieval!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLNotebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
