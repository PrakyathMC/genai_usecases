{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Search: BM25 + Dense Retrieval\n",
    "\n",
    "## Welcome!\n",
    "So far, we've used only semantic search (embeddings). But sometimes keywords matter!\n",
    "\n",
    "## The Problem with Semantic-Only Search\n",
    "\n",
    "```\n",
    "Query: \"What is GPT-4?\"\n",
    "\n",
    "Semantic search might return:\n",
    "- \"Large language models have revolutionized AI...\" (semantically similar but no GPT-4 mention!)\n",
    "\n",
    "Keyword search would find:\n",
    "- \"GPT-4 is OpenAI's latest model...\" (exact match!)\n",
    "```\n",
    "\n",
    "## The Solution: Combine Both!\n",
    "\n",
    "**Hybrid Search** = Keyword Search (BM25) + Semantic Search (Dense)\n",
    "\n",
    "- **BM25**: Finds exact keyword matches (\"GPT-4\", \"LoRA\", specific terms)\n",
    "- **Dense**: Finds semantically similar content (meaning, context)\n",
    "- **Hybrid**: Gets the best of both worlds!\n",
    "\n",
    "## What You'll Learn\n",
    "1. How BM25 (keyword search) works\n",
    "2. How to combine BM25 with semantic search\n",
    "3. Reciprocal Rank Fusion (RRF) for combining results\n",
    "4. When to use hybrid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY loaded: True\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # This should work since .env is in the same folder\n",
    "print(\"OPENAI_API_KEY loaded:\", \"OPENAI_API_KEY\" in os.environ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install rank_bm25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load and Prepare Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lapde\\anaconda3\\envs\\MLNotebook\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 37 chunks\n",
      "\n",
      "Sample chunk:\n",
      "@genieincodebottle \n",
      "Instagram | GitHub | Medium | YouTube \n",
      "How to Be Better Than Most in GenAI \n",
      " \n",
      "Contents \n",
      " \n",
      "Core LLM Building Blocks ....................................................................\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load PDF\n",
    "pdf_path = \"\\llm_fundamentals.pdf\"\n",
    "if not os.path.exists(pdf_path):\n",
    "    pdf_path = \"../RAG/llm_fundamentals.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "documents = loader.load()\n",
    "\n",
    "# Split into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# Extract just the text for our examples\n",
    "chunk_texts = [chunk.page_content for chunk in chunks]\n",
    "\n",
    "print(f\"Loaded {len(chunks)} chunks\")\n",
    "print(f\"\\nSample chunk:\")\n",
    "print(chunk_texts[0][:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Understanding BM25 (Keyword Search)\n",
    "\n",
    "**BM25** (Best Matching 25) is a classic keyword search algorithm.\n",
    "\n",
    "**How it works (simplified):**\n",
    "1. Looks for exact word matches\n",
    "2. Gives higher scores to:\n",
    "   - Rare words (\"LoRA\" is more important than \"the\")\n",
    "   - Multiple matches (more occurrences = higher score)\n",
    "3. Normalizes by document length\n",
    "\n",
    "**Think of it like:** Google search before AI - pure keyword matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 index created!\n",
      "Vocabulary size: 914 unique words\n"
     ]
    }
   ],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Tokenize documents (split into words)\n",
    "def simple_tokenize(text):\n",
    "    \"\"\"\n",
    "    Simple tokenization: lowercase and split by spaces.\n",
    "    In production, you'd use better tokenization.\n",
    "    \"\"\"\n",
    "    return text.lower().split()\n",
    "\n",
    "# Tokenize all chunks\n",
    "tokenized_chunks = [simple_tokenize(chunk) for chunk in chunk_texts]\n",
    "\n",
    "# Step 2: Create BM25 index\n",
    "bm25 = BM25Okapi(tokenized_chunks)\n",
    "\n",
    "print(\"BM25 index created!\")\n",
    "print(f\"Vocabulary size: {len(bm25.idf)} unique words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 Search Results for: 'What is LoRA?'\n",
      "================================================================================\n",
      "\n",
      "Result 1 (BM25 Score: 2.92):\n",
      "9. Data Pipelines → Automated data processing and validation workflows \n",
      "10. Model Governance → Policies and processes for responsible model deployment \n",
      "11. Rollback Strategies → Safe deployment and qu...\n",
      "----------------------------------------\n",
      "\n",
      "Result 2 (BM25 Score: 0.00):\n",
      "15. Jailbreaking Prevention → Protecting against attempts to bypass safety measures...\n",
      "----------------------------------------\n",
      "\n",
      "Result 3 (BM25 Score: 0.00):\n",
      "9. Red-Teaming → Stress-test models for safety, robustness, and alignment \n",
      "10. Robustness Evaluation → Test model resilience against noise, domain shifts, and edge \n",
      "cases \n",
      "11. Mechanistic Interpretabi...\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def bm25_search(query: str, k: int = 5):\n",
    "    \"\"\"\n",
    "    Search using BM25 (keyword matching).\n",
    "    \n",
    "    Args:\n",
    "        query: Search query\n",
    "        k: Number of results to return\n",
    "    \n",
    "    Returns:\n",
    "        List of (chunk_text, score) tuples\n",
    "    \"\"\"\n",
    "    # Tokenize the query\n",
    "    tokenized_query = simple_tokenize(query)\n",
    "    \n",
    "    # Get BM25 scores for all chunks\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    \n",
    "    # Get top-k indices\n",
    "    top_indices = np.argsort(scores)[::-1][:k]\n",
    "    \n",
    "    # Return results with scores\n",
    "    results = [(chunk_texts[i], scores[i]) for i in top_indices]\n",
    "    return results\n",
    "\n",
    "# Test BM25 search\n",
    "query = \"What is LoRA?\"\n",
    "bm25_results = bm25_search(query, k=3)\n",
    "\n",
    "print(f\"BM25 Search Results for: '{query}'\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, (text, score) in enumerate(bm25_results, 1):\n",
    "    print(f\"\\nResult {i} (BM25 Score: {score:.2f}):\")\n",
    "    print(f\"{text[:200]}...\")\n",
    "    print(\"-\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Setting Up Dense (Semantic) Search\n",
    "\n",
    "This is what we've been doing - using embeddings for semantic similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense (Semantic) search ready!\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Create embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create vector store\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"hybrid_demo\"\n",
    ")\n",
    "\n",
    "print(\"Dense (Semantic) search ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense Search Results for: 'What is LoRA?'\n",
      "================================================================================\n",
      "\n",
      "Result 1 (Distance: 1.5255 - lower is better):\n",
      "9. QLoRA → LoRA + quantization, enabling fine-tuning of huge models on modest hardware \n",
      "10. PEFT → Family of methods (e.g., LoRA, QLoRA, adapters) updating only small parts of the \n",
      "model \n",
      "11. Instruct...\n",
      "----------------------------------------\n",
      "\n",
      "Result 2 (Distance: 1.6433 - lower is better):\n",
      "@genieincodebottle \n",
      "Instagram | GitHub | Medium | YouTube \n",
      "How to Be Better Than Most in GenAI \n",
      " \n",
      "Contents \n",
      " \n",
      "Core LLM Building Blocks ....................................................................\n",
      "----------------------------------------\n",
      "\n",
      "Result 3 (Distance: 1.7043 - lower is better):\n",
      "requirements \n",
      "11. Explainability / Interpretability Evaluation → Assess clarity and transparency of model \n",
      "reasoning \n",
      "Extensions \n",
      "1. Multimodality → Combine text, images, audio, and video for richer u...\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def dense_search(query: str, k: int = 5):\n",
    "    \"\"\"\n",
    "    Search using dense embeddings (semantic similarity).\n",
    "    \"\"\"\n",
    "    results = vectorstore.similarity_search_with_score(query, k=k)\n",
    "    # Return in same format as BM25\n",
    "    return [(doc.page_content, score) for doc, score in results]\n",
    "\n",
    "# Test dense search\n",
    "dense_results = dense_search(query, k=3)\n",
    "\n",
    "print(f\"Dense Search Results for: '{query}'\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, (text, score) in enumerate(dense_results, 1):\n",
    "    print(f\"\\nResult {i} (Distance: {score:.4f} - lower is better):\")\n",
    "    print(f\"{text[:200]}...\")\n",
    "    print(\"-\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Comparing BM25 vs Dense\n",
    "\n",
    "Let's see where each method shines!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPARISON 1: Specific term query\n",
      "Query: 'LoRA'\n",
      "\n",
      "================================================================================\n",
      "\n",
      "BM25 (Keyword) Results:\n",
      "----------------------------------------\n",
      "1. (score: 2.52) 3. Sharded / Distributed Training → Scale across multiple GPUs/nodes \n",
      "4. Continual / Lifelong Learni...\n",
      "2. (score: 2.31) 9. QLoRA → LoRA + quantization, enabling fine-tuning of huge models on modest hardware \n",
      "10. PEFT → F...\n",
      "\n",
      "Dense (Semantic) Results:\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. (dist: 1.4533) 9. QLoRA → LoRA + quantization, enabling fine-tuning of huge models on modest hardware \n",
      "10. PEFT → F...\n",
      "2. (dist: 1.5870) @genieincodebottle \n",
      "Instagram | GitHub | Medium | YouTube \n",
      "How to Be Better Than Most in GenAI \n",
      " \n",
      "Co...\n",
      "\n",
      "================================================================================\n",
      "\n",
      "COMPARISON 2: Conceptual query\n",
      "Query: 'How can I make my model smaller?'\n",
      "\n",
      "================================================================================\n",
      "\n",
      "BM25 (Keyword) Results:\n",
      "----------------------------------------\n",
      "1. (score: 2.83) 3. Top-k / Top-p → Sampling filters, Higher = safer, looser = more diverse \n",
      "4. Repetition Penalty → ...\n",
      "2. (score: 2.61) @genieincodebottle \n",
      "Instagram | GitHub | Medium | YouTube \n",
      "How to Be Better Than Most in GenAI \n",
      " \n",
      "Co...\n",
      "\n",
      "Dense (Semantic) Results:\n",
      "----------------------------------------\n",
      "1. (dist: 1.5048) @genieincodebottle \n",
      "11. Sharded / Distributed Training → Split model parameters across devices for m...\n",
      "2. (dist: 1.5145) 15. Distillation → Transfer knowledge from a large model into a smaller one \n",
      "16. Gradient Descent & ...\n",
      "\n",
      "================================================================================\n",
      "\n",
      "COMPARISON 3: Acronym query\n",
      "Query: 'QLoRA'\n",
      "\n",
      "================================================================================\n",
      "\n",
      "BM25 (Keyword) Results:\n",
      "----------------------------------------\n",
      "1. (score: 2.78) 9. QLoRA → LoRA + quantization, enabling fine-tuning of huge models on modest hardware \n",
      "10. PEFT → F...\n",
      "2. (score: 0.00) 9. Red-Teaming → Stress-test models for safety, robustness, and alignment \n",
      "10. Robustness Evaluation...\n",
      "\n",
      "Dense (Semantic) Results:\n",
      "----------------------------------------\n",
      "1. (dist: 1.2662) 9. QLoRA → LoRA + quantization, enabling fine-tuning of huge models on modest hardware \n",
      "10. PEFT → F...\n",
      "2. (dist: 1.6151) 16. Elastic / Dynamic Batching → Adjust batch size dynamically to optimize throughput \n",
      "17. Inference...\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def compare_search_methods(query: str):\n",
    "    \"\"\"\n",
    "    Compare BM25 and Dense search side by side.\n",
    "    \"\"\"\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    # BM25 results\n",
    "    print(\"\\nBM25 (Keyword) Results:\")\n",
    "    print(\"-\"*40)\n",
    "    bm25_results = bm25_search(query, k=2)\n",
    "    for i, (text, score) in enumerate(bm25_results, 1):\n",
    "        print(f\"{i}. (score: {score:.2f}) {text[:100]}...\")\n",
    "    \n",
    "    # Dense results\n",
    "    print(\"\\nDense (Semantic) Results:\")\n",
    "    print(\"-\"*40)\n",
    "    dense_results = dense_search(query, k=2)\n",
    "    for i, (text, score) in enumerate(dense_results, 1):\n",
    "        print(f\"{i}. (dist: {score:.4f}) {text[:100]}...\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Test with different query types\n",
    "print(\"COMPARISON 1: Specific term query\")\n",
    "compare_search_methods(\"LoRA\")\n",
    "\n",
    "print(\"COMPARISON 2: Conceptual query\")\n",
    "compare_search_methods(\"How can I make my model smaller?\")\n",
    "\n",
    "print(\"COMPARISON 3: Acronym query\")\n",
    "compare_search_methods(\"QLoRA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Hybrid Search with Reciprocal Rank Fusion (RRF)\n",
    "\n",
    "**The Problem:**\n",
    "BM25 and Dense scores are on different scales - we can't just add them!\n",
    "\n",
    "**The Solution: Reciprocal Rank Fusion (RRF)**\n",
    "\n",
    "Instead of combining scores, we combine RANKS:\n",
    "\n",
    "```\n",
    "RRF Score = 1/(k + rank_bm25) + 1/(k + rank_dense)\n",
    "\n",
    "Where k is a constant (usually 60)\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- If a document ranks high in BOTH methods, it gets a high combined score\n",
    "- Documents that rank high in only one method still get credit\n",
    "- Handles different score scales automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid search function ready!\n"
     ]
    }
   ],
   "source": [
    "def hybrid_search_rrf(query: str, k: int = 5, rrf_k: int = 60):\n",
    "    \"\"\"\n",
    "    Hybrid search combining BM25 and Dense using Reciprocal Rank Fusion.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query\n",
    "        k: Number of final results to return\n",
    "        rrf_k: RRF constant (default 60, standard value)\n",
    "    \n",
    "    Returns:\n",
    "        List of (chunk_text, rrf_score) tuples\n",
    "    \"\"\"\n",
    "    # Get more results than needed from each method\n",
    "    num_candidates = k * 3\n",
    "    \n",
    "    # Step 1: Get BM25 results\n",
    "    bm25_results = bm25_search(query, k=num_candidates)\n",
    "    \n",
    "    # Step 2: Get Dense results  \n",
    "    dense_results = dense_search(query, k=num_candidates)\n",
    "    \n",
    "    # Step 3: Create rank dictionaries\n",
    "    # Map chunk text to its rank in each method\n",
    "    bm25_ranks = {text: rank for rank, (text, _) in enumerate(bm25_results, 1)}\n",
    "    dense_ranks = {text: rank for rank, (text, _) in enumerate(dense_results, 1)}\n",
    "    \n",
    "    # Step 4: Get all unique chunks\n",
    "    all_chunks = set(bm25_ranks.keys()) | set(dense_ranks.keys())\n",
    "    \n",
    "    # Step 5: Calculate RRF scores\n",
    "    rrf_scores = {}\n",
    "    for chunk in all_chunks:\n",
    "        # Get rank from each method (use large number if not in results)\n",
    "        bm25_rank = bm25_ranks.get(chunk, 1000)\n",
    "        dense_rank = dense_ranks.get(chunk, 1000)\n",
    "        \n",
    "        # Calculate RRF score\n",
    "        rrf_score = (1 / (rrf_k + bm25_rank)) + (1 / (rrf_k + dense_rank))\n",
    "        rrf_scores[chunk] = {\n",
    "            'rrf_score': rrf_score,\n",
    "            'bm25_rank': bm25_rank,\n",
    "            'dense_rank': dense_rank\n",
    "        }\n",
    "    \n",
    "    # Step 6: Sort by RRF score and return top-k\n",
    "    sorted_results = sorted(rrf_scores.items(), key=lambda x: x[1]['rrf_score'], reverse=True)\n",
    "    \n",
    "    return sorted_results[:k]\n",
    "\n",
    "print(\"Hybrid search function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid Search Results for: 'What is LoRA and how does it help with fine-tuning?'\n",
      "================================================================================\n",
      "\n",
      "Result 1:\n",
      "  RRF Score: 0.0320\n",
      "  BM25 Rank: 4 | Dense Rank: 1\n",
      "  Text: 9. QLoRA → LoRA + quantization, enabling fine-tuning of huge models on modest hardware \n",
      "10. PEFT → Family of methods (e.g., LoRA, QLoRA, adapters) upd...\n",
      "----------------------------------------\n",
      "\n",
      "Result 2:\n",
      "  RRF Score: 0.0308\n",
      "  BM25 Rank: 8 | Dense Rank: 2\n",
      "  Text: 3. Sharded / Distributed Training → Scale across multiple GPUs/nodes \n",
      "4. Continual / Lifelong Learning → Update models without forgetting old knowledg...\n",
      "----------------------------------------\n",
      "\n",
      "Result 3:\n",
      "  RRF Score: 0.0292\n",
      "  BM25 Rank: 6 | Dense Rank: 11\n",
      "  Text: @genieincodebottle \n",
      "Instagram | GitHub | Medium | YouTube \n",
      "How to Be Better Than Most in GenAI \n",
      " \n",
      "Contents \n",
      " \n",
      "Core LLM Building Blocks ..................\n",
      "----------------------------------------\n",
      "\n",
      "Result 4:\n",
      "  RRF Score: 0.0286\n",
      "  BM25 Rank: 13 | Dense Rank: 7\n",
      "  Text: 16. Elastic / Dynamic Batching → Adjust batch size dynamically to optimize throughput \n",
      "17. Inference Optimization → Operator fusion, kernel tuning, an...\n",
      "----------------------------------------\n",
      "\n",
      "Result 5:\n",
      "  RRF Score: 0.0282\n",
      "  BM25 Rank: 10 | Dense Rank: 12\n",
      "  Text: @genieincodebottle \n",
      "3. Model Monitoring & Drift Detection → Ensure continued performance over time \n",
      "4. Continuous Learning → Updating models with new ...\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test hybrid search\n",
    "query = \"What is LoRA and how does it help with fine-tuning?\"\n",
    "\n",
    "print(f\"Hybrid Search Results for: '{query}'\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "hybrid_results = hybrid_search_rrf(query, k=5)\n",
    "\n",
    "for i, (text, scores) in enumerate(hybrid_results, 1):\n",
    "    print(f\"\\nResult {i}:\")\n",
    "    print(f\"  RRF Score: {scores['rrf_score']:.4f}\")\n",
    "    print(f\"  BM25 Rank: {scores['bm25_rank']} | Dense Rank: {scores['dense_rank']}\")\n",
    "    print(f\"  Text: {text[:150]}...\")\n",
    "    print(\"-\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Weighted Hybrid Search\n",
    "\n",
    "Sometimes you want to give more weight to one method over the other.\n",
    "\n",
    "**Use cases:**\n",
    "- Technical docs with many acronyms → Weight BM25 higher\n",
    "- Conceptual queries → Weight Dense higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'QLoRA quantization'\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Pure Dense (BM25 weight: 0.0):\n",
      "  1. 9. QLoRA → LoRA + quantization, enabling fine-tuning of huge models on modest ha...\n",
      "  2. 16. Elastic / Dynamic Batching → Adjust batch size dynamically to optimize throu...\n",
      "\n",
      "Balanced (BM25 weight: 0.5):\n",
      "  1. 9. QLoRA → LoRA + quantization, enabling fine-tuning of huge models on modest ha...\n",
      "  2. 16. Elastic / Dynamic Batching → Adjust batch size dynamically to optimize throu...\n",
      "\n",
      "Pure BM25 (BM25 weight: 1.0):\n",
      "  1. 9. QLoRA → LoRA + quantization, enabling fine-tuning of huge models on modest ha...\n",
      "  2. 16. Elastic / Dynamic Batching → Adjust batch size dynamically to optimize throu...\n"
     ]
    }
   ],
   "source": [
    "def weighted_hybrid_search(query: str, k: int = 5, bm25_weight: float = 0.5):\n",
    "    \"\"\"\n",
    "    Weighted hybrid search - control the balance between BM25 and Dense.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query\n",
    "        k: Number of results\n",
    "        bm25_weight: Weight for BM25 (0.0 to 1.0)\n",
    "                     0.0 = Pure dense, 1.0 = Pure BM25, 0.5 = Equal\n",
    "    \"\"\"\n",
    "    dense_weight = 1 - bm25_weight\n",
    "    num_candidates = k * 3\n",
    "    rrf_k = 60\n",
    "    \n",
    "    # Get results from both methods\n",
    "    bm25_results = bm25_search(query, k=num_candidates)\n",
    "    dense_results = dense_search(query, k=num_candidates)\n",
    "    \n",
    "    # Create rank dictionaries\n",
    "    bm25_ranks = {text: rank for rank, (text, _) in enumerate(bm25_results, 1)}\n",
    "    dense_ranks = {text: rank for rank, (text, _) in enumerate(dense_results, 1)}\n",
    "    \n",
    "    # Get all unique chunks\n",
    "    all_chunks = set(bm25_ranks.keys()) | set(dense_ranks.keys())\n",
    "    \n",
    "    # Calculate WEIGHTED RRF scores\n",
    "    rrf_scores = {}\n",
    "    for chunk in all_chunks:\n",
    "        bm25_rank = bm25_ranks.get(chunk, 1000)\n",
    "        dense_rank = dense_ranks.get(chunk, 1000)\n",
    "        \n",
    "        # Weighted RRF\n",
    "        rrf_score = (\n",
    "            bm25_weight * (1 / (rrf_k + bm25_rank)) + \n",
    "            dense_weight * (1 / (rrf_k + dense_rank))\n",
    "        )\n",
    "        rrf_scores[chunk] = rrf_score\n",
    "    \n",
    "    # Sort and return\n",
    "    sorted_results = sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_results[:k]\n",
    "\n",
    "# Compare different weights\n",
    "query = \"QLoRA quantization\"\n",
    "\n",
    "print(f\"Query: '{query}'\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "for weight in [0.0, 0.5, 1.0]:\n",
    "    weight_name = {0.0: \"Pure Dense\", 0.5: \"Balanced\", 1.0: \"Pure BM25\"}[weight]\n",
    "    print(f\"\\n{weight_name} (BM25 weight: {weight}):\")\n",
    "    results = weighted_hybrid_search(query, k=2, bm25_weight=weight)\n",
    "    for i, (text, score) in enumerate(results, 1):\n",
    "        print(f\"  {i}. {text[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: Using LangChain's Built-in Ensemble Retriever\n",
    "\n",
    "LangChain provides an easy way to do hybrid search!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain Ensemble Retriever ready!\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.retrievers import BM25Retriever, EnsembleRetriever\n",
    "\n",
    "# Create BM25 retriever from documents\n",
    "bm25_retriever = BM25Retriever.from_documents(chunks)\n",
    "bm25_retriever.k = 5  # Number of results\n",
    "\n",
    "# Create dense retriever from vector store\n",
    "dense_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# Create ensemble (hybrid) retriever/\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, dense_retriever],\n",
    "    weights=[0.5, 0.5]  # Equal weights\n",
    ")\n",
    "\n",
    "print(\"LangChain Ensemble Retriever ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Retriever Results for: 'What is attention mechanism in transformers?'\n",
      "================================================================================\n",
      "\n",
      "Result 1:\n",
      "5. Attention → Highlights the most relevant tokens in context \n",
      "6. Self-Attention → Each token attends to every other token for context \n",
      "7. Cross-Attention → Connect encoder and decoder (in encoder-dec...\n",
      "----------------------------------------\n",
      "\n",
      "Result 2:\n",
      "17. ALiBi / Relative Positional Encoding → Alternative to RoPE for long contexts \n",
      "18. Linear / Performer Attention → Efficient attention variants for very long sequences \n",
      "19. Grouped Query Attention (...\n",
      "----------------------------------------\n",
      "\n",
      "Result 3:\n",
      "9. Data Pipelines → Automated data processing and validation workflows \n",
      "10. Model Governance → Policies and processes for responsible model deployment \n",
      "11. Rollback Strategies → Safe deployment and qu...\n",
      "----------------------------------------\n",
      "\n",
      "Result 4:\n",
      "11. Layer Normalization → Normalizes activations to stabilize and speed up training \n",
      "12. Output Projection (LM Head) → Final linear layer mapping hidden states into logits \n",
      "13. Logits → Raw prediction...\n",
      "----------------------------------------\n",
      "\n",
      "Result 5:\n",
      "@genieincodebottle \n",
      "11. Sharded / Distributed Training → Split model parameters across devices for massive \n",
      "models \n",
      "12. Pipeline Parallelism → Overlap sequential layer computation across devices for s...\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test the ensemble retriever\n",
    "query = \"What is attention mechanism in transformers?\"\n",
    "\n",
    "print(f\"Ensemble Retriever Results for: '{query}'\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = ensemble_retriever.invoke(query)\n",
    "\n",
    "for i, doc in enumerate(results[:5], 1):\n",
    "    print(f\"\\nResult {i}:\")\n",
    "    print(f\"{doc.page_content[:200]}...\")\n",
    "    print(\"-\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 7: Complete Hybrid RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid RAG pipeline ready!\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_classic.chains.retrieval_qa.base import RetrievalQA\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.7,\n",
    "    api_key = os.environ['OPENAI_API_KEY']\n",
    ")\n",
    "\n",
    "# Create RAG chain with hybrid retriever\n",
    "hybrid_qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=ensemble_retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "print(\"Hybrid RAG pipeline ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is LoRA and how does it differ from QLoRA?\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Answer:\n",
      "LoRA (Low-Rank Adaptation) is a method that involves parameter-efficient adapters for fine-tuning machine learning models. It allows for cheap fine-tuning by updating only a small subset of model parameters, which can be beneficial for adapting models to specific tasks or domains without the need for extensive retraining.\n",
      "\n",
      "QLoRA (Quantized LoRA) combines LoRA with quantization techniques. This approach enables fine-tuning of large models on modest hardware by reducing the memory footprint of the model. By quantizing the parameters, QLoRA makes it feasible to work with larger models even on hardware with limited resources.\n",
      "\n",
      "In summary, while both LoRA and QLoRA are designed for efficient fine-tuning, QLoRA specifically incorporates quantization to facilitate the training of larger models on less powerful hardware.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Sources (9 documents):\n",
      "  1. 3. Sharded / Distributed Training → Scale across multiple GPUs/nodes \n",
      "4. Continual / Lifelong Learni...\n",
      "  2. 9. Data Pipelines → Automated data processing and validation workflows \n",
      "10. Model Governance → Polic...\n",
      "  3. 9. QLoRA → LoRA + quantization, enabling fine-tuning of huge models on modest hardware \n",
      "10. PEFT → F...\n"
     ]
    }
   ],
   "source": [
    "# Test the hybrid RAG pipeline\n",
    "def ask_hybrid_rag(question: str):\n",
    "    \"\"\"\n",
    "    Ask a question using the hybrid RAG pipeline.\n",
    "    \"\"\"\n",
    "    result = hybrid_qa_chain.invoke({\"query\": question})\n",
    "    \n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(f\"\\nAnswer:\\n{result['result']}\")\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(f\"\\nSources ({len(result['source_documents'])} documents):\")\n",
    "    for i, doc in enumerate(result['source_documents'][:3], 1):\n",
    "        print(f\"  {i}. {doc.page_content[:100]}...\")\n",
    "\n",
    "# Test with different types of queries\n",
    "ask_hybrid_rag(\"What is LoRA and how does it differ from QLoRA?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How can I efficiently fine-tune a large model on limited hardware?\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Answer:\n",
      "You can efficiently fine-tune a large model on limited hardware by using techniques such as:\n",
      "\n",
      "1. **LoRA (Low-Rank Adaptation)**: This method allows for parameter-efficient fine-tuning, enabling you to adapt the model without needing to adjust all of its parameters.\n",
      "\n",
      "2. **QLoRA**: This combines LoRA with quantization, allowing for fine-tuning of large models while using modest hardware resources.\n",
      "\n",
      "3. **Gradient Checkpointing**: This technique saves memory by recomputing intermediate activations on demand instead of storing them all.\n",
      "\n",
      "4. **Mixed Precision Training**: Using FP16 or BF16 can speed up training and reduce memory usage.\n",
      "\n",
      "5. **Sharded / Distributed Training**: If you have access to multiple devices, you can scale across multiple GPUs or nodes to distribute the training workload.\n",
      "\n",
      "These strategies can help you maximize efficiency while working within the constraints of limited hardware.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Sources (9 documents):\n",
      "  1. @genieincodebottle \n",
      "11. Sharded / Distributed Training → Split model parameters across devices for m...\n",
      "  2. 9. Edge Deployment → Run models on local or mobile devices with limited resources \n",
      "10. Mixed Precisi...\n",
      "  3. 15. Distillation → Transfer knowledge from a large model into a smaller one \n",
      "16. Gradient Descent & ...\n"
     ]
    }
   ],
   "source": [
    "# Test with a more conceptual query\n",
    "ask_hybrid_rag(\"How can I efficiently fine-tune a large model on limited hardware?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## When to Use Hybrid Search?\n",
    "\n",
    "| Scenario | Recommendation |\n",
    "|----------|----------------|\n",
    "| Technical docs with acronyms | Hybrid (weight BM25 higher) |\n",
    "| Conceptual questions | Hybrid or Pure Dense |\n",
    "| Exact term lookups | BM25 or Hybrid |\n",
    "| Synonyms/paraphrasing | Dense or Hybrid |\n",
    "| General RAG | Hybrid (balanced) |\n",
    "\n",
    "**Rule of thumb:** When in doubt, use Hybrid with equal weights. It's rarely worse than either method alone!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### What You've Learned:\n",
    "1. **BM25**: Keyword-based search (exact matches)\n",
    "2. **Dense**: Semantic search (meaning-based)\n",
    "3. **Hybrid**: Combines both for better retrieval\n",
    "4. **RRF**: Reciprocal Rank Fusion for combining results\n",
    "5. **Weighted Hybrid**: Control the balance between methods\n",
    "\n",
    "### Key Takeaways:\n",
    "- Neither BM25 nor Dense is always better - it depends on the query!\n",
    "- Hybrid search gives you the best of both worlds\n",
    "- RRF is a simple but effective way to combine rankings\n",
    "- LangChain's EnsembleRetriever makes hybrid search easy\n",
    "\n",
    "### BM25 vs Dense vs Hybrid:\n",
    "```\n",
    "BM25:   \"GPT-4\" → Finds exact \"GPT-4\" mentions\n",
    "Dense:  \"GPT-4\" → Might find \"large language model\" (semantically similar)\n",
    "Hybrid: \"GPT-4\" → Finds both! Best coverage.\n",
    "```\n",
    "\n",
    "### Next Up:\n",
    "**Re-ranking** - After retrieval, use a more powerful model to re-order results for even better quality!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLNotebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
