{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-ranking with Cross-Encoders\n",
    "\n",
    "## Welcome!\n",
    "We've learned retrieval methods (BM25, Dense, Hybrid). But there's a problem:\n",
    "**Initial retrieval is fast but not always accurate.**\n",
    "\n",
    "## The Problem\n",
    "\n",
    "```\n",
    "Bi-Encoder (what we've been using):\n",
    "┌─────────────────────────────────────────────────┐\n",
    "│  Query → Embedding  ─────┐                      │\n",
    "│                          ├── Compare (fast!)    │\n",
    "│  Document → Embedding ───┘                      │\n",
    "└─────────────────────────────────────────────────┘\n",
    "Problem: Query and document are encoded SEPARATELY.\n",
    "         No direct interaction = might miss nuances.\n",
    "```\n",
    "\n",
    "## The Solution: Cross-Encoders\n",
    "\n",
    "```\n",
    "Cross-Encoder:\n",
    "┌─────────────────────────────────────────────────┐\n",
    "│  [Query + Document] → Model → Relevance Score   │\n",
    "└─────────────────────────────────────────────────┘\n",
    "Query and document processed TOGETHER.\n",
    "Much more accurate, but slower.\n",
    "```\n",
    "\n",
    "## The Strategy: Two-Stage Retrieval\n",
    "\n",
    "```\n",
    "Stage 1 (Fast): Retrieve top 20-50 candidates (Bi-Encoder)\n",
    "Stage 2 (Accurate): Re-rank to get top 3-5 (Cross-Encoder)\n",
    "```\n",
    "\n",
    "**This is how production RAG systems work!**\n",
    "\n",
    "## What You'll Learn\n",
    "1. Bi-Encoder vs Cross-Encoder differences\n",
    "2. How to use cross-encoders for re-ranking\n",
    "3. Two-stage retrieval pipeline\n",
    "4. When re-ranking helps most"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "print(\"Environment loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load and Prepare Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Load PDF\n",
    "pdf_path = \"../LangChain_projects/llm_fundamentals.pdf\"\n",
    "if not os.path.exists(pdf_path):\n",
    "    pdf_path = \"../RAG/llm_fundamentals.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "documents = loader.load()\n",
    "\n",
    "# Split into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Loaded {len(chunks)} chunks\")\n",
    "\n",
    "# Create vector store for initial retrieval\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"rerank_demo\"\n",
    ")\n",
    "\n",
    "print(\"Vector store ready for initial retrieval!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Understanding Bi-Encoder vs Cross-Encoder\n",
    "\n",
    "Let's visualize the difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation = \"\"\"\n",
    "BI-ENCODER (Fast, Less Accurate)\n",
    "================================\n",
    "Used for: Initial retrieval from large collections\n",
    "\n",
    "How it works:\n",
    "1. Encode query ONCE → query_embedding\n",
    "2. Encode all documents ONCE (pre-computed) → doc_embeddings\n",
    "3. Compare using simple cosine similarity\n",
    "\n",
    "       Query          Document\n",
    "         ↓               ↓\n",
    "    [Encoder]       [Encoder]\n",
    "         ↓               ↓\n",
    "    [Vector]        [Vector]\n",
    "         ↓               ↓\n",
    "         └───Compare────┘\n",
    "              (fast!)\n",
    "\n",
    "Pros: Very fast (can search millions of docs)\n",
    "Cons: Query and doc don't \"see\" each other\n",
    "\n",
    "\n",
    "CROSS-ENCODER (Slow, Very Accurate)\n",
    "====================================\n",
    "Used for: Re-ranking a small set of candidates\n",
    "\n",
    "How it works:\n",
    "1. Concatenate query + document\n",
    "2. Pass through model TOGETHER\n",
    "3. Model outputs relevance score directly\n",
    "\n",
    "    [Query, Document]\n",
    "           ↓\n",
    "      [Encoder]\n",
    "           ↓\n",
    "    Relevance Score\n",
    "\n",
    "Pros: Very accurate (sees full context)\n",
    "Cons: Slow (must process each query-doc pair)\n",
    "\"\"\"\n",
    "print(explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Load Cross-Encoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# Load a pre-trained cross-encoder model\n",
    "# This model is specifically trained for re-ranking!\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "print(\"Cross-encoder model loaded!\")\n",
    "print(\"Model: cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "print(\"Trained on: MS MARCO (Microsoft Machine Reading Comprehension)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Basic Cross-Encoder Usage\n",
    "\n",
    "Let's see how the cross-encoder scores query-document pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Score query-document pairs\n",
    "query = \"What is LoRA?\"\n",
    "\n",
    "# Some example documents (one relevant, one not)\n",
    "doc_relevant = \"LoRA (Low-Rank Adaptation) is a method for efficiently fine-tuning large language models by updating only small matrices.\"\n",
    "doc_irrelevant = \"The weather today is sunny with a high of 75 degrees.\"\n",
    "doc_somewhat = \"Fine-tuning large models requires significant computational resources.\"\n",
    "\n",
    "# Cross-encoder takes pairs of (query, document)\n",
    "pairs = [\n",
    "    (query, doc_relevant),\n",
    "    (query, doc_somewhat),\n",
    "    (query, doc_irrelevant)\n",
    "]\n",
    "\n",
    "# Get relevance scores\n",
    "scores = cross_encoder.predict(pairs)\n",
    "\n",
    "print(f\"Query: '{query}'\")\n",
    "print(\"\\nCross-Encoder Scores (higher = more relevant):\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nRelevant doc:   {scores[0]:.4f}\")\n",
    "print(f\"Document: {doc_relevant[:60]}...\")\n",
    "print(f\"\\nSomewhat related: {scores[1]:.4f}\")\n",
    "print(f\"Document: {doc_somewhat[:60]}...\")\n",
    "print(f\"\\nIrrelevant doc: {scores[2]:.4f}\")\n",
    "print(f\"Document: {doc_irrelevant[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Two-Stage Retrieval Pipeline\n",
    "\n",
    "Now let's build a complete pipeline:\n",
    "1. **Stage 1**: Fast retrieval with bi-encoder (get top 20)\n",
    "2. **Stage 2**: Re-rank with cross-encoder (get top 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_and_rerank(query: str, initial_k: int = 20, final_k: int = 5):\n",
    "    \"\"\"\n",
    "    Two-stage retrieval with re-ranking.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query\n",
    "        initial_k: Number of candidates from stage 1 (bi-encoder)\n",
    "        final_k: Number of final results after re-ranking\n",
    "    \n",
    "    Returns:\n",
    "        List of (document, cross_encoder_score) tuples\n",
    "    \"\"\"\n",
    "    # STAGE 1: Initial retrieval (fast, bi-encoder)\n",
    "    print(f\"Stage 1: Retrieving top {initial_k} candidates...\")\n",
    "    initial_results = vectorstore.similarity_search(query, k=initial_k)\n",
    "    \n",
    "    # STAGE 2: Re-ranking (accurate, cross-encoder)\n",
    "    print(f\"Stage 2: Re-ranking with cross-encoder...\")\n",
    "    \n",
    "    # Prepare pairs for cross-encoder\n",
    "    pairs = [(query, doc.page_content) for doc in initial_results]\n",
    "    \n",
    "    # Get cross-encoder scores\n",
    "    scores = cross_encoder.predict(pairs)\n",
    "    \n",
    "    # Combine documents with scores and sort\n",
    "    scored_docs = list(zip(initial_results, scores))\n",
    "    scored_docs.sort(key=lambda x: x[1], reverse=True)  # Sort by score descending\n",
    "    \n",
    "    # Return top final_k\n",
    "    return scored_docs[:final_k]\n",
    "\n",
    "print(\"Two-stage retrieval function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the two-stage pipeline\n",
    "query = \"How does LoRA help with fine-tuning efficiency?\"\n",
    "\n",
    "print(f\"Query: '{query}'\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "reranked_results = retrieve_and_rerank(query, initial_k=15, final_k=5)\n",
    "\n",
    "print(f\"\\nTop {len(reranked_results)} Re-ranked Results:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, (doc, score) in enumerate(reranked_results, 1):\n",
    "    print(f\"\\nRank {i} (Score: {score:.4f}):\")\n",
    "    print(f\"{doc.page_content[:200]}...\")\n",
    "    print(\"-\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Comparing Before and After Re-ranking\n",
    "\n",
    "Let's see how re-ranking improves results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_with_without_reranking(query: str, k: int = 5):\n",
    "    \"\"\"\n",
    "    Compare search results with and without re-ranking.\n",
    "    \"\"\"\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    # WITHOUT re-ranking (just bi-encoder)\n",
    "    print(\"\\nWITHOUT Re-ranking (Bi-Encoder only):\")\n",
    "    print(\"-\"*40)\n",
    "    basic_results = vectorstore.similarity_search(query, k=k)\n",
    "    for i, doc in enumerate(basic_results, 1):\n",
    "        print(f\"{i}. {doc.page_content[:100]}...\")\n",
    "    \n",
    "    # WITH re-ranking\n",
    "    print(\"\\nWITH Re-ranking (Bi-Encoder + Cross-Encoder):\")\n",
    "    print(\"-\"*40)\n",
    "    reranked = retrieve_and_rerank(query, initial_k=20, final_k=k)\n",
    "    for i, (doc, score) in enumerate(reranked, 1):\n",
    "        print(f\"{i}. (score: {score:.3f}) {doc.page_content[:80]}...\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Compare with a specific query\n",
    "compare_with_without_reranking(\"What are the benefits of using LoRA for fine-tuning?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try another query\n",
    "compare_with_without_reranking(\"How does attention mechanism work in transformers?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: Complete RAG Pipeline with Re-ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.7,\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"]\n",
    ")\n",
    "\n",
    "# Create prompt template\n",
    "prompt_template = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a helpful AI assistant. Answer the question based on the provided context.\n",
    "If the context doesn't contain the answer, say \"I don't have enough information to answer that.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\")\n",
    "\n",
    "print(\"LLM and prompt ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_with_reranking(question: str, initial_k: int = 20, final_k: int = 5):\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline with re-ranking.\n",
    "    \n",
    "    Steps:\n",
    "    1. Retrieve initial candidates (bi-encoder)\n",
    "    2. Re-rank candidates (cross-encoder)\n",
    "    3. Generate answer (LLM)\n",
    "    \"\"\"\n",
    "    print(f\"Question: {question}\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    # Stage 1 & 2: Retrieve and re-rank\n",
    "    reranked_results = retrieve_and_rerank(question, initial_k, final_k)\n",
    "    \n",
    "    # Prepare context from top re-ranked documents\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc, _ in reranked_results])\n",
    "    \n",
    "    # Stage 3: Generate answer\n",
    "    print(\"Stage 3: Generating answer with LLM...\")\n",
    "    \n",
    "    # Format prompt\n",
    "    prompt = prompt_template.format(context=context, question=question)\n",
    "    \n",
    "    # Get answer\n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ANSWER:\")\n",
    "    print(response.content)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"SOURCES (Top {final_k} re-ranked):\")\n",
    "    for i, (doc, score) in enumerate(reranked_results, 1):\n",
    "        print(f\"  {i}. (score: {score:.3f}) {doc.page_content[:80]}...\")\n",
    "    \n",
    "    return response.content\n",
    "\n",
    "print(\"RAG with re-ranking function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the complete pipeline\n",
    "answer = rag_with_reranking(\"What is LoRA and why is it useful for fine-tuning large models?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try another question\n",
    "answer = rag_with_reranking(\"Explain the difference between LoRA and QLoRA.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 7: Performance Considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def measure_performance(query: str):\n",
    "    \"\"\"\n",
    "    Measure time for each stage.\n",
    "    \"\"\"\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(\"\\nPerformance Breakdown:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Stage 1: Bi-encoder retrieval\n",
    "    start = time.time()\n",
    "    initial_results = vectorstore.similarity_search(query, k=20)\n",
    "    stage1_time = time.time() - start\n",
    "    print(f\"Stage 1 (Bi-Encoder, 20 docs):    {stage1_time*1000:.1f} ms\")\n",
    "    \n",
    "    # Stage 2: Cross-encoder re-ranking\n",
    "    start = time.time()\n",
    "    pairs = [(query, doc.page_content) for doc in initial_results]\n",
    "    scores = cross_encoder.predict(pairs)\n",
    "    stage2_time = time.time() - start\n",
    "    print(f\"Stage 2 (Cross-Encoder, 20 docs): {stage2_time*1000:.1f} ms\")\n",
    "    \n",
    "    print(f\"\\nTotal retrieval time:             {(stage1_time + stage2_time)*1000:.1f} ms\")\n",
    "    print(f\"\\nNote: Cross-encoder is slower but more accurate!\")\n",
    "    print(f\"      That's why we only re-rank a small candidate set.\")\n",
    "\n",
    "measure_performance(\"What is attention mechanism?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## When to Use Re-ranking?\n",
    "\n",
    "| Scenario | Re-ranking Helps? | Reason |\n",
    "|----------|-------------------|--------|\n",
    "| High accuracy needed | Yes | Cross-encoder is more accurate |\n",
    "| Latency-sensitive | Maybe | Adds ~100-500ms per query |\n",
    "| Complex queries | Yes | Better at understanding nuance |\n",
    "| Simple keyword lookup | No | BM25/bi-encoder is sufficient |\n",
    "| Large result set needed | No | Re-ranking doesn't help if you need 100+ results |\n",
    "\n",
    "**Best practice:**\n",
    "- Initial retrieval: 20-50 candidates\n",
    "- Re-rank to: 3-10 final results\n",
    "- More candidates = better recall but slower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### What You've Learned:\n",
    "1. **Bi-Encoder vs Cross-Encoder**: Trade-off between speed and accuracy\n",
    "2. **Two-Stage Retrieval**: Fast initial retrieval + accurate re-ranking\n",
    "3. **Cross-Encoder Usage**: How to score query-document pairs\n",
    "4. **Complete Pipeline**: Retrieval → Re-ranking → Generation\n",
    "\n",
    "### Key Takeaways:\n",
    "- Re-ranking significantly improves result quality\n",
    "- Use bi-encoder for initial retrieval (fast, scalable)\n",
    "- Use cross-encoder for re-ranking (accurate, but slow)\n",
    "- Always re-rank a small candidate set (20-50 docs)\n",
    "\n",
    "### The Two-Stage Pattern:\n",
    "```\n",
    "All Documents (millions)\n",
    "        ↓\n",
    "  [Bi-Encoder]  ← Fast, approximate\n",
    "        ↓\n",
    "Top 20-50 Candidates\n",
    "        ↓\n",
    "  [Cross-Encoder] ← Slow, accurate\n",
    "        ↓\n",
    "Top 3-5 Results\n",
    "        ↓\n",
    "      [LLM]\n",
    "        ↓\n",
    "     Answer\n",
    "```\n",
    "\n",
    "### Next Up:\n",
    "**Query Expansion** - Generate multiple queries from one question for better recall!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
