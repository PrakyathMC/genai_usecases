{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb20d673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key loaded: True\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "print(f\"OpenAI API Key loaded: {api_key is not None}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f768b00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is ready to answer your questions!\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key = api_key)\n",
    "\n",
    "def ask_llm(question: str) ->str:\n",
    "    response = client.chat.completions.create(\n",
    "        model = \"gpt-3.5-turbo\",\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "        ],\n",
    "        temperature = 0.7,\n",
    "        max_tokens = 150\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "print(\"LLM is ready to answer your questions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e0683b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is FastAPI?\n",
      "\n",
      "Answer: FastAPI is a modern, fast (high-performance), web framework for building APIs with Python 3.6+ based on standard Python type hints. It is designed to be simple, intuitive, and easy to use, while also being highly efficient and scalable. FastAPI is built on top of Starlette for the web parts and Pydantic for the data parts, making it a powerful and feature-rich framework for building web applications and APIs.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is FastAPI?\"\n",
    "answer = ask_llm(question)\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "294edc4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API KEy loaded: True\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(f\"OpenAI API KEy loaded: {api_key is not None}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ea05bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is ready to answer your questions!\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "def ask_llm(question:str) ->str:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "        ],\n",
    "        temperature = 0.7,\n",
    "        max_tokens=100\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "print(\"LLM is ready to answer your questions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2f1ee72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the capital of France?\n",
      "\n",
      "Answer: The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the capital of France?\"\n",
    "answer = ask_llm(question)\n",
    "\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8e5ea7",
   "metadata": {},
   "source": [
    "# Using Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c8494012",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_classic.chains.retrieval_qa.base import RetrievalQA\n",
    "\n",
    "from typing import List\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6033acfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of documents: 1 docs\n",
      "Contents of first doc: Prakyath Chandran \n",
      "Bangalore, India \n",
      "cha..\n",
      "metadata of first doc: {'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2026-01-06T09:15:40+05:30', 'author': 'Un-named', 'moddate': '2026-01-06T09:15:40+05:30', 'source': 'Cover_Letter.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "loader = PyPDFLoader(\"Cover_Letter.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"Length of documents: {len(documents)} docs\")\n",
    "print(f\"Contents of first doc: {documents[0].page_content[:40]}..\")\n",
    "print(f\"metadata of first doc: {documents[0].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8f1d2e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks created: 2\n"
     ]
    }
   ],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 200,\n",
    "    length_function=len,\n",
    "    separators = [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "chunks = splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Number of chunks created: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6c1c5483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding vector length/Dimension: 384\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name = \"all-MiniLM-L6-v2\",\n",
    ")\n",
    "\n",
    "test_text = \"What is the name of the applicant?\"\n",
    "test_embeding = embeddings.embed_query(test_text)\n",
    "\n",
    "print(f\"Embedding vector length/Dimension: {len(test_embeding)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f9adabcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created successfully!\n",
      "Number of documents in vector store: 2\n"
     ]
    }
   ],
   "source": [
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"cover_letter_docs\"\n",
    ")\n",
    "\n",
    "print(\"Vector store created successfully!\")\n",
    "print(f\"Number of documents in vector store: {vectorstore._collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6b5b03b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What role is applicant applying for??\n",
      "\n",
      "Result 1:\n",
      "At HaiX AI, I benchmarked 5+ LLMs systematically, testing 100+ prompt variations and \n",
      "identifying cost-efficiency opportunities. I built FastAPI endpo...\n",
      "Source: Page 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What role is applicant applying for??\"\n",
    "results = vectorstore.similarity_search(query, k=1)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"Result {i}:\")\n",
    "    print(f\"{doc.page_content[:150]}...\")\n",
    "    print(f\"Source: Page {doc.metadata['page']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f9b428b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from LLM: The applicant is applying for a specific job or position at a company or organization. They may mention the job title or department they are interested in, and highlight their qualifications and experience that make them a strong candidate for the role.\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=150,\n",
    "    api_key= os.environ[\"OPENAI_API_KEY\"]\n",
    ")\n",
    "\n",
    "response = llm.invoke(\"What is applicant applying for in the cover letter?\")\n",
    "print(f\"Response from LLM: {response.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e2a65225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG chain is set up and ready to use!\n"
     ]
    }
   ],
   "source": [
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\":1}),\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "print(\"RAG chain is set up and ready to use!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d2cafa8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the tone of the cover letter?\n",
      "\n",
      "Answer:\n",
      "The tone of the cover letter is professional and enthusiastic.\n",
      "\n",
      "================================================================================\n",
      "Sources (1 chunks):\n",
      "\n",
      "1. Page 0:\n",
      "   Prakyath Chandran \n",
      "Bangalore, India \n",
      "chandranpraky...\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the tone of the cover letter?\"\n",
    "result = rag_chain.invoke(question)\n",
    "\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(f\"Answer:\\n{result['result']}\\n\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Sources ({len(result['source_documents'])} chunks):\")\n",
    "for i, doc in enumerate(result['source_documents'], 1):\n",
    "    print(f\"\\n{i}. Page {doc.metadata['page']}:\")\n",
    "    print(f\"   {doc.page_content[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c1b8fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLNotebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
