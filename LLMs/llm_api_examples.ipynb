{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9201e864",
   "metadata": {},
   "source": [
    "# LLM API Examples - The Big 3\n",
    "\n",
    "This notebook shows you how to call the three major LLM providers:\n",
    "1. **OpenAI** (GPT-4, GPT-4o, GPT-3.5)\n",
    "2. **Google Gemini** (Gemini 2.0, Gemini 1.5)\n",
    "3. **Anthropic Claude** (Claude 3.5 Sonnet, Claude Opus)\n",
    "\n",
    "## What are LLMs?\n",
    "Large Language Models (LLMs) are AI models that can:\n",
    "- Generate human-like text\n",
    "- Answer questions\n",
    "- Have conversations\n",
    "- Write code, summarize text, translate, and more\n",
    "\n",
    "## Learning Path Check ✅\n",
    "Learning in the right order:\n",
    "1. **Embeddings**  → Convert text to vectors for similarity/search\n",
    "2. **LLM APIs**  → Generate intelligent responses\n",
    "3. **Next: RAG** → Combine both (use embeddings to find context, LLM to generate answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28a8d93",
   "metadata": {},
   "source": [
    "## Setup: Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc6354f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run if you need to install:\n",
    "# !pip install openai google-genai anthropic python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55e1369",
   "metadata": {},
   "source": [
    "## Setup: API Keys\n",
    "\n",
    "Add these to your `.env` file:\n",
    "```\n",
    "OPENAI_API_KEY=sk-...\n",
    "GOOGLE_API_KEY=AI...\n",
    "ANTHROPIC_API_KEY=sk-ant-...\n",
    "```\n",
    "\n",
    "Get your API keys:\n",
    "- **OpenAI**: https://platform.openai.com/api-keys\n",
    "- **Google**: https://aistudio.google.com/apikey\n",
    "- **Anthropic**: https://console.anthropic.com/settings/keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026a8d8b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Example 1: OpenAI (GPT-4o)\n",
    "\n",
    "**Model:** `gpt-4o` (GPT-4 Optimized - fast, intelligent, multimodal)\n",
    "\n",
    "**Popular Models:**\n",
    "- `gpt-4o` - Best balance of speed and intelligence\n",
    "- `gpt-4o-mini` - Faster, cheaper, good for simple tasks\n",
    "- `gpt-3.5-turbo` - Cheapest, fastest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48084e67",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093ed119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI Response:\n",
      "Machine learning is a field of artificial intelligence that involves training algorithms to recognize patterns and make decisions based on data.\n",
      "\n",
      "Tokens used: 49\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize client\n",
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "# Simple chat completion\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain what machine learning is in one sentence.\"}\n",
    "    ],\n",
    "    temperature=0.7,  # 0-2, higher = more creative\n",
    "    max_tokens=100    # Maximum response length\n",
    ")\n",
    "\n",
    "# Extract the response\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "print(\"OpenAI Response:\")\n",
    "print(answer)\n",
    "print(f\"\\nTokens used: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c977bed6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Example 2: Google Gemini 2.0\n",
    "\n",
    "**Model:** `gemini-2.0-flash-exp` (Latest, experimental, fast)\n",
    "\n",
    "**Popular Models:**\n",
    "- `gemini-2.0-flash-exp` - Newest, fastest (experimental)\n",
    "- `gemini-1.5-pro` - Most capable, best reasoning\n",
    "- `gemini-1.5-flash` - Fast, cost-effective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5fe0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: models/embedding-gecko-001\n",
      "Model: models/gemini-2.5-flash\n",
      "Model: models/gemini-2.5-pro\n",
      "Model: models/gemini-2.0-flash-exp\n",
      "Model: models/gemini-2.0-flash\n",
      "Model: models/gemini-2.0-flash-001\n",
      "Model: models/gemini-2.0-flash-exp-image-generation\n",
      "Model: models/gemini-2.0-flash-lite-001\n",
      "Model: models/gemini-2.0-flash-lite\n",
      "Model: models/gemini-2.0-flash-lite-preview-02-05\n",
      "Model: models/gemini-2.0-flash-lite-preview\n",
      "Model: models/gemini-exp-1206\n",
      "Model: models/gemini-2.5-flash-preview-tts\n",
      "Model: models/gemini-2.5-pro-preview-tts\n",
      "Model: models/gemma-3-1b-it\n",
      "Model: models/gemma-3-4b-it\n",
      "Model: models/gemma-3-12b-it\n",
      "Model: models/gemma-3-27b-it\n",
      "Model: models/gemma-3n-e4b-it\n",
      "Model: models/gemma-3n-e2b-it\n",
      "Model: models/gemini-flash-latest\n",
      "Model: models/gemini-flash-lite-latest\n",
      "Model: models/gemini-pro-latest\n",
      "Model: models/gemini-2.5-flash-lite\n",
      "Model: models/gemini-2.5-flash-image-preview\n",
      "Model: models/gemini-2.5-flash-image\n",
      "Model: models/gemini-2.5-flash-preview-09-2025\n",
      "Model: models/gemini-2.5-flash-lite-preview-09-2025\n",
      "Model: models/gemini-3-pro-preview\n",
      "Model: models/gemini-3-flash-preview\n",
      "Model: models/gemini-3-pro-image-preview\n",
      "Model: models/nano-banana-pro-preview\n",
      "Model: models/gemini-robotics-er-1.5-preview\n",
      "Model: models/gemini-2.5-computer-use-preview-10-2025\n",
      "Model: models/deep-research-pro-preview-12-2025\n",
      "Model: models/embedding-001\n",
      "Model: models/text-embedding-004\n",
      "Model: models/gemini-embedding-exp-03-07\n",
      "Model: models/gemini-embedding-exp\n",
      "Model: models/gemini-embedding-001\n",
      "Model: models/aqa\n",
      "Model: models/imagen-4.0-generate-preview-06-06\n",
      "Model: models/imagen-4.0-ultra-generate-preview-06-06\n",
      "Model: models/imagen-4.0-generate-001\n",
      "Model: models/imagen-4.0-ultra-generate-001\n",
      "Model: models/imagen-4.0-fast-generate-001\n",
      "Model: models/veo-2.0-generate-001\n",
      "Model: models/veo-3.0-generate-001\n",
      "Model: models/veo-3.0-fast-generate-001\n",
      "Model: models/veo-3.1-generate-preview\n",
      "Model: models/veo-3.1-fast-generate-preview\n",
      "Model: models/gemini-2.5-flash-native-audio-latest\n",
      "Model: models/gemini-2.5-flash-native-audio-preview-09-2025\n",
      "Model: models/gemini-2.5-flash-native-audio-preview-12-2025\n"
     ]
    }
   ],
   "source": [
    "# code to identify currently available models\n",
    "\n",
    "from google import genai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize client\n",
    "client = genai.Client(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "\n",
    "models = client.models.list()\n",
    "for model in models:\n",
    "    print(f\"Model: {model.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17fe99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Gemini Response:\n",
      "Machine learning enables computers to learn from data, identify patterns, and make decisions or predictions without being explicitly programmed for every task.\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize client\n",
    "client = genai.Client(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "\n",
    "# Simple chat completion\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"Explain what machine learning is in one sentence.\",\n",
    "    config={\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_output_tokens\": 1000  # Increased from 100 to 1000\n",
    "    }\n",
    ")\n",
    "\n",
    "# Extract the response\n",
    "answer = response.text\n",
    "\n",
    "print(\"Google Gemini Response:\")\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2e7b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Gemini Response:\n",
      "Machine learning is a method where computers learn from data to identify patterns and make predictions or decisions, without being explicitly programmed for every task.\n",
      "\n",
      "Finish reason: STOP\n",
      "\n",
      "Tokens used:\n",
      "Input tokens: 10\n",
      "Output tokens: 27\n",
      "Total tokens: 994\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "client = genai.Client(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"Explain what machine learning is in one sentence.\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        temperature=0.7,\n",
    "        max_output_tokens=1000  # Increase this\n",
    "    )\n",
    ")\n",
    "\n",
    "answer = response.text\n",
    "print(\"Google Gemini Response:\")\n",
    "print(answer)\n",
    "\n",
    "# Check WHY it stopped\n",
    "print(f\"\\nFinish reason: {response.candidates[0].finish_reason}\")\n",
    "print(f\"\\nTokens used:\")\n",
    "print(f\"Input tokens: {response.usage_metadata.prompt_token_count}\")\n",
    "print(f\"Output tokens: {response.usage_metadata.candidates_token_count}\")\n",
    "print(f\"Total tokens: {response.usage_metadata.total_token_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa11fa8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Example 3: Anthropic Claude 3.5\n",
    "\n",
    "**Model:** `claude-3-5-sonnet-20241022` (Latest Sonnet - best for coding)\n",
    "\n",
    "**Popular Models:**\n",
    "- `claude-3-5-sonnet-20241022` - Best balance (this is what powers me!)\n",
    "- `claude-3-5-haiku-20241022` - Fastest, cheapest\n",
    "- `claude-opus-4` - Most intelligent (expensive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e36803",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize client\n",
    "client = Anthropic(api_key=os.environ[\"ANTHROPIC_API_KEY\"])\n",
    "\n",
    "# Simple chat completion\n",
    "response = client.messages.create(\n",
    "    model=\"claude-3-5-sonnet-20241022\",\n",
    "    max_tokens=100,  # Maximum response length\n",
    "    temperature=0.7, # 0-1, higher = more creative\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Explain what machine learning is in one sentence.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Extract the response\n",
    "answer = response.content[0].text\n",
    "\n",
    "print(\"Anthropic Claude Response:\")\n",
    "print(answer)\n",
    "print(f\"\\nTokens used: Input={response.usage.input_tokens}, Output={response.usage.output_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3718e3d7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Bonus: Conversation with Context (Multi-turn Chat)\n",
    "\n",
    "Real applications need to maintain conversation history. Here's how:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2800d95",
   "metadata": {},
   "source": [
    "## OpenAI Conversation Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2bf1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What is Python?\n",
      "Assistant: Python is a high-level, interpreted programming language known for its simplicity and readability, making it an excellent choice for beginners and experienced developers alike. Created by Guido van Rossum and first released in 1991, Python emphasizes clear and concise code, utilizing indentation to define code blocks rather than brackets or keywords.\n",
      "\n",
      "Python supports multiple programming paradigms, including procedural, object-oriented, and functional programming. It has a vast standard library and a thriving ecosystem of third-party packages, making it highly versatile for a wide range of applications. Some common use cases for Python include:\n",
      "\n",
      "1. **Web Development**: Frameworks like Django and Flask enable the creation of robust web applications.\n",
      "2. **Data Analysis and Machine Learning**: Libraries such as Pandas, NumPy, SciPy, and Scikit-learn empower scientists and analysts to process and analyze data efficiently.\n",
      "3. **Artificial Intelligence**: TensorFlow, Keras, and PyTorch are popular libraries for building machine learning and deep learning models.\n",
      "4. **Scripting and Automation**: Python's simplicity makes it ideal for writing scripts to automate repetitive tasks.\n",
      "5. **Scientific Computing**: Used in research and academic environments for simulations and computations.\n",
      "6. **Software Development**: Suitable for prototyping and developing applications in various fields.\n",
      "\n",
      "Python's popularity is also due to its strong community support, extensive documentation, and cross-platform compatibility, making it a go-to language for many programming projects.\n",
      "\n",
      "User: What are its main use cases?\n",
      "Assistant: Python is a versatile language with a wide range of use cases, thanks to its simplicity and the powerful libraries and frameworks available in its ecosystem. Some of the main use cases of Python include:\n",
      "\n",
      "1. **Web Development**: Python is a popular choice for web development due to frameworks like Django and Flask, which provide tools for building secure, efficient, and scalable web applications.\n",
      "\n",
      "2. **Data Analysis and Data Science**: Python is widely used in data analysis with libraries such as Pandas, NumPy, and SciPy. It allows for efficient data manipulation, analysis, and visualization, making it essential for data scientists.\n",
      "\n",
      "3. **Machine Learning and Artificial Intelligence**: Python is a leading language for machine learning and AI development. Tools like TensorFlow, Keras, and PyTorch allow developers to create complex models and algorithms for tasks such as image and speech recognition, natural language processing, and predictive analytics.\n",
      "\n",
      "4. **Scientific Computing**: Researchers and scientists use Python for scientific computing tasks. Libraries like SciPy and SymPy help with quantitative analysis, complex calculations, and simulations.\n",
      "\n",
      "5. **Automation and Scripting**: Its ease of use makes Python ideal for writing scripts to automate repetitive tasks or manage system operations, which is widely utilized in DevOps and system administration.\n",
      "\n",
      "6. **Software Development**: Python is often used to build applications' prototypes, given its rapid development time, and it's also used in developing software tools and applications in various domains.\n",
      "\n",
      "7. **Game Development**: While not as popular as other languages for game development, Python is still used for developing simple games and in engine scripting. Pygame is a library commonly used for this purpose.\n",
      "\n",
      "8. **Networking and Security**: Tools like Scapy and libraries such as Socket help in network scripting and security analysis, making Python a good choice for security professionals.\n",
      "\n",
      "9. **Internet of Things (IoT)**: Python is increasingly used in IoT development for prototyping and even deploying applications, often in conjunction with platforms like Raspberry Pi.\n",
      "\n",
      "10. **Education**: Python's simplicity and readability make it an excellent choice for teaching programming and computer science concepts to beginners.\n",
      "\n",
      "These use cases demonstrate Python's versatility and why it has become one of the most popular programming languages across different industries and applications.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "# Conversation history\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI tutor.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is Python?\"},\n",
    "]\n",
    "\n",
    "# First exchange\n",
    "response = client.chat.completions.create(model=\"gpt-4o\", messages=messages)\n",
    "assistant_reply = response.choices[0].message.content\n",
    "messages.append({\"role\": \"assistant\", \"content\": assistant_reply})\n",
    "\n",
    "print(\"User: What is Python?\")\n",
    "print(f\"Assistant: {assistant_reply}\\n\")\n",
    "\n",
    "# Follow-up question (model remembers context!)\n",
    "messages.append({\"role\": \"user\", \"content\": \"What are its main use cases?\"})\n",
    "response = client.chat.completions.create(model=\"gpt-4o\", messages=messages)\n",
    "assistant_reply = response.choices[0].message.content\n",
    "\n",
    "print(\"User: What are its main use cases?\")\n",
    "print(f\"Assistant: {assistant_reply}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b84928b",
   "metadata": {},
   "source": [
    "## Claude Conversation Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffbba8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "client = Anthropic(api_key=os.environ[\"ANTHROPIC_API_KEY\"])\n",
    "\n",
    "# Conversation history\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is Python?\"},\n",
    "]\n",
    "\n",
    "# First exchange\n",
    "response = client.messages.create(\n",
    "    model=\"claude-3-5-sonnet-20241022\",\n",
    "    max_tokens=200,\n",
    "    messages=messages\n",
    ")\n",
    "assistant_reply = response.content[0].text\n",
    "messages.append({\"role\": \"assistant\", \"content\": assistant_reply})\n",
    "\n",
    "print(\"User: What is Python?\")\n",
    "print(f\"Assistant: {assistant_reply}\\n\")\n",
    "\n",
    "# Follow-up question (model remembers context!)\n",
    "messages.append({\"role\": \"user\", \"content\": \"What are its main use cases?\"})\n",
    "response = client.messages.create(\n",
    "    model=\"claude-3-5-sonnet-20241022\",\n",
    "    max_tokens=200,\n",
    "    messages=messages\n",
    ")\n",
    "assistant_reply = response.content[0].text\n",
    "\n",
    "print(\"User: What are its main use cases?\")\n",
    "print(f\"Assistant: {assistant_reply}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c2a805",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Quick Reference\n",
    "\n",
    "## Common Parameters Explained\n",
    "\n",
    "| Parameter | Range | What it does |\n",
    "|-----------|-------|-------------|\n",
    "| **temperature** | 0-2 (OpenAI, Google)<br>0-1 (Claude) | Controls randomness:<br>• 0 = deterministic, focused<br>• 1+ = creative, varied |\n",
    "| **max_tokens** | 1-model limit | Maximum length of response |\n",
    "| **top_p** | 0-1 | Alternative to temperature (nucleus sampling) |\n",
    "| **messages** | Array | Conversation history (role + content) |\n",
    "\n",
    "## Message Roles\n",
    "\n",
    "- **system** (OpenAI only): Sets the AI's behavior/personality\n",
    "- **user**: Your questions/prompts\n",
    "- **assistant**: AI's responses\n",
    "\n",
    "## Cost Comparison (Approximate, Jan 2025)\n",
    "\n",
    "| Provider | Model | Input (per 1M tokens) | Output (per 1M tokens) |\n",
    "|----------|-------|----------------------|------------------------|\n",
    "| OpenAI | gpt-4o | $2.50 | $10.00 |\n",
    "| OpenAI | gpt-4o-mini | $0.15 | $0.60 |\n",
    "| Google | gemini-1.5-pro | $1.25 | $5.00 |\n",
    "| Google | gemini-1.5-flash | $0.075 | $0.30 |\n",
    "| Anthropic | claude-3.5-sonnet | $3.00 | $15.00 |\n",
    "| Anthropic | claude-3.5-haiku | $0.80 | $4.00 |\n",
    "\n",
    "## When to Use Which?\n",
    "\n",
    "- **OpenAI GPT-4o**: Best for general tasks, great multimodal support\n",
    "- **Google Gemini**: Great for free tier learning, good multilingual support\n",
    "- **Claude**: Best for coding tasks, long context (200K tokens), strong reasoning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de68cf29",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLNotebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
