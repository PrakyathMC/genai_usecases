{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Fundamentals: From Manual RAG to Framework\n",
    "\n",
    "\n",
    "**You just built:** A complete RAG system from scratch (8 steps, 200+ lines of code)\n",
    "\n",
    "**LangChain does:** The same thing in ~20 lines!\n",
    "\n",
    "## Why LangChain?\n",
    "\n",
    "### What You Built Manually:\n",
    "```python\n",
    "# Load PDF\n",
    "reader = PdfReader(pdf_path)\n",
    "pages = [page.extract_text() for page in reader.pages]\n",
    "\n",
    "# Chunk text\n",
    "chunks = chunk_text(pages, chunk_size=500)\n",
    "\n",
    "# Create embeddings + store\n",
    "collection.add(documents=chunks)\n",
    "\n",
    "# Retrieve + Generate\n",
    "retrieved = collection.query(question)\n",
    "answer = llm.generate(question, retrieved)\n",
    "```\n",
    "\n",
    "### With LangChain:\n",
    "```python\n",
    "# Load + chunk + embed + store\n",
    "loader = PyPDFLoader(\"file.pdf\")\n",
    "docs = loader.load_and_split()\n",
    "vectorstore = Chroma.from_documents(docs, embeddings)\n",
    "\n",
    "# Retrieve + generate (ONE LINE!)\n",
    "qa_chain = RetrievalQA.from_chain_type(llm, retriever=vectorstore.as_retriever())\n",
    "answer = qa_chain.invoke({\"query\": question})\n",
    "```\n",
    "\n",
    "**Same result, 80% less code!**\n",
    "\n",
    "## What is LangChain?\n",
    "\n",
    "**Framework for building LLM applications** with:\n",
    "- Pre-built components (loaders, splitters, retrievers)\n",
    "- Modular design (swap components easily)\n",
    "- Production patterns (error handling, monitoring)\n",
    "- Industry standard (used by thousands of companies)\n",
    "\n",
    "## Your Learning Path\n",
    "\n",
    "```\n",
    "Manual RAG (You built) ‚Üí LangChain (Now) ‚Üí Production Apps\n",
    "     ‚Üì                        ‚Üì                    ‚Üì\n",
    "Understanding           Speed & Scale      Real Products\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install LangChain and dependencies (uncomment if needed)\n",
    "# !pip install langchain langchain-openai langchain-community langchain-chroma pypdf chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment loaded\n",
      "API Key found: True\n",
      "Key starts with: sk-proj-3Q...\n",
      "‚úÖ Environment loaded\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "# Verify it loaded\n",
    "print(\"‚úÖ Environment loaded\")\n",
    "print(f\"API Key found: {'OPENAI_API_KEY' in os.environ}\")\n",
    "if 'OPENAI_API_KEY' in os.environ:\n",
    "    print(f\"Key starts with: {os.environ['OPENAI_API_KEY'][:10]}...\")\n",
    "print(\"‚úÖ Environment loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Core LangChain Components\n",
    "\n",
    "Let's learn the building blocks one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Component 1: Document Loaders\n",
    "\n",
    "**What you did manually:** Read PDF with PyPDF2, extract text, handle pages\n",
    "\n",
    "**LangChain way:** One line!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lapde\\anaconda3\\envs\\MLNotebook\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 8 pages\n",
      "\n",
      "Sample document:\n",
      "Content preview: @genieincodebottle \n",
      "Instagram | GitHub | Medium | YouTube \n",
      "How to Be Better Than Most in GenAI \n",
      " \n",
      "Contents \n",
      " \n",
      "Core LLM Building Blocks ....................................................................\n",
      "Metadata: {'producer': 'Microsoft¬Æ Word 2019', 'creator': 'Microsoft¬Æ Word 2019', 'creationdate': '2025-09-02T20:12:32+05:30', 'author': 'Rajesh Srivastava', 'moddate': '2025-09-02T20:12:32+05:30', 'source': 'llm_fundamentals.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load PDF (automatically handles pages, metadata, etc.)\n",
    "loader = PyPDFLoader(\"llm_fundamentals.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(documents)} pages\")\n",
    "print(f\"\\nSample document:\")\n",
    "print(f\"Content preview: {documents[0].page_content[:200]}...\")\n",
    "print(f\"Metadata: {documents[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened?**\n",
    "\n",
    "- ‚úÖ Loaded PDF\n",
    "- ‚úÖ Extracted text from all pages\n",
    "- ‚úÖ Created `Document` objects with metadata\n",
    "- ‚úÖ All in 2 lines!\n",
    "\n",
    "**Document object:** Standard format LangChain uses everywhere\n",
    "```python\n",
    "Document(\n",
    "    page_content=\"text here\",\n",
    "    metadata={\"source\": \"file.pdf\", \"page\": 1}\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Loaders (LangChain has 100+ loaders!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain supports 100+ data sources out of the box!\n"
     ]
    }
   ],
   "source": [
    "# Examples (don't run, just see the pattern)\n",
    "from langchain_community.document_loaders import (\n",
    "    TextLoader,           # .txt files\n",
    "    CSVLoader,            # .csv files\n",
    "    UnstructuredMarkdownLoader,  # .md files\n",
    "    WebBaseLoader,        # Web pages\n",
    "    DirectoryLoader,      # Entire folders\n",
    "    NotionDBLoader,       # Notion databases\n",
    "    SlackDirectoryLoader, # Slack messages\n",
    ")\n",
    "\n",
    "print(\"LangChain supports 100+ data sources out of the box!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Component 2: Text Splitters\n",
    "\n",
    "**What you did manually:** Custom `chunk_text()` function with overlap logic\n",
    "\n",
    "**LangChain way:** Pre-built splitters with best practices!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Split 8 pages into 37 chunks\n",
      "\n",
      "Sample chunk:\n",
      "5. Attention ‚Üí Highlights the most relevant tokens in context \n",
      "6. Self-Attention ‚Üí Each token attends to every other token for context \n",
      "7. Cross-Attention ‚Üí Connect encoder and decoder (in encoder-decoder models) \n",
      "8. Multi-Head Attention ‚Üí Several attention heads capture different patterns in parallel \n",
      "9. Feed-Forward Networks ‚Üí Nonlinear layers that transform representations between \n",
      "attention blocks \n",
      "10. Residual Connections ‚Üí Shortcut links that preserve signals and help gradient flow\n",
      "\n",
      "Metadata: {'producer': 'Microsoft¬Æ Word 2019', 'creator': 'Microsoft¬Æ Word 2019', 'creationdate': '2025-09-02T20:12:32+05:30', 'author': 'Rajesh Srivastava', 'moddate': '2025-09-02T20:12:32+05:30', 'source': 'llm_fundamentals.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Create splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,        # Max characters per chunk\n",
    "    chunk_overlap=50,      # Overlap between chunks\n",
    "    length_function=len,   # How to measure length\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # Try these in order\n",
    ")\n",
    "\n",
    "# Split documents\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"‚úÖ Split {len(documents)} pages into {len(chunks)} chunks\")\n",
    "print(f\"\\nSample chunk:\")\n",
    "print(chunks[5].page_content)\n",
    "print(f\"\\nMetadata: {chunks[5].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why RecursiveCharacterTextSplitter?**\n",
    "\n",
    "It tries separators in order:\n",
    "1. First try paragraph breaks (`\\n\\n`)\n",
    "2. Then sentence breaks (`\\n`)\n",
    "3. Then word breaks (` `)\n",
    "4. Finally characters if needed\n",
    "\n",
    "**Result:** Smart, context-preserving chunks!\n",
    "\n",
    "### Other Splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different splitters for different content types!\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import (\n",
    "    CharacterTextSplitter,     # Simple character-based\n",
    "    TokenTextSplitter,         # Token-based (for LLMs)\n",
    "    MarkdownTextSplitter,      # Markdown-aware\n",
    "    PythonCodeTextSplitter,    # Code-aware\n",
    ")\n",
    "\n",
    "print(\"Different splitters for different content types!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Component 3: Embeddings\n",
    "\n",
    "**What you did manually:** SentenceTransformer model, manual encoding\n",
    "\n",
    "**LangChain way:** Unified interface for any embedding model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embeddings model loaded\n",
      "Embedding dimensions: 384\n",
      "First 5 values: [-0.06979136914014816, 0.04351036995649338, 0.07203606516122818, -0.0365731455385685, -0.08926713466644287]\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Same model you used before!\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# Test it\n",
    "test_text = \"LangChain makes RAG development easy\"\n",
    "embedding_vector = embeddings.embed_query(test_text)\n",
    "\n",
    "print(f\"‚úÖ Embeddings model loaded\")\n",
    "print(f\"Embedding dimensions: {len(embedding_vector)}\")\n",
    "print(f\"First 5 values: {embedding_vector[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Swap Models Easily!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same interface, different providers - that's the power of LangChain!\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# Just change one line to switch providers!\n",
    "# embeddings = OpenAIEmbeddings()  # Use OpenAI instead\n",
    "# embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")  # Or Google\n",
    "\n",
    "print(\"Same interface, different providers - that's the power of LangChain!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Component 4: Vector Stores\n",
    "\n",
    "**What you did manually:** ChromaDB client, collections, manual add/query\n",
    "\n",
    "**LangChain way:** Unified interface for any vector store!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Vector store created with 37 chunks\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Create vector store from documents (one line!)\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"langchain_llm_fundamentals\"\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Vector store created with {vectorstore._collection.count()} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened?**\n",
    "\n",
    "1. ‚úÖ Created embeddings for all chunks\n",
    "2. ‚úÖ Stored in ChromaDB\n",
    "3. ‚úÖ Built similarity search index\n",
    "4. ‚úÖ All automatic!\n",
    "\n",
    "**Your manual version:** ~30 lines of code\n",
    "\n",
    "**LangChain:** 1 line!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is RAG?\n",
      "\n",
      "Result 1:\n",
      "17. ALiBi / Relative Positional Encoding ‚Üí Alternative to RoPE for long contexts \n",
      "18. Linear / Performer Attention ‚Üí Efficient attention variants for ...\n",
      "Source: Page 1\n",
      "\n",
      "Result 2:\n",
      "evaluation \n",
      "4. Human Evaluation ‚Üí Collect human judgments for accuracy, coherence, and safety \n",
      "5. Factuality / Truthfulness Metrics ‚Üí Specialized eval...\n",
      "Source: Page 5\n",
      "\n",
      "Result 3:\n",
      "@genieincodebottle \n",
      "Instagram | GitHub | Medium | YouTube \n",
      "How to Be Better Than Most in GenAI \n",
      " \n",
      "Contents \n",
      " \n",
      "Core LLM Building Blocks ..................\n",
      "Source: Page 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Search for relevant chunks\n",
    "query = \"What is RAG?\"\n",
    "results = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"Result {i}:\")\n",
    "    print(f\"{doc.page_content[:150]}...\")\n",
    "    print(f\"Source: Page {doc.metadata['page']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Swap Vector Stores Easily!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_pinecone import Pinecone\n",
    "# from langchain_community.vectorstores import FAISS, Weaviate\n",
    "\n",
    "# Same code, just change the import!\n",
    "# vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "# vectorstore = Pinecone.from_documents(chunks, embeddings)\n",
    "\n",
    "print(\"Change one line to switch vector databases - modularity!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Component 5: LLMs\n",
    "\n",
    "**What you did manually:** OpenAI client, manual API calls, prompt formatting\n",
    "\n",
    "**LangChain way:** Unified interface for all LLMs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM initialized\n",
      "Response: RAG, or Retrieval-Augmented Generation, is a machine learning approach that combines retrieval of relevant information from a knowledge base with generative models to produce more accurate and contextually relevant responses in natural language processing tasks.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.3,\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"]\n",
    ")\n",
    "\n",
    "# Test it\n",
    "response = llm.invoke(\"Explain RAG in one sentence\")\n",
    "\n",
    "print(\"‚úÖ LLM initialized\")\n",
    "print(f\"Response: {response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Swap LLMs Easily!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "# from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "# Just change one line!\n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
    "# llm = ChatAnthropic(model=\"claude-3-5-sonnet-20241022\")\n",
    "\n",
    "print(\"Same interface for OpenAI, Google, Claude, and 50+ other providers!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Building RAG with LangChain\n",
    "\n",
    "Now let's combine everything into a complete RAG system!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Using RetrievalQA Chain\n",
    "\n",
    "**The simplest way** - LangChain handles everything!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RAG chain created!\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.chains.retrieval_qa.base import RetrievalQA\n",
    "# Create QA chain (combines retriever + LLM)\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",  # \"stuff\" = put all context in one prompt\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ RAG chain created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is LoRA and why is it useful?\n",
      "\n",
      "Answer:\n",
      "LoRA (Low-Rank Adaptation) is a method that involves using parameter-efficient adapters for fine-tuning large models. It allows for the adaptation of models to specific tasks or domains without the need to retrain the entire model, making it a cost-effective and efficient approach. LoRA is particularly useful because it enables fine-tuning on modest hardware, which is beneficial for users who may not have access to extensive computational resources.\n",
      "\n",
      "================================================================================\n",
      "Sources (3 chunks):\n",
      "\n",
      "1. Page 2:\n",
      "   9. QLoRA ‚Üí LoRA + quantization, enabling fine-tuning of huge models on modest hardware \n",
      "10. PEFT ‚Üí F...\n",
      "\n",
      "2. Page 2:\n",
      "   3. Sharded / Distributed Training ‚Üí Scale across multiple GPUs/nodes \n",
      "4. Continual / Lifelong Learni...\n",
      "\n",
      "3. Page 0:\n",
      "   @genieincodebottle \n",
      "Instagram | GitHub | Medium | YouTube \n",
      "How to Be Better Than Most in GenAI \n",
      " \n",
      "Co...\n"
     ]
    }
   ],
   "source": [
    "# Ask a question\n",
    "question = \"What is LoRA and why is it useful?\"\n",
    "result = qa_chain.invoke({\"query\": question})\n",
    "\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(f\"Answer:\\n{result['result']}\\n\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Sources ({len(result['source_documents'])} chunks):\")\n",
    "for i, doc in enumerate(result['source_documents'], 1):\n",
    "    print(f\"\\n{i}. Page {doc.metadata['page']}:\")\n",
    "    print(f\"   {doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare this to your manual code:**\n",
    "\n",
    "```python\n",
    "# Your manual RAG (simplified)\n",
    "retrieved = collection.query(question)\n",
    "context = \"\\n\".join([doc for doc, _ in retrieved])\n",
    "prompt = f\"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "response = openai_client.chat.completions.create(...)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "# LangChain\n",
    "answer = qa_chain.invoke({\"query\": question})\n",
    "```\n",
    "\n",
    "**Same result, way cleaner!** ‚ú®"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Using LCEL (LangChain Expression Language)\n",
    "\n",
    "**More control** - Build your own chain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Custom RAG chain created with LCEL!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Custom prompt template\n",
    "template = \"\"\"You are an AI assistant helping users understand LLM fundamentals.\n",
    "Answer the question based ONLY on the provided context. Cite page numbers when possible.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Helper function to format retrieved docs\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([\n",
    "        f\"[Page {doc.metadata['page']}]\\n{doc.page_content}\"\n",
    "        for doc in docs\n",
    "    ])\n",
    "\n",
    "# Build the chain using LCEL\n",
    "rag_chain = (\n",
    "    {\"context\": vectorstore.as_retriever() | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Custom RAG chain created with LCEL!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is attention mechanism?\n",
      "\n",
      "Answer:\n",
      "The attention mechanism highlights the most relevant tokens in context, allowing the model to focus on specific parts of the input when generating outputs. It enables each token to attend to every other token for context (self-attention) and connects the encoder and decoder in encoder-decoder models (cross-attention). Additionally, multi-head attention captures different patterns in parallel by using several attention heads. This mechanism is crucial for improving the model's ability to understand and generate language effectively. (Page 1)\n"
     ]
    }
   ],
   "source": [
    "# Use the custom chain\n",
    "question = \"What is attention mechanism?\"\n",
    "answer = rag_chain.invoke(question)\n",
    "\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(f\"Answer:\\n{answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LCEL (|) Explained:**\n",
    "\n",
    "```python\n",
    "# The pipe (|) chains operations\n",
    "chain = step1 | step2 | step3 | step4\n",
    "\n",
    "# Same as:\n",
    "result = step1(input)\n",
    "result = step2(result)\n",
    "result = step3(result)\n",
    "output = step4(result)\n",
    "```\n",
    "\n",
    "**Our chain:**\n",
    "```python\n",
    "question ‚Üí retriever ‚Üí format ‚Üí prompt ‚Üí llm ‚Üí parse ‚Üí answer\n",
    "```\n",
    "\n",
    "Clean, readable, composable! üéØ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Advanced RAG Features\n",
    "\n",
    "LangChain makes advanced techniques easy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature 1: Conversational RAG (with Memory)\n",
    "\n",
    "**Remember previous questions!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Conversational RAG chain created with memory!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lapde\\AppData\\Local\\Temp\\ipykernel_42816\\3319698367.py:5: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.chains import ConversationalRetrievalChain\n",
    "from langchain_classic.memory import ConversationBufferMemory\n",
    "\n",
    "# Create memory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    "    output_key=\"answer\"\n",
    ")\n",
    "\n",
    "# Create conversational chain\n",
    "conversational_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
    "    memory=memory,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Conversational RAG chain created with memory!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: What is LoRA?\n",
      "A1: LoRA (Low-Rank Adaptation) is a method used in machine learning that allows for the fine-tuning of large models by updating only a small number of parameters. This approach helps to make the fine-tuning process more efficient, particularly when working with large models on limited hardware.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Q2: What's the difference between that and QLoRA?\n",
      "A2: LoRA (Low-Rank Adaptation) is a method that allows for the efficient fine-tuning of large models by updating only a small number of parameters. QLoRA (Quantized LoRA) builds on this by incorporating quantization, which enables the fine-tuning of huge models on more modest hardware. Essentially, QLoRA combines the principles of LoRA with quantization techniques to make the process more resource-efficient.\n",
      "\n",
      "================================================================================\n",
      "Notice: The model knew 'that' = LoRA from previous question!\n"
     ]
    }
   ],
   "source": [
    "# First question\n",
    "result1 = conversational_chain.invoke({\"question\": \"What is LoRA?\"})\n",
    "print(\"Q1: What is LoRA?\")\n",
    "print(f\"A1: {result1['answer']}\\n\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Follow-up question (uses context from previous!)\n",
    "result2 = conversational_chain.invoke({\"question\": \"What's the difference between that and QLoRA?\"})\n",
    "print(\"Q2: What's the difference between that and QLoRA?\")\n",
    "print(f\"A2: {result2['answer']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Notice: The model knew 'that' = LoRA from previous question!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Without memory:**\n",
    "- \"What's the difference between that and QLoRA?\" ‚Üí Doesn't know what \"that\" is\n",
    "\n",
    "**With memory:**\n",
    "- Remembers \"that\" = LoRA from previous question!\n",
    "\n",
    "**Perfect for chatbots!** üí¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature 2: Multiple Retrieval Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ MMR Retrieval (diverse results):\n",
      "1. Page 2: 3. Sharded / Distributed Training ‚Üí Scale across multiple GPUs/nodes \n",
      "4. Continu...\n",
      "2. Page 3: 3. Top-k / Top-p ‚Üí Sampling filters, Higher = safer, looser = more diverse \n",
      "4. R...\n",
      "3. Page 0: Training & Tuning .................................................................\n"
     ]
    }
   ],
   "source": [
    "# 1. Similarity Search (default)\n",
    "retriever_similarity = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "# 2. MMR (Maximum Marginal Relevance) - diverse results\n",
    "retriever_mmr = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 3, \"fetch_k\": 10})\n",
    "\n",
    "# 3. Similarity with score threshold\n",
    "retriever_threshold = vectorstore.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\"score_threshold\": 0.5}\n",
    ")\n",
    "\n",
    "# Test MMR (gets diverse results)\n",
    "docs_mmr = retriever_mmr.invoke(\"What is fine-tuning?\")\n",
    "print(\"‚úÖ MMR Retrieval (diverse results):\")\n",
    "for i, doc in enumerate(docs_mmr, 1):\n",
    "    print(f\"{i}. Page {doc.metadata['page']}: {doc.page_content[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Retrieval Strategies:**\n",
    "\n",
    "| Strategy | Best For |\n",
    "|----------|----------|\n",
    "| **similarity** | Most relevant results |\n",
    "| **mmr** | Diverse results (avoid duplicates) |\n",
    "| **similarity_score_threshold** | Only high-confidence matches |\n",
    "\n",
    "**Swap with one parameter!** üîÑ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature 3: Document Metadata Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results from page 2 only:\n",
      "Page 2: 3. Sharded / Distributed Training ‚Üí Scale across multiple GPUs/nodes \n",
      "4. Continual / Lifelong Learni...\n",
      "\n",
      "Page 2: 9. QLoRA ‚Üí LoRA + quantization, enabling fine-tuning of huge models on modest hardware \n",
      "10. PEFT ‚Üí F...\n",
      "\n",
      "Page 2: 15. Distillation ‚Üí Transfer knowledge from a large model into a smaller one \n",
      "16. Gradient Descent & ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Search only specific pages\n",
    "docs_filtered = vectorstore.similarity_search(\n",
    "    \"What is attention?\",\n",
    "    k=3,\n",
    "    filter={\"page\": 2}  # Only search page 2\n",
    ")\n",
    "\n",
    "print(\"Results from page 2 only:\")\n",
    "for doc in docs_filtered:\n",
    "    print(f\"Page {doc.metadata['page']}: {doc.page_content[:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use cases:**\n",
    "- Search only recent documents (filter by date)\n",
    "- Search specific sections (filter by chapter)\n",
    "- User-specific data (filter by user_id)\n",
    "\n",
    "**Production essential!** üéØ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Complete LangChain RAG Class\n",
    "\n",
    "Let's build a production-ready RAG system using LangChain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LangChainRAG class defined\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "class LangChainRAG:\n",
    "    \"\"\"\n",
    "    Production RAG system powered by LangChain.\n",
    "    \n",
    "    Why LangChain?\n",
    "        - 10x less code than manual implementation\n",
    "        - Easy to swap components (LLMs, vector stores, embeddings)\n",
    "        - Built-in features (memory, streaming, error handling)\n",
    "        - Production-tested by thousands of companies\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        pdf_path: str,\n",
    "        llm_model: str = \"gpt-4o-mini\",\n",
    "        embedding_model: str = \"all-MiniLM-L6-v2\",\n",
    "        chunk_size: int = 500,\n",
    "        chunk_overlap: int = 50\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize RAG system from a PDF.\n",
    "        \n",
    "        Args:\n",
    "            pdf_path: Path to PDF file\n",
    "            llm_model: OpenAI model name\n",
    "            embedding_model: HuggingFace embedding model\n",
    "            chunk_size: Characters per chunk\n",
    "            chunk_overlap: Overlap between chunks\n",
    "        \"\"\"\n",
    "        print(\"Initializing LangChain RAG system...\")\n",
    "        \n",
    "        # Load documents\n",
    "        print(f\"Loading PDF: {pdf_path}\")\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        documents = loader.load()\n",
    "        \n",
    "        # Split documents\n",
    "        print(f\"Splitting into chunks (size={chunk_size}, overlap={chunk_overlap})\")\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap\n",
    "        )\n",
    "        self.chunks = text_splitter.split_documents(documents)\n",
    "        \n",
    "        # Setup embeddings\n",
    "        print(f\"Loading embedding model: {embedding_model}\")\n",
    "        self.embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n",
    "        \n",
    "        # Create vector store\n",
    "        print(\"Creating vector store...\")\n",
    "        self.vectorstore = Chroma.from_documents(\n",
    "            documents=self.chunks,\n",
    "            embedding=self.embeddings,\n",
    "            collection_name=\"langchain_rag\"\n",
    "        )\n",
    "        \n",
    "        # Setup LLM\n",
    "        print(f\"Initializing LLM: {llm_model}\")\n",
    "        self.llm = ChatOpenAI(\n",
    "            model=llm_model,\n",
    "            temperature=0.3,\n",
    "            api_key=os.environ[\"OPENAI_API_KEY\"]\n",
    "        )\n",
    "        \n",
    "        # Create QA chain\n",
    "        self.qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=self.llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=self.vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
    "            return_source_documents=True\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ RAG system ready! ({len(self.chunks)} chunks indexed)\\n\")\n",
    "    \n",
    "    def ask(self, question: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Ask a question and get an answer with sources.\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with answer and source information\n",
    "        \"\"\"\n",
    "        result = self.qa_chain.invoke({\"query\": question})\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": result['result'],\n",
    "            \"sources\": [\n",
    "                {\n",
    "                    \"page\": doc.metadata.get('page', 'N/A'),\n",
    "                    \"text\": doc.page_content[:150] + \"...\"\n",
    "                }\n",
    "                for doc in result['source_documents']\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def ask_multiple(self, questions: List[str]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Ask multiple questions at once.\n",
    "        \n",
    "        Args:\n",
    "            questions: List of questions\n",
    "            \n",
    "        Returns:\n",
    "            List of results\n",
    "        \"\"\"\n",
    "        return [self.ask(q) for q in questions]\n",
    "\n",
    "print(\"‚úÖ LangChainRAG class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the LangChain RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing LangChain RAG system...\n",
      "Loading PDF: llm_fundamentals.pdf\n",
      "Splitting into chunks (size=500, overlap=50)\n",
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "Creating vector store...\n",
      "Initializing LLM: gpt-4o-mini\n",
      "‚úÖ RAG system ready! (37 chunks indexed)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize (does everything automatically!)\n",
    "rag = LangChainRAG(\n",
    "    pdf_path=\"llm_fundamentals.pdf\",\n",
    "    llm_model=\"gpt-4o-mini\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are the main components of transformer architecture?\n",
      "\n",
      "Answer:\n",
      "The main components of transformer architecture include:\n",
      "\n",
      "1. **Encoder and Decoder**: The architecture consists of an encoder that processes the input and a decoder that generates the output. Some models use only the encoder (like BERT), while others use only the decoder (like GPT) or both (like T5).\n",
      "\n",
      "2. **Self-Attention Mechanism**: This allows the model to weigh the importance of different words in a sentence when encoding or decoding.\n",
      "\n",
      "3. **Feedforward Neural Networks**: Each layer of the encoder and decoder contains a feedforward neural network that processes the output of the self-attention mechanism.\n",
      "\n",
      "4. **Layer Normalization**: This is applied to stabilize and speed up the training process.\n",
      "\n",
      "5. **Positional Encoding**: Since transformers do not have a built-in sense of order, positional encoding is added to the input embeddings to provide information about the position of tokens in the sequence.\n",
      "\n",
      "6. **Multi-Head Attention**: This allows the model to focus on different parts of the input sequence simultaneously, enhancing its ability to capture complex relationships.\n",
      "\n",
      "These components work together to enable the transformer architecture to effectively process and generate sequences of data.\n",
      "\n",
      "================================================================================\n",
      "Sources (3 chunks):\n",
      "\n",
      "1. Page 2:\n",
      "   @genieincodebottle \n",
      "Advanced Architectures \n",
      "1. Diffusion Models ‚Üí Generate images/video by learning to reverse noise process (DALL-E, \n",
      "Midjourney, Sta...\n",
      "\n",
      "2. Page 2:\n",
      "   sequences \n",
      "5. Autoregressive Models ‚Üí Generate sequences one token at a time (GPT family, Claude, \n",
      "Gemini etc) \n",
      "6. Flow-Based Models ‚Üí Invertible neur...\n",
      "\n",
      "3. Page 1:\n",
      "   @genieincodebottle \n",
      "Core LLM Building Blocks \n",
      "1. Transformer Architecture ‚Üí Core design (encoder-only like BERT, decoder-only like \n",
      "GPT/Claude/Gemini ...\n"
     ]
    }
   ],
   "source": [
    "# Ask questions\n",
    "result = rag.ask(\"What are the main components of transformer architecture?\")\n",
    "\n",
    "print(f\"Question: {result['question']}\\n\")\n",
    "print(f\"Answer:\\n{result['answer']}\\n\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Sources ({len(result['sources'])} chunks):\")\n",
    "for i, source in enumerate(result['sources'], 1):\n",
    "    print(f\"\\n{i}. Page {source['page']}:\")\n",
    "    print(f\"   {source['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Q: What is RLHF?\n",
      "A: RLHF stands for Reinforcement Learning from Human Feedback. It is a method used to align model outputs with human preferences by incorporating feedback from humans into the training process. This approach helps ensure that the model's responses are more in line with what users expect or prefer.\n",
      "Sources: Pages [3, 2, 7]\n",
      "\n",
      "================================================================================\n",
      "Q: Explain quantization\n",
      "A: Quantization is a technique used in machine learning and neural networks to reduce the precision of the numbers used to represent model parameters and activations. This process involves converting floating-point numbers (which typically use 32 bits) into lower-bit representations, such as 16-bit or 8-bit integers. The main goals of quantization are to decrease the model size, reduce memory bandwidth requirements, and improve inference speed, especially on hardware with limited computational resources.\n",
      "\n",
      "There are different types of quantization, including:\n",
      "\n",
      "1. **Post-training quantization**: This is applied after the model has been trained, where the weights and activations are quantized without further training.\n",
      "2. **Quantization-aware training (QAT)**: This involves training the model with quantization in mind, allowing it to learn to maintain accuracy despite the reduced precision.\n",
      "\n",
      "By using quantization, models can be more efficient while still retaining a level of accuracy that is acceptable for many applications.\n",
      "Sources: Pages [5, 2, 1]\n",
      "\n",
      "================================================================================\n",
      "Q: What are vector databases?\n",
      "A: I don't know.\n",
      "Sources: Pages [4, 2, 5]\n"
     ]
    }
   ],
   "source": [
    "# Multiple questions\n",
    "questions = [\n",
    "    \"What is RLHF?\",\n",
    "    \"Explain quantization\",\n",
    "    \"What are vector databases?\"\n",
    "]\n",
    "\n",
    "results = rag.ask_multiple(questions)\n",
    "\n",
    "for result in results:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Q: {result['question']}\")\n",
    "    print(f\"A: {result['answer']}\")\n",
    "    print(f\"Sources: Pages {[s['page'] for s in result['sources']]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Comparison - Manual vs LangChain\n",
    "\n",
    "## Code Comparison\n",
    "\n",
    "### Your Manual RAG:\n",
    "```python\n",
    "# ~200+ lines of code\n",
    "\n",
    "# Load PDF\n",
    "reader = PdfReader(pdf_path)\n",
    "pages = [...]\n",
    "\n",
    "# Chunk\n",
    "def chunk_text(...):\n",
    "    # 20 lines of logic\n",
    "chunks = chunk_text(pages)\n",
    "\n",
    "# Embeddings + Store\n",
    "model = SentenceTransformer(...)\n",
    "client = chromadb.Client()\n",
    "collection = client.create_collection(...)\n",
    "collection.add(...)\n",
    "\n",
    "# Retrieve + Generate\n",
    "def retrieve(...):\n",
    "    # 15 lines\n",
    "def generate(...):\n",
    "    # 25 lines\n",
    "\n",
    "class SimpleRAG:\n",
    "    # 100+ lines\n",
    "```\n",
    "\n",
    "### LangChain RAG:\n",
    "```python\n",
    "# ~50 lines of code\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Load + Split\n",
    "loader = PyPDFLoader(\"file.pdf\")\n",
    "docs = loader.load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "# Embed + Store\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "vectorstore = Chroma.from_documents(chunks, embeddings)\n",
    "\n",
    "# LLM + Chain\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")\n",
    "\n",
    "# Use it\n",
    "result = qa_chain.invoke({\"query\": \"question\"})\n",
    "```\n",
    "\n",
    "**75% less code!** üéâ\n",
    "\n",
    "## Feature Comparison\n",
    "\n",
    "| Feature | Manual | LangChain |\n",
    "|---------|--------|------------|\n",
    "| **Lines of code** | 200+ | ~50 |\n",
    "| **PDF loading** | Manual | ‚úÖ Built-in |\n",
    "| **Text splitting** | Custom logic | ‚úÖ Pre-built splitters |\n",
    "| **Embeddings** | Manual encode | ‚úÖ Unified interface |\n",
    "| **Vector store** | Manual setup | ‚úÖ One-liner |\n",
    "| **Retrieval** | Custom similarity | ‚úÖ Multiple strategies |\n",
    "| **Generation** | Manual prompts | ‚úÖ Chains |\n",
    "| **Memory** | None | ‚úÖ Built-in |\n",
    "| **Swap components** | Rewrite code | ‚úÖ Change one line |\n",
    "| **Error handling** | Manual | ‚úÖ Built-in |\n",
    "| **Streaming** | Manual | ‚úÖ Built-in |\n",
    "| **Monitoring** | Manual | ‚úÖ LangSmith |\n",
    "\n",
    "## When to Use Each?\n",
    "\n",
    "### Use Manual RAG when:\n",
    "- ‚úÖ Learning fundamentals (you did this!)\n",
    "- ‚úÖ Need complete control\n",
    "- ‚úÖ Very simple, specific use case\n",
    "\n",
    "### Use LangChain when:\n",
    "- ‚úÖ **Production applications**\n",
    "- ‚úÖ **Need to iterate fast**\n",
    "- ‚úÖ **Want to swap components easily**\n",
    "- ‚úÖ **Team collaboration**\n",
    "- ‚úÖ **Most real-world projects**\n",
    "\n",
    "**90% of the time ‚Üí Use LangChain!** üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary: Why LangChain?\n",
    "\n",
    "## What You Learned\n",
    "\n",
    "### Core Components:\n",
    "1. ‚úÖ **Document Loaders** - PDF, CSV, Web, 100+ sources\n",
    "2. ‚úÖ **Text Splitters** - Smart chunking strategies\n",
    "3. ‚úÖ **Embeddings** - Unified interface for any model\n",
    "4. ‚úÖ **Vector Stores** - ChromaDB, FAISS, Pinecone, etc.\n",
    "5. ‚úÖ **LLMs** - OpenAI, Google, Claude, 50+ providers\n",
    "6. ‚úÖ **Chains** - RetrievalQA, ConversationalRetrievalChain\n",
    "7. ‚úÖ **Memory** - Conversation history\n",
    "8. ‚úÖ **LCEL** - Composable chains with `|`\n",
    "\n",
    "### Key Benefits:\n",
    "\n",
    "‚úÖ **75% less code** - Focus on logic, not boilerplate  \n",
    "‚úÖ **Modular** - Swap any component easily  \n",
    "‚úÖ **Production-tested** - Used by thousands of companies  \n",
    "‚úÖ **Well-documented** - Great community support  \n",
    "‚úÖ **Fast iteration** - Try ideas quickly  \n",
    "‚úÖ **Built-in features** - Memory, streaming, monitoring  \n",
    "\n",
    "## Your Learning Journey\n",
    "\n",
    "```\n",
    "‚úÖ Day 1: Embeddings (manual)\n",
    "‚úÖ Day 2: LLM APIs (manual)\n",
    "‚úÖ Day 3: Basic RAG (manual)\n",
    "‚úÖ Day 4: Production RAG (manual)\n",
    "‚úÖ Day 5: LangChain (framework) ‚Üê You are here!\n",
    "```\n",
    "\n",
    "**You now know:**\n",
    "1. ‚úÖ How RAG works under the hood (manual implementation)\n",
    "2. ‚úÖ How to build production RAG fast (LangChain)\n",
    "\n",
    "**This is powerful!** Most people only know #2. You know both! üí™\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. ‚úÖ **Practice** - Build a LangChain RAG with your own PDFs\n",
    "2. üîú **Advanced RAG** - Multi-query, parent-child chunks, hybrid search\n",
    "3. üîú **LangChain Agents** - LLMs that use tools and make decisions\n",
    "4. üîú **LangSmith** - Monitor and debug LangChain apps\n",
    "5. üîú **Deploy** - Build a web UI (Streamlit) or API (FastAPI)\n",
    "\n",
    "## Practice Exercise\n",
    "\n",
    "**Challenge:** Build a multi-document RAG system\n",
    "\n",
    "1. Load 3 different PDFs\n",
    "2. Store them in the same vector store\n",
    "3. Add metadata to track which PDF each chunk came from\n",
    "4. Allow users to filter by source document\n",
    "\n",
    "**Hint:** Use `DirectoryLoader` and metadata filtering!\n",
    "\n",
    "---\n",
    "\n",
    "## You're Now a LangChain Developer! üéâ\n",
    "\n",
    "You can:\n",
    "- ‚úÖ Build RAG systems in minutes (not hours)\n",
    "- ‚úÖ Swap LLMs, embeddings, vector stores easily\n",
    "- ‚úÖ Add memory and conversation history\n",
    "- ‚úÖ Use production-ready patterns\n",
    "- ‚úÖ Understand what's happening under the hood\n",
    "\n",
    "**That last point is crucial** - because you built RAG manually first, you're not just using LangChain blindly. You understand every component! üß†\n",
    "\n",
    "Keep building! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLNotebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
