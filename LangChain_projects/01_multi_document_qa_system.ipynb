{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Document QA System\n",
    "\n",
    "## Project Overview\n",
    "This project builds a Question-Answering system that can query across multiple PDF documents.\n",
    "It allows you to:\n",
    "- Load multiple PDF files simultaneously\n",
    "- Query information across all documents\n",
    "- Get answers with source attribution (which document and page)\n",
    "- Compare information from different sources\n",
    "\n",
    "## Use Cases\n",
    "- Research: Compare information across multiple research papers\n",
    "- Legal: Search through multiple contracts or legal documents\n",
    "- Business: Analyze multiple reports or policies\n",
    "\n",
    "## What is Learnt\n",
    "1. Loading multiple documents with metadata\n",
    "2. Combining documents from different sources in one vector store\n",
    "3. Source attribution and tracking\n",
    "4. Cross-document information retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment loaded\n",
      "OpenAI API Key found: True\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "print(\"‚úÖ Environment loaded\")\n",
    "print(f\"OpenAI API Key found: {'OPENAI_API_KEY' in os.environ}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Document loading and processing\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Embeddings and vector store\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# LLM and chains\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_classic.chains.retrieval_qa.base import RetrievalQA\n",
    "\n",
    "# Utilities\n",
    "from typing import List\n",
    "import glob\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Multiple PDF Documents\n",
    "\n",
    "For this demo, we'll load all PDF files from a directory.\n",
    "Each document will retain its source file information in metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 8 pages from llm_fundamentals.pdf\n",
      "‚úÖ Loaded 1 pages from Cover_Letter.pdf\n",
      "‚úÖ Loaded 1 pages from prakyath_resume.pdf\n",
      "\n",
      "üìö Total documents loaded: 10 pages from 3 files\n"
     ]
    }
   ],
   "source": [
    "def load_multiple_pdfs(pdf_paths: List[str]):\n",
    "    \"\"\"\n",
    "    Load multiple PDF files and combine their documents.\n",
    "    \n",
    "    Args:\n",
    "        pdf_paths: List of file paths to PDF documents\n",
    "        \n",
    "    Returns:\n",
    "        List of document objects with metadata\n",
    "    \"\"\"\n",
    "    all_documents = []\n",
    "    \n",
    "    for pdf_path in pdf_paths:\n",
    "        # Load each PDF\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        documents = loader.load()\n",
    "        \n",
    "        # Add custom metadata to track source file\n",
    "        for doc in documents:\n",
    "            # Extract just the filename (not full path)\n",
    "            doc.metadata['source_file'] = os.path.basename(pdf_path)\n",
    "        \n",
    "        all_documents.extend(documents)\n",
    "        print(f\"‚úÖ Loaded {len(documents)} pages from {os.path.basename(pdf_path)}\")\n",
    "    \n",
    "    return all_documents\n",
    "\n",
    "# Example: Load PDFs from the parent RAG folder\n",
    "# Modify this list to include your PDF files\n",
    "pdf_files = [\n",
    "    \"llm_fundamentals.pdf\",\n",
    "    # Add more PDF paths here\n",
    "    \"Cover_Letter.pdf\",\n",
    "    \"prakyath_resume.pdf\",\n",
    "]\n",
    "\n",
    "# Load all documents\n",
    "all_documents = load_multiple_pdfs(pdf_files)\n",
    "print(f\"\\nüìö Total documents loaded: {len(all_documents)} pages from {len(pdf_files)} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Split Documents into Chunks\n",
    "\n",
    "We split the documents while preserving the source metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Split 10 pages into 50 chunks\n",
      "\n",
      "Sample chunk with metadata:\n",
      "Source File: llm_fundamentals.pdf\n",
      "Page: 0\n",
      "Content Preview: @genieincodebottle \n",
      "Instagram | GitHub | Medium | YouTube \n",
      "How to Be Better Than Most in GenAI \n",
      " \n",
      "Co...\n"
     ]
    }
   ],
   "source": [
    "# Initialize text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,        # Size of each chunk\n",
    "    chunk_overlap=50,      # Overlap between chunks to maintain context\n",
    "    length_function=len,   # Function to measure chunk length\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # Split on paragraphs first, then sentences\n",
    ")\n",
    "\n",
    "# Split all documents\n",
    "chunks = text_splitter.split_documents(all_documents)\n",
    "\n",
    "print(f\"‚úÖ Split {len(all_documents)} pages into {len(chunks)} chunks\")\n",
    "print(f\"\\nSample chunk with metadata:\")\n",
    "print(f\"Source File: {chunks[0].metadata.get('source_file', 'Unknown')}\")\n",
    "print(f\"Page: {chunks[0].metadata.get('page', 'Unknown')}\")\n",
    "print(f\"Content Preview: {chunks[0].page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create Embeddings and Vector Store\n",
    "\n",
    "Store all chunks in a single vector store for unified search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Vector store created with 50 chunks\n",
      "   Chunks from 3 different documents\n"
     ]
    }
   ],
   "source": [
    "# Initialize embedding model\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# Create vector store from all chunks\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"multi_document_collection\"\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Vector store created with {vectorstore._collection.count()} chunks\")\n",
    "print(f\"   Chunks from {len(pdf_files)} different documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Initialize LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM initialized\n"
     ]
    }
   ],
   "source": [
    "# Initialize OpenAI LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.7,  # Balanced creativity and accuracy\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"]\n",
    ")\n",
    "\n",
    "print(\"‚úÖ LLM initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Create Multi-Document QA Chain\n",
    "\n",
    "This chain will retrieve relevant chunks from ANY of the loaded documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Multi-Document QA Chain created\n"
     ]
    }
   ],
   "source": [
    "# Create QA chain with MMR retrieval for diverse results\n",
    "multi_doc_qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",  # Put all retrieved context in the prompt\n",
    "    retriever=vectorstore.as_retriever(\n",
    "        search_type=\"mmr\",  # Maximum Marginal Relevance for diverse results\n",
    "        search_kwargs={\n",
    "            \"k\": 5,          # Return top 5 chunks\n",
    "            \"fetch_k\": 20    # Consider top 20 for diversity selection\n",
    "        }\n",
    "    ),\n",
    "    return_source_documents=True  # Return source chunks for attribution\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Multi-Document QA Chain created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Query Across Multiple Documents\n",
    "\n",
    "Now we can ask questions and get answers from any of the loaded documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "QUESTION: What is LoRA and how does it work?\n",
      "================================================================================\n",
      "\n",
      "ANSWER:\n",
      "LoRA (Low-Rank Adaptation) is a method used to fine-tune large language models efficiently. It works by adding low-rank matrices to the original model's weights, allowing for the update of only a small portion of the model during training. This approach reduces the computational resources required for fine-tuning by focusing on a smaller set of parameters, making it possible to adapt large models even on modest hardware.\n",
      "\n",
      "================================================================================\n",
      "SOURCES (5 chunks):\n",
      "================================================================================\n",
      "\n",
      "Source 1: llm_fundamentals.pdf (Page 2)\n",
      "   Content: 9. QLoRA ‚Üí LoRA + quantization, enabling fine-tuning of huge models on modest hardware \n",
      "10. PEFT ‚Üí Family of methods (e.g., LoRA, QLoRA, adapters) upd...\n",
      "\n",
      "Source 2: llm_fundamentals.pdf (Page 0)\n",
      "   Content: @genieincodebottle \n",
      "Instagram | GitHub | Medium | YouTube \n",
      "How to Be Better Than Most in GenAI \n",
      " \n",
      "Contents \n",
      " \n",
      "Core LLM Building Blocks ..................\n",
      "\n",
      "Source 3: prakyath_resume.pdf (Page 0)\n",
      "   Content: Calling, LangGraph, Multi-Agent Systems), A/B Testing, Model Benchmarking, CrewAI, A2A (Agent-to-Agent) \n",
      "Backend: Python, FastAPI, Streamlit, Docker, ...\n",
      "\n",
      "Source 4: Cover_Letter.pdf (Page 0)\n",
      "   Content: thrive in environments where I can lead projects end-to-end, experiment with new \n",
      "approaches, and deliver work that matters. \n",
      " \n",
      "I'd welcome the opport...\n",
      "\n",
      "Source 5: prakyath_resume.pdf (Page 0)\n",
      "   Content: ‚óè Deployed as Streamlit app with OpenCV video processing \n",
      "Crypto Price Prediction System | Python, LSTM, SARIMA, Pandas, Streamlit Link \n",
      "‚óè Developed t...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def query_multi_documents(question: str):\n",
    "    \"\"\"\n",
    "    Query the multi-document QA system and display results with source attribution.\n",
    "    \n",
    "    Args:\n",
    "        question: The question to ask\n",
    "    \"\"\"\n",
    "    # Get answer from the chain\n",
    "    result = multi_doc_qa_chain.invoke({\"query\": question})\n",
    "    \n",
    "    # Display question and answer\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"QUESTION: {question}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    print(f\"ANSWER:\\n{result['result']}\\n\")\n",
    "    \n",
    "    # Display sources with file and page information\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"SOURCES ({len(result['source_documents'])} chunks):\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    for i, doc in enumerate(result['source_documents'], 1):\n",
    "        source_file = doc.metadata.get('source_file', 'Unknown')\n",
    "        page = doc.metadata.get('page', 'Unknown')\n",
    "        \n",
    "        print(f\"Source {i}: {source_file} (Page {page})\")\n",
    "        print(f\"   Content: {doc.page_content[:150]}...\")\n",
    "        print()\n",
    "\n",
    "# Example query\n",
    "query_multi_documents(\"What is LoRA and how does it work?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Try More Questions\n",
    "\n",
    "Test with different types of questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "QUESTION: What are the different types of attention mechanisms?\n",
      "================================================================================\n",
      "\n",
      "ANSWER:\n",
      "The different types of attention mechanisms are:\n",
      "\n",
      "1. **Attention** - Highlights the most relevant tokens in context.\n",
      "2. **Self-Attention** - Each token attends to every other token for context.\n",
      "3. **Cross-Attention** - Connects the encoder and decoder in encoder-decoder models.\n",
      "4. **Multi-Head Attention** - Several attention heads capture different patterns in parallel.\n",
      "\n",
      "================================================================================\n",
      "SOURCES (5 chunks):\n",
      "================================================================================\n",
      "\n",
      "Source 1: llm_fundamentals.pdf (Page 1)\n",
      "   Content: 5. Attention ‚Üí Highlights the most relevant tokens in context \n",
      "6. Self-Attention ‚Üí Each token attends to every other token for context \n",
      "7. Cross-Atten...\n",
      "\n",
      "Source 2: llm_fundamentals.pdf (Page 6)\n",
      "   Content: 11. Self-Consistency ‚Üí Compare multiple reasoning paths ‚Üí pick best answer \n",
      "12. Tree of Thoughts (ToT) ‚Üí Explore many reasoning branches before decidi...\n",
      "\n",
      "Source 3: llm_fundamentals.pdf (Page 7)\n",
      "   Content: 3. Guardrails ‚Üí Rule-based or learned filters to prevent harmful outputs \n",
      "4. Bias & Fairness ‚Üí Detect and mitigate demographic or cultural prejudices ...\n",
      "\n",
      "Source 4: prakyath_resume.pdf (Page 0)\n",
      "   Content: Calling, LangGraph, Multi-Agent Systems), A/B Testing, Model Benchmarking, CrewAI, A2A (Agent-to-Agent) \n",
      "Backend: Python, FastAPI, Streamlit, Docker, ...\n",
      "\n",
      "Source 5: llm_fundamentals.pdf (Page 5)\n",
      "   Content: evaluation \n",
      "4. Human Evaluation ‚Üí Collect human judgments for accuracy, coherence, and safety \n",
      "5. Factuality / Truthfulness Metrics ‚Üí Specialized eval...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question about a specific topic\n",
    "query_multi_documents(\"What are the different types of attention mechanisms?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "QUESTION: Compare the approaches to fine-tuning discussed in the documents\n",
      "================================================================================\n",
      "\n",
      "ANSWER:\n",
      "The documents discuss several approaches to fine-tuning, each with its unique characteristics:\n",
      "\n",
      "1. **Quantization-Aware Training (QAT)**: This approach involves fine-tuning models while considering quantization to retain accuracy. It aims to prepare models for deployment in environments with limited resources by adjusting the model's weights to minimize the loss of performance when quantized.\n",
      "\n",
      "2. **QLoRA**: This is a combination of LoRA (Low-Rank Adaptation) and quantization, which enables the fine-tuning of large models on modest hardware. It allows for efficient adaptation of models by updating only a small number of parameters, making it suitable for situations where hardware resources are limited.\n",
      "\n",
      "3. **PEFT (Parameter-Efficient Fine-Tuning)**: This method includes techniques like LoRA, QLoRA, and adapters that focus on updating only small parts of the model rather than the entire model. This approach is efficient and reduces the computational burden during fine-tuning.\n",
      "\n",
      "4. **Instruction Tuning**: This technique involves training models to better understand and follow natural language instructions. It focuses on aligning model behavior with user expectations by teaching the model how to interpret and execute instructions effectively.\n",
      "\n",
      "5. **RLHF (Reinforcement Learning from Human Feedback)**: This approach aims to align model outputs with human preferences by incorporating feedback from users. It involves training models to produce outputs that are more desirable or relevant based on human evaluations.\n",
      "\n",
      "Each of these approaches offers a different strategy to improve model performance and adapt to specific needs, with a focus on efficiency, resource constraints, and alignment with user requirements.\n",
      "\n",
      "================================================================================\n",
      "SOURCES (5 chunks):\n",
      "================================================================================\n",
      "\n",
      "Source 1: llm_fundamentals.pdf (Page 5)\n",
      "   Content: 16. Elastic / Dynamic Batching ‚Üí Adjust batch size dynamically to optimize throughput \n",
      "17. Inference Optimization ‚Üí Operator fusion, kernel tuning, an...\n",
      "\n",
      "Source 2: llm_fundamentals.pdf (Page 2)\n",
      "   Content: 9. QLoRA ‚Üí LoRA + quantization, enabling fine-tuning of huge models on modest hardware \n",
      "10. PEFT ‚Üí Family of methods (e.g., LoRA, QLoRA, adapters) upd...\n",
      "\n",
      "Source 3: llm_fundamentals.pdf (Page 6)\n",
      "   Content: @genieincodebottle \n",
      "6. Consistency / Contradiction Metrics ‚Üí Check if model outputs are logically consistent \n",
      "across queries \n",
      "7. Bias & Fairness Metri...\n",
      "\n",
      "Source 4: llm_fundamentals.pdf (Page 0)\n",
      "   Content: Training & Tuning .......................................................................................................................................\n",
      "\n",
      "Source 5: Cover_Letter.pdf (Page 0)\n",
      "   Content: built a financial analysis tool that processed 50+ page institutional documents into \n",
      "actionable metrics using RAG pipelines, achieving sub-10-second ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Comparative question (if you have multiple documents)\n",
    "query_multi_documents(\"Compare the approaches to fine-tuning discussed in the documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Filter by Source Document (Optional)\n",
    "\n",
    "Query specific documents only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FILTERED QUERY (Document: llm_fundamentals.pdf)\n",
      "QUESTION: What is attention mechanism?\n",
      "================================================================================\n",
      "\n",
      "ANSWER:\n",
      "The attention mechanism is a technique used in machine learning, particularly in natural language processing and computer vision, to focus on specific parts of the input data that are most relevant for a given task. It allows models to weigh different tokens or elements differently based on their importance in the context, enhancing the model's ability to capture relevant information and dependencies. This mechanism helps improve the performance of various models, especially in tasks involving sequences, such as translation or summarization.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def query_specific_document(question: str, source_file: str):\n",
    "    \"\"\"\n",
    "    Query a specific document by filtering on source_file metadata.\n",
    "    \n",
    "    Args:\n",
    "        question: The question to ask\n",
    "        source_file: The filename to search in\n",
    "    \"\"\"\n",
    "    # Create retriever with metadata filter\n",
    "    filtered_retriever = vectorstore.as_retriever(\n",
    "        search_type=\"mmr\",\n",
    "        search_kwargs={\n",
    "            \"k\": 5,\n",
    "            \"fetch_k\": 20,\n",
    "            \"filter\": {\"source_file\": source_file}  # Filter by source file\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Create temporary chain with filtered retriever\n",
    "    filtered_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=filtered_retriever,\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    \n",
    "    # Get answer\n",
    "    result = filtered_chain.invoke({\"query\": question})\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"FILTERED QUERY (Document: {source_file})\")\n",
    "    print(f\"QUESTION: {question}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    print(f\"ANSWER:\\n{result['result']}\\n\")\n",
    "\n",
    "# Example: Query only the llm_fundamentals.pdf\n",
    "query_specific_document(\n",
    "    \"What is attention mechanism?\",\n",
    "    \"llm_fundamentals.pdf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What is Built:\n",
    "- ‚úÖ Multi-document loading system with metadata tracking\n",
    "- ‚úÖ Unified vector store for cross-document search\n",
    "- ‚úÖ QA system with source attribution (file + page)\n",
    "- ‚úÖ Document-specific filtering capability\n",
    "\n",
    "### Key Concepts Learned:\n",
    "1. **Metadata Management**: Tracking source files and pages\n",
    "2. **Document Combination**: Merging multiple sources in one vector store\n",
    "3. **Source Attribution**: Showing which document provided the answer\n",
    "4. **Filtered Retrieval**: Querying specific documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLNotebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
