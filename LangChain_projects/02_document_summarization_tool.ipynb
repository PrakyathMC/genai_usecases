{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Summarization Tool\n",
    "\n",
    "## Project Overview\n",
    "This project builds a sophisticated document summarization system using different LangChain techniques.\n",
    "It can:\n",
    "- Summarize long documents that exceed LLM context limits\n",
    "- Generate summaries in different styles (executive, technical, bullet points)\n",
    "- Use multiple chain types (stuff, map_reduce, refine)\n",
    "- Handle documents of any length\n",
    "\n",
    "## Use Cases\n",
    "- Research: Quickly understand research papers\n",
    "- Business: Create executive summaries of reports\n",
    "- Legal: Summarize contracts and legal documents\n",
    "- Academic: Generate study notes from textbooks\n",
    "\n",
    "## What You'll Learn\n",
    "1. Different chain types (stuff, map_reduce, refine)\n",
    "2. Prompt engineering for different summary styles\n",
    "3. Handling long documents\n",
    "4. Custom prompts with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Environment loaded\n",
      "OpenAI API Key found: True\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "print(\"✅ Environment loaded\")\n",
    "print(f\"OpenAI API Key found: {'OPENAI_API_KEY' in os.environ}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Document loading and processing\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# LLM and chains\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_classic.chains.summarize import load_summarize_chain\n",
    "# Prompts\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "print(\"✅ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Document to Summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 pages\n",
      "\n",
      "Document info:\n",
      "   Total pages: 1\n",
      "   First page preview: Prakyath Chandran \n",
      "Bangalore, India \n",
      "chandranpraky...\n"
     ]
    }
   ],
   "source": [
    "# Load PDF document\n",
    "pdf_path = \"Cover_Letter.pdf\"\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"✅ Loaded {len(documents)} pages\")\n",
    "print(f\"\\nDocument info:\")\n",
    "print(f\"   Total pages: {len(documents)}\")\n",
    "print(f\"   First page preview: {documents[0].page_content[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Initialize LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLM initialized\n"
     ]
    }
   ],
   "source": [
    "# Initialize OpenAI LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.3,  # Lower temperature for more focused summaries\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"]\n",
    ")\n",
    "\n",
    "print(\"✅ LLM initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Stuff Chain (Simple - for short documents)\n",
    "\n",
    "**How it works:**\n",
    "- Puts ALL document content into a single prompt\n",
    "- Best for documents that fit in LLM context window\n",
    "- Fastest and most accurate for small documents\n",
    "- **Limitation**: Fails if document is too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating summary using STUFF chain...\n",
      "\n",
      "================================================================================\n",
      "STUFF CHAIN SUMMARY\n",
      "================================================================================\n",
      "Prakyath Chandran, an AI developer from Bangalore, India, is applying for the AI Engineer position at YipitData. He has experience building AI systems that convert complex data into actionable insights, notably at Phracta, where he developed a financial analysis tool with rapid query response times. At HaiX AI, he benchmarked LLMs and created FastAPI endpoints for brand analytics. His background includes classical machine learning and deep learning projects, such as real-time sign language detection and crypto price forecasting. Prakyath is drawn to YipitData's focus on ownership and rapid growth and is eager to contribute to their AI initiatives.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create stuff chain\n",
    "stuff_chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",  # Put everything in one prompt\n",
    "    verbose=False  # Set to True to see the prompt being used\n",
    ")\n",
    "\n",
    "# Generate summary\n",
    "print(\"Generating summary using STUFF chain...\\n\")\n",
    "stuff_summary = stuff_chain.invoke(documents)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STUFF CHAIN SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(stuff_summary['output_text'])\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Map-Reduce Chain (For long documents)\n",
    "\n",
    "**How it works:**\n",
    "1. **Map**: Summarize each chunk independently\n",
    "2. **Reduce**: Combine all chunk summaries into final summary\n",
    "\n",
    "**Advantages:**\n",
    "- Handles documents of ANY length\n",
    "- Parallelizable (can process chunks simultaneously)\n",
    "\n",
    "**Disadvantages:**\n",
    "- May lose connections between chunks\n",
    "- More API calls = higher cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 1 chunks for map-reduce\n",
      "\n",
      "Generating summary using MAP-REDUCE chain...\n",
      "\n",
      "================================================================================\n",
      "MAP-REDUCE CHAIN SUMMARY\n",
      "================================================================================\n",
      "Prakyath Chandran, an AI Engineer from Bangalore, India, is applying for a position at YipitData. He has developed AI systems at Phracta and HaiX AI, including a financial analysis tool and benchmarking large language models. His experience encompasses classical machine learning and deep learning projects, such as sign language detection and crypto price forecasting. Prakyath is attracted to YipitData's emphasis on ownership and growth and is keen to contribute to its AI initiatives.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Split document into chunks for map-reduce\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,      # Larger chunks for summarization\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "split_docs = text_splitter.split_documents(documents)\n",
    "print(f\"Split into {len(split_docs)} chunks for map-reduce\\n\")\n",
    "\n",
    "# Create map-reduce chain\n",
    "map_reduce_chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type=\"map_reduce\",  # Summarize chunks then combine\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Generate summary\n",
    "print(\"Generating summary using MAP-REDUCE chain...\\n\")\n",
    "map_reduce_summary = map_reduce_chain.invoke(split_docs)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MAP-REDUCE CHAIN SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(map_reduce_summary['output_text'])\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: Refine Chain (For coherent long summaries)\n",
    "\n",
    "**How it works:**\n",
    "1. Summarize first chunk\n",
    "2. Refine summary with second chunk\n",
    "3. Refine again with third chunk\n",
    "4. Continue until all chunks processed\n",
    "\n",
    "**Advantages:**\n",
    "- Maintains coherence across chunks\n",
    "- Builds context iteratively\n",
    "- Good for narrative documents\n",
    "\n",
    "**Disadvantages:**\n",
    "- Sequential (cannot parallelize)\n",
    "- Slower than map-reduce\n",
    "- More API calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating summary using REFINE chain...\n",
      "\n",
      "================================================================================\n",
      "REFINE CHAIN SUMMARY\n",
      "================================================================================\n",
      "Prakyath Chandran, an AI Engineer based in Bangalore, India, is applying for a position at YipitData. He has experience in building AI systems that convert complex data into actionable insights, notably as the sole AI developer at Phracta, where he created a financial analysis tool with rapid query response times. At HaiX AI, he benchmarked large language models and developed FastAPI endpoints for brand analytics. With a strong foundation in classical machine learning and deep learning, he has completed projects including real-time sign language detection and crypto price forecasting. Prakyath is drawn to YipitData's culture of ownership and rapid growth and is eager to contribute to its AI initiatives.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create refine chain\n",
    "refine_chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type=\"refine\",  # Iteratively refine the summary\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Generate summary\n",
    "print(\"Generating summary using REFINE chain...\\n\")\n",
    "refine_summary = refine_chain.invoke(split_docs)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"REFINE CHAIN SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(refine_summary['output_text'])\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Prompts: Executive Summary Style\n",
    "\n",
    "Create custom prompts to control the summary style and format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating EXECUTIVE SUMMARY...\n",
      "\n",
      "================================================================================\n",
      "EXECUTIVE SUMMARY\n",
      "================================================================================\n",
      "**Executive Summary**\n",
      "\n",
      "**Key Takeaways:**\n",
      "- Prakyath Chandran has extensive experience in AI development, particularly in transforming complex data into actionable insights.\n",
      "- Successfully delivered end-to-end MVPs, including a financial analysis tool with rapid query response times.\n",
      "- Proven track record in benchmarking LLMs and optimizing cost-efficiency in AI models.\n",
      "- Strong foundation in classical machine learning and deep learning, with practical applications in real-time detection and forecasting.\n",
      "\n",
      "**Main Themes:**\n",
      "- Alignment of skills with YipitData's mission to leverage alternative data.\n",
      "- Emphasis on ownership, rapid execution, and high-impact results.\n",
      "\n",
      "**Actionable Insights:**\n",
      "- Consider Prakyath for the AI Engineer role to enhance YipitData's capabilities in data-driven decision-making.\n",
      "- Leverage his experience in building scalable AI solutions to accelerate project timelines and improve operational efficiency.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Custom prompt for executive summary\n",
    "executive_prompt_template = \"\"\"\n",
    "You are a senior executive summarizing a document for C-level executives.\n",
    "\n",
    "Create a concise executive summary with:\n",
    "1. Key takeaways (3-5 bullet points)\n",
    "2. Main themes\n",
    "3. Actionable insights\n",
    "\n",
    "Keep it under 150 words and focus on business value.\n",
    "\n",
    "DOCUMENT:\n",
    "{text}\n",
    "\n",
    "EXECUTIVE SUMMARY:\n",
    "\"\"\"\n",
    "\n",
    "executive_prompt = PromptTemplate(\n",
    "    template=executive_prompt_template,\n",
    "    input_variables=[\"text\"] \n",
    ")\n",
    "\n",
    "# Create chain with custom prompt\n",
    "executive_chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    prompt=executive_prompt,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Generate executive summary\n",
    "print(\"Generating EXECUTIVE SUMMARY...\\n\")\n",
    "executive_summary = executive_chain.invoke(documents)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"EXECUTIVE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(executive_summary['output_text'])\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Prompts: Technical Summary Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating TECHNICAL SUMMARY...\n",
      "\n",
      "================================================================================\n",
      "TECHNICAL SUMMARY\n",
      "================================================================================\n",
      "### Technical Summary\n",
      "\n",
      "#### 1. Core Concepts and Terminology\n",
      "- **AI Systems**: Refers to the development of artificial intelligence frameworks that can analyze data and generate insights.\n",
      "- **MVP (Minimum Viable Product)**: A version of a product with just enough features to satisfy early adopters and provide feedback for future development.\n",
      "- **RAG Pipelines**: Refers to Retrieval-Augmented Generation pipelines, a method that combines retrieval of relevant information with generative models to produce accurate outputs.\n",
      "- **LLMs (Large Language Models)**: Advanced neural network models trained on vast datasets to understand and generate human-like text.\n",
      "- **FastAPI**: A modern web framework for building APIs with Python, known for its speed and ease of use.\n",
      "- **CNN-LSTM**: A hybrid model combining Convolutional Neural Networks (CNN) for feature extraction and Long Short-Term Memory (LSTM) networks for sequence prediction.\n",
      "- **SARIMA (Seasonal Autoregressive Integrated Moving Average)**: A statistical method for forecasting time series data that accounts for seasonality.\n",
      "- **Streamlit**: An open-source app framework for Machine Learning and Data Science projects, enabling the creation of web applications for data visualization.\n",
      "\n",
      "#### 2. Technical Approaches and Methods\n",
      "- **End-to-End Development**: Involves the complete process from conceptualization to deployment of AI solutions, ensuring that all components work seamlessly together.\n",
      "- **Benchmarking LLMs**: Systematic evaluation of multiple large language models to assess their performance, efficiency, and cost-effectiveness, using a variety of prompt variations to optimize results.\n",
      "- **Integration of Data Sources**: Utilizing FastAPI to create endpoints that aggregate data from various platforms, facilitating comprehensive brand analytics.\n",
      "- **Real-Time Processing**: Implementing models that can analyze and respond to data inputs in real-time, as demonstrated in the sign language detection project.\n",
      "- **Time Series Forecasting**: Employing SARIMA and LSTM models to predict future values based on historical data, particularly in volatile markets like cryptocurrency.\n",
      "\n",
      "#### 3. Key Technical Details\n",
      "- **Financial Analysis Tool**: Developed a tool capable of processing extensive institutional documents (50+ pages) into actionable metrics, achieving query response times under 10 seconds, indicative of high efficiency in data processing.\n",
      "- **Prompt Variations**: Conducted extensive testing (100+ variations) to refine the interaction with LLMs, focusing on optimizing for both performance and cost.\n",
      "- **Deployment**: Utilized Streamlit for deploying machine learning models, allowing for user-friendly interfaces and real-time interaction with the models.\n",
      "\n",
      "#### 4. Implementation Considerations\n",
      "- **Scalability**: When developing AI systems, it is crucial to ensure that the architecture can handle increasing amounts of data and user requests without significant degradation in performance.\n",
      "- **Cost Efficiency**: Continuous evaluation of model performance and operational costs is essential, particularly when deploying LLMs, to maintain a balance between quality and expenditure.\n",
      "- **Rapid Prototyping**: The ability to quickly iterate on MVPs is vital in a startup environment, necessitating agile methodologies and flexible frameworks like FastAPI and Streamlit.\n",
      "- **Data Integration**: Ensuring seamless integration of disparate data sources is critical for comprehensive analysis, which may involve handling different data formats and ensuring data quality.\n",
      "- **User Experience**: The deployment of applications should prioritize user experience, ensuring that interfaces are intuitive and that the underlying models provide accurate and timely results.\n",
      "\n",
      "This summary encapsulates the technical expertise and methodologies applied by Prakyath Chandran in the development of AI solutions, emphasizing his capability to deliver impactful results in fast-paced environments.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Custom prompt for technical summary\n",
    "technical_prompt_template = \"\"\"\n",
    "You are a technical expert summarizing a document for engineers and researchers.\n",
    "\n",
    "Create a detailed technical summary including:\n",
    "1. Core concepts and terminology\n",
    "2. Technical approaches and methods\n",
    "3. Key technical details\n",
    "4. Implementation considerations\n",
    "\n",
    "Use technical language and include specific details.\n",
    "\n",
    "DOCUMENT:\n",
    "{text}\n",
    "\n",
    "TECHNICAL SUMMARY:\n",
    "\"\"\"\n",
    "\n",
    "technical_prompt = PromptTemplate(\n",
    "    template=technical_prompt_template,\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Create chain with custom prompt\n",
    "technical_chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    prompt=technical_prompt,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Generate technical summary\n",
    "print(\"Generating TECHNICAL SUMMARY...\\n\")\n",
    "technical_summary = technical_chain.invoke(documents)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TECHNICAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(technical_summary['output_text'])\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Prompts: Bullet Points Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom prompt for bullet points\n",
    "bullet_prompt_template = \"\"\"\n",
    "Summarize the following document as a structured list of bullet points.\n",
    "\n",
    "Format:\n",
    "• Main Point 1\n",
    "  - Sub-point 1a\n",
    "  - Sub-point 1b\n",
    "• Main Point 2\n",
    "  - Sub-point 2a\n",
    "  - Sub-point 2b\n",
    "\n",
    "Focus on the most important information and organize hierarchically.\n",
    "\n",
    "DOCUMENT:\n",
    "{text}\n",
    "\n",
    "BULLET POINT SUMMARY:\n",
    "\"\"\"\n",
    "\n",
    "bullet_prompt = PromptTemplate(\n",
    "    template=bullet_prompt_template,\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Create chain with custom prompt\n",
    "bullet_chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    prompt=bullet_prompt,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Generate bullet point summary\n",
    "print(\"Generating BULLET POINT SUMMARY...\\n\")\n",
    "bullet_summary = bullet_chain.invoke(documents)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"BULLET POINT SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(bullet_summary['output_text'])\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing All Summary Methods\n",
    "\n",
    "Let's compare the different approaches side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_summaries():\n",
    "    \"\"\"\n",
    "    Display all summaries for comparison.\n",
    "    \"\"\"\n",
    "    summaries = [\n",
    "        (\"STUFF CHAIN\", stuff_summary['output_text']),\n",
    "        (\"MAP-REDUCE CHAIN\", map_reduce_summary['output_text']),\n",
    "        (\"REFINE CHAIN\", refine_summary['output_text']),\n",
    "        (\"EXECUTIVE STYLE\", executive_summary['output_text']),\n",
    "        (\"TECHNICAL STYLE\", technical_summary['output_text']),\n",
    "        (\"BULLET POINTS STYLE\", bullet_summary['output_text'])\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n\" + \"#\"*80)\n",
    "    print(\"SUMMARY COMPARISON\")\n",
    "    print(\"#\"*80 + \"\\n\")\n",
    "    \n",
    "    for method, summary in summaries:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"{method}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Length: {len(summary)} characters\")\n",
    "        print(f\"\\n{summary}\")\n",
    "        print()\n",
    "\n",
    "# Display comparison\n",
    "compare_summaries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced: Map-Reduce with Custom Prompts\n",
    "\n",
    "Combine map-reduce chain with custom prompts for long documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom map prompt (for each chunk)\n",
    "map_prompt_template = \"\"\"\n",
    "Write a concise summary of the following chunk:\n",
    "\n",
    "{text}\n",
    "\n",
    "CONCISE SUMMARY:\n",
    "\"\"\"\n",
    "\n",
    "map_prompt = PromptTemplate(\n",
    "    template=map_prompt_template,\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Custom combine prompt (for combining summaries)\n",
    "combine_prompt_template = \"\"\"\n",
    "Combine the following summaries into a comprehensive final summary.\n",
    "Organize by themes and eliminate redundancy.\n",
    "\n",
    "{text}\n",
    "\n",
    "COMPREHENSIVE SUMMARY:\n",
    "\"\"\"\n",
    "\n",
    "combine_prompt = PromptTemplate(\n",
    "    template=combine_prompt_template,\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Create map-reduce chain with custom prompts\n",
    "custom_map_reduce_chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type=\"map_reduce\",\n",
    "    map_prompt=map_prompt,        # Prompt for each chunk\n",
    "    combine_prompt=combine_prompt, # Prompt for combining\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Generate custom map-reduce summary\n",
    "print(\"Generating CUSTOM MAP-REDUCE SUMMARY...\\n\")\n",
    "custom_summary = custom_map_reduce_chain.invoke(split_docs)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CUSTOM MAP-REDUCE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(custom_summary['output_text'])\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function: Summarize Any Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_document(pdf_path: str, style: str = \"executive\", chain_type: str = \"stuff\"):\n",
    "    \"\"\"\n",
    "    Flexible function to summarize any document with chosen style and chain type.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to PDF file\n",
    "        style: Summary style - \"executive\", \"technical\", \"bullets\", or \"default\"\n",
    "        chain_type: Chain type - \"stuff\", \"map_reduce\", or \"refine\"\n",
    "    \n",
    "    Returns:\n",
    "        Summary text\n",
    "    \"\"\"\n",
    "    # Load document\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    docs = loader.load()\n",
    "    \n",
    "    # Choose prompt based on style\n",
    "    prompt_map = {\n",
    "        \"executive\": executive_prompt,\n",
    "        \"technical\": technical_prompt,\n",
    "        \"bullets\": bullet_prompt\n",
    "    }\n",
    "    \n",
    "    # Prepare documents based on chain type\n",
    "    if chain_type in [\"map_reduce\", \"refine\"]:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=2000,\n",
    "            chunk_overlap=200\n",
    "        )\n",
    "        docs = text_splitter.split_documents(docs)\n",
    "    \n",
    "    # Create chain\n",
    "    if style in prompt_map:\n",
    "        chain = load_summarize_chain(\n",
    "            llm=llm,\n",
    "            chain_type=chain_type,\n",
    "            prompt=prompt_map[style],\n",
    "            verbose=False\n",
    "        )\n",
    "    else:\n",
    "        chain = load_summarize_chain(\n",
    "            llm=llm,\n",
    "            chain_type=chain_type,\n",
    "            verbose=False\n",
    "        )\n",
    "    \n",
    "    # Generate summary\n",
    "    result = chain.invoke(docs)\n",
    "    return result['output_text']\n",
    "\n",
    "# Example usage\n",
    "print(\"\\nTesting flexible summarization function:\\n\")\n",
    "test_summary = summarize_document(\n",
    "    pdf_path=\"../RAG/llm_fundamentals.pdf\",\n",
    "    style=\"bullets\",\n",
    "    chain_type=\"stuff\"\n",
    ")\n",
    "print(test_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What You've Built:\n",
    "- ✅ Three chain types: stuff, map_reduce, refine\n",
    "- ✅ Three summary styles: executive, technical, bullet points\n",
    "- ✅ Custom prompts for different use cases\n",
    "- ✅ Flexible summarization function\n",
    "\n",
    "### Key Concepts Learned:\n",
    "1. **Chain Types**:\n",
    "   - **Stuff**: Fast, simple, for short documents\n",
    "   - **Map-Reduce**: Handles long documents, parallelizable\n",
    "   - **Refine**: Iterative, maintains coherence\n",
    "\n",
    "2. **Prompt Engineering**:\n",
    "   - Custom prompts control output style\n",
    "   - Different prompts for different audiences\n",
    "   - Map and combine prompts for map-reduce\n",
    "\n",
    "3. **Trade-offs**:\n",
    "   - Speed vs. quality\n",
    "   - Cost vs. comprehensiveness\n",
    "   - Document length considerations\n",
    "\n",
    "### When to Use Each Method:\n",
    "| Chain Type | Best For | Speed | Cost |\n",
    "|------------|----------|-------|------|\n",
    "| Stuff | Short docs (<4k tokens) | Fastest | Lowest |\n",
    "| Map-Reduce | Long docs, any length | Medium | Medium |\n",
    "| Refine | Coherent narrative summaries | Slowest | Highest |\n",
    "\n",
    "### Next Steps:\n",
    "- Try summarizing different types of documents (technical papers, articles, books)\n",
    "- Experiment with different chunk sizes for map-reduce\n",
    "- Create custom prompts for specific domains (legal, medical, etc.)\n",
    "- Combine with translation for multilingual summaries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLNotebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
