{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad4f5004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Loaded Successfully\n",
      " API Key Found: True\n",
      "Key starts with: sk-proj-3Q...\n",
      "✅ Environment loaded\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "print(\"Environment Loaded Successfully\")\n",
    "print(f\" API Key Found: {'OPENAI_API_KEY' in os.environ}\")\n",
    "\n",
    "if 'OPENAI_API_KEY' in os.environ:\n",
    "    print(f\"Key starts with: {os.environ['OPENAI_API_KEY'][:10]}...\")\n",
    "print(\"✅ Environment loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88379d99",
   "metadata": {},
   "source": [
    "# Core Langchain - Document Loaders, Text Splitters, Embedding, Vector Stroe, LLMs, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37578540",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lapde\\anaconda3\\envs\\MLNotebook\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8 documents\n",
      "Content Preview: @genieincodebottle \n",
      "Instagram | GitHub | Medium | ...\n",
      "Metadata Preview: {'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2025-09-02T20:12:32+05:30', 'author': 'Rajesh Srivastava', 'moddate': '2025-09-02T20:12:32+05:30', 'source': 'llm_fundamentals.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "# Document Loading\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"llm_fundamentals.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents\")\n",
    "print(f\"Content Preview: {documents[0].page_content[:50]}...\")\n",
    "print(f\"Metadata Preview: {documents[0].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddaa58f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Split 8 pages into 37 chunks\n",
      "\n",
      "Sample chunk:\n",
      "5. Attention → Highlights the most relevant tokens in context \n",
      "6. Self-Attention → Each token attends to every other token for context \n",
      "7. Cross-Attention → Connect encoder and decoder (in encoder-decoder models) \n",
      "8. Multi-Head Attention → Several attention heads capture different patterns in parallel \n",
      "9. Feed-Forward Networks → Nonlinear layers that transform representations between \n",
      "attention blocks \n",
      "10. Residual Connections → Shortcut links that preserve signals and help gradient flow\n",
      "\n",
      "Metadata: {'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2025-09-02T20:12:32+05:30', 'author': 'Rajesh Srivastava', 'moddate': '2025-09-02T20:12:32+05:30', 'source': 'llm_fundamentals.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap = 50,\n",
    "    length_function = len,\n",
    "    separators = [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"✅ Split {len(documents)} pages into {len(chunks)} chunks\")\n",
    "print(f\"\\nSample chunk:\")\n",
    "print(chunks[5].page_content)\n",
    "print(f\"\\nMetadata: {chunks[5].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "060669e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Model loaded successfully.\n",
      "Embedding Dimension: 384\n",
      "First 5 values: [0.045804012566804886, -0.08357568085193634, 0.008843314833939075, -0.02673397585749626, -0.05191471427679062]\n"
     ]
    }
   ],
   "source": [
    "#embedding\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name = \"all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "test_text = \"What is a large language model?\"\n",
    "test_embedding_vector = embeddings.embed_query(test_text)\n",
    "\n",
    "print(f\"Embedding Model loaded successfully.\")\n",
    "print(f\"Embedding Dimension: {len(test_embedding_vector)}\")\n",
    "print(f\"First 5 values: {test_embedding_vector[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1cf97bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Vector store created with Chroma\n",
      "✅ Vector store created with 37 chunks\n"
     ]
    }
   ],
   "source": [
    "# vector store\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents = chunks,\n",
    "    embedding = embeddings,\n",
    "    collection_name = \"llm_fundamentals_collection\"\n",
    ")\n",
    "\n",
    "print(\"✅ Vector store created with Chroma\")\n",
    "print(f\"✅ Vector store created with {vectorstore._collection.count()} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11d06cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is RAG?\n",
      "\n",
      "Result 1:\n",
      "17. ALiBi / Relative Positional Encoding → Alternative to RoPE for long contexts \n",
      "18. Linear / Performer Attention → Efficient attention variants for ...\n",
      "Source: Page 1\n",
      "\n",
      "Result 2:\n",
      "evaluation \n",
      "4. Human Evaluation → Collect human judgments for accuracy, coherence, and safety \n",
      "5. Factuality / Truthfulness Metrics → Specialized eval...\n",
      "Source: Page 5\n",
      "\n",
      "Result 3:\n",
      "@genieincodebottle \n",
      "Instagram | GitHub | Medium | YouTube \n",
      "How to Be Better Than Most in GenAI \n",
      " \n",
      "Contents \n",
      " \n",
      "Core LLM Building Blocks ..................\n",
      "Source: Page 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test Similarity Search\n",
    "query = \"What is RAG?\"\n",
    "results = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"Result {i}:\")\n",
    "    print(f\"{doc.page_content[:150]}...\")\n",
    "    print(f\"Source: Page {doc.metadata['page']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2daf99bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Initialized\n",
      "Response: RAG stands for \"Retrieval-Augmented Generation.\" It is a method used in natural language processing that combines two approaches: retrieving information from a database or external source and generating text based on that information.\n",
      "\n",
      "Here’s how it works in simple terms:\n",
      "\n",
      "1. **Retrieval**: When you ask a question or give a prompt, the system first searches for relevant information from a large collection of documents or data. This could be anything from articles to books or other text sources.\n",
      "\n",
      "2. **Augmentation**: After finding relevant pieces of information, the system uses these to help create a more informed and accurate response.\n",
      "\n",
      "3. **Generation**: Finally, the system generates a response based on the retrieved information, combining it with its own understanding to produce a coherent answer.\n",
      "\n",
      "In essence, RAG helps improve the quality and relevance of generated text by grounding it in real-world information, rather than relying solely on the model's training data. This makes the responses more accurate and informative.\n"
     ]
    }
   ],
   "source": [
    "# LLMs\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature = 0.7,\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"]\n",
    ")\n",
    "\n",
    "response = llm.invoke(\"Explain RAG in simple terms.\")\n",
    "\n",
    "print(\"LLM Initialized\")\n",
    "print(f\"Response: {response.content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39292ac7",
   "metadata": {},
   "source": [
    "## Now lets combine all components into a RAG system using RetrievalQA form (simplest method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1ab02b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RAG QA Chain created successfully.\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.chains.retrieval_qa.base import RetrievalQA\n",
    "\n",
    "# RAG QA Chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm = llm,\n",
    "    chain_type = \"stuff\", # stuff = put all context in prompt\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\":3}),\n",
    "    return_source_documents = True\n",
    ")\n",
    "\n",
    "print(\"✅ RAG QA Chain created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08810fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is LoRA and how is it used in LLMs?\n",
      "\n",
      "Answer:\n",
      "LoRA (Low-Rank Adaptation) is a method used in the context of fine-tuning large language models (LLMs) by updating only small parts of the model instead of the entire model. It allows for efficient adaptation of pre-trained models to specific tasks or domains without the need for extensive computational resources. By focusing on low-rank updates, LoRA enables fine-tuning on modest hardware, making it feasible to work with large models.\n",
      "\n",
      "================================================================================\n",
      "Sources (3 chunks):\n",
      "\n",
      "1. Page 2:\n",
      "   9. QLoRA → LoRA + quantization, enabling fine-tuning of huge models on modest hardware \n",
      "10. PEFT → F...\n",
      "\n",
      "2. Page 6:\n",
      "   requirements \n",
      "11. Explainability / Interpretability Evaluation → Assess clarity and transparency of ...\n",
      "\n",
      "3. Page 0:\n",
      "   @genieincodebottle \n",
      "Instagram | GitHub | Medium | YouTube \n",
      "How to Be Better Than Most in GenAI \n",
      " \n",
      "Co...\n"
     ]
    }
   ],
   "source": [
    "# Test RAG QA Chain\n",
    "\n",
    "question = \"What is LoRA and how is it used in LLMs?\"\n",
    "result = qa_chain.invoke({\"query\": question})\n",
    "\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(f\"Answer:\\n{result['result']}\\n\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Sources ({len(result['source_documents'])} chunks):\")\n",
    "for i, doc in enumerate(result['source_documents'], 1):\n",
    "    print(f\"\\n{i}. Page {doc.metadata['page']}:\")\n",
    "    print(f\"   {doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d11f4862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RAG QA Chain with MMR created successfully.\n",
      "Question: What is LoRA and how is it used in LLMs?\n",
      "\n",
      "Answer:\n",
      "LoRA (Low-Rank Adaptation) is a method used in the fine-tuning of large language models (LLMs). It enables the adaptation of these models by updating only a small number of parameters, rather than the entire model. This approach is particularly useful because it allows for the fine-tuning of large models on modest hardware by reducing the computational resources required. In essence, LoRA adds low-rank matrices to the existing weight matrices of the model, which helps in efficiently capturing the necessary adjustments while keeping the base model intact.\n",
      "\n",
      "================================================================================\n",
      "Sources (3 chunks):\n",
      "\n",
      "1. Page 2:\n",
      "   9. QLoRA → LoRA + quantization, enabling fine-tuning of huge models on modest hardware \n",
      "10. PEFT → F...\n",
      "\n",
      "2. Page 0:\n",
      "   @genieincodebottle \n",
      "Instagram | GitHub | Medium | YouTube \n",
      "How to Be Better Than Most in GenAI \n",
      " \n",
      "Co...\n",
      "\n",
      "3. Page 1:\n",
      "   @genieincodebottle \n",
      "Core LLM Building Blocks \n",
      "1. Transformer Architecture → Core design (encoder-onl...\n"
     ]
    }
   ],
   "source": [
    "# MMR avoids duplicate/similar results\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(\n",
    "        search_type=\"mmr\",  # Maximum Marginal Relevance\n",
    "        search_kwargs={\"k\": 3, \"fetch_k\": 10}\n",
    "    ),\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "print(\"✅ RAG QA Chain with MMR created successfully.\")\n",
    "# Test RAG QA Chain with MMR\n",
    "question = \"What is LoRA and how is it used in LLMs?\"\n",
    "result = qa_chain.invoke({\"query\": question})\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(f\"Answer:\\n{result['result']}\\n\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Sources ({len(result['source_documents'])} chunks):\")\n",
    "for i, doc in enumerate(result['source_documents'], 1):\n",
    "    print(f\"\\n{i}. Page {doc.metadata['page']}:\")\n",
    "    print(f\"   {doc.page_content[:100]}...\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd432e2",
   "metadata": {},
   "source": [
    "## USing LECL Method, building personal/custom chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40756b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Custom RAG Chain created successfully.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate # used for custom prompt\n",
    "from langchain_core.output_parsers import StrOutputParser # used for custom output parsing\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "# custom prompt\n",
    "template = \"\"\"You are an AI assistant helping users understand LLM fundamentals.\n",
    "Answer the question based ONLY on the provided context. Cite page numbers when possible.\n",
    "\n",
    "Context: \n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# custom output parser. Helper function for custom output parsing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([\n",
    "        f\"[Page {doc.metadata['page']}]\\n{doc.page_content}\"\n",
    "        for doc in docs\n",
    "    ])\n",
    "\n",
    "# build chain using LECL\n",
    "rag_chain = (\n",
    "    {\"context\": vectorstore.as_retriever() | format_docs,  \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "\n",
    ")\n",
    "print(\"✅ Custom RAG Chain created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69db41e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is attention mechanism?\n",
      "\n",
      "Answer:\n",
      "The attention mechanism highlights the most relevant tokens in context, allowing the model to focus on specific parts of the input data that are more important for generating an output. This is essential for understanding relationships within the data and improving the overall performance of the model (Page 1).\n"
     ]
    }
   ],
   "source": [
    "# Use the custom chain\n",
    "question = \"What is attention mechanism?\"\n",
    "answer = rag_chain.invoke(question)\n",
    "\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(f\"Answer:\\n{answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e946663",
   "metadata": {},
   "source": [
    "## Memory Status "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3bc2b3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Conversational Retrieval Chain created successfully.\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.chains import ConversationalRetrievalChain\n",
    "from langchain_classic.memory import ConversationBufferMemory\n",
    "\n",
    "# create conversation memory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key = \"chat_history\",\n",
    "    return_messages = True,\n",
    "    output_key = \"answer\"\n",
    ")\n",
    "\n",
    "# create conversational retrieval chain\n",
    "conversational_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm = llm,\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_type = \"mmr\",\n",
    "        search_kwargs = {\"k\":3, \"fetch_k\":10}\n",
    "    ),\n",
    "    memory = memory,\n",
    "    return_source_documents = True\n",
    ")\n",
    "print(\"✅ Conversational Retrieval Chain created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38281163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: What is LoRA?\n",
      "A1: LoRA stands for Low-Rank Adaptation, which is a method used for fine-tuning large models by updating only a small number of parameters. This allows for efficient model training on limited hardware resources.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Q2: What's the difference between that and QLoRA?\n",
      "A2: LoRA (Low-Rank Adaptation) is a method that enables fine-tuning of large language models by updating only a small number of parameters, while QLoRA is an extension of LoRA that incorporates quantization. QLoRA allows for fine-tuning huge models on modest hardware by reducing the model size through quantization techniques, making it more efficient than standard LoRA.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Q3: Can you give me a simple example of how it works?\n",
      "A3: I don't know.\n"
     ]
    }
   ],
   "source": [
    "# Test conversational chain with follow-up questions\n",
    "\n",
    "# First question\n",
    "result1 = conversational_chain.invoke({\"question\": \"What is LoRA?\"})\n",
    "print(\"Q1: What is LoRA?\")\n",
    "print(f\"A1: {result1['answer']}\\n\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Follow-up question (uses context from previous question!)\n",
    "result2 = conversational_chain.invoke({\"question\": \"What's the difference between that and QLoRA?\"})\n",
    "print(\"Q2: What's the difference between that and QLoRA?\")\n",
    "print(f\"A2: {result2['answer']}\\n\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Another follow-up\n",
    "result3 = conversational_chain.invoke({\"question\": \"Can you give me a simple example of how it works?\"})\n",
    "print(\"Q3: Can you give me a simple example of how it works?\")\n",
    "print(f\"A3: {result3['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34599e5",
   "metadata": {},
   "source": [
    "## Complete RAG chain using a single Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "06f03a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LangChainRAG class defined\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "class LangChainRAG:\n",
    "    \"\"\"\n",
    "    Production RAG system powered by LangChain.\n",
    "    \n",
    "    Why LangChain?\n",
    "        - 10x less code than manual implementation\n",
    "        - Easy to swap components (LLMs, vector stores, embeddings)\n",
    "        - Built-in features (memory, streaming, error handling)\n",
    "        - Production-tested by thousands of companies\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        pdf_path: str,\n",
    "        llm_model: str = \"gpt-4o-mini\",\n",
    "        embedding_model: str = \"all-MiniLM-L6-v2\",\n",
    "        chunk_size: int = 500,\n",
    "        chunk_overlap: int = 50\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize RAG system from a PDF.\n",
    "        \n",
    "        Args:\n",
    "            pdf_path: Path to PDF file\n",
    "            llm_model: OpenAI model name\n",
    "            embedding_model: HuggingFace embedding model\n",
    "            chunk_size: Characters per chunk\n",
    "            chunk_overlap: Overlap between chunks\n",
    "        \"\"\"\n",
    "        print(\"Initializing LangChain RAG system...\")\n",
    "        \n",
    "        # Load documents\n",
    "        print(f\"Loading PDF: {pdf_path}\")\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        documents = loader.load()\n",
    "        \n",
    "        # Split documents\n",
    "        print(f\"Splitting into chunks (size={chunk_size}, overlap={chunk_overlap})\")\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap\n",
    "        )\n",
    "        self.chunks = text_splitter.split_documents(documents)\n",
    "        \n",
    "        # Setup embeddings\n",
    "        print(f\"Loading embedding model: {embedding_model}\")\n",
    "        self.embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n",
    "        \n",
    "        # Create vector store\n",
    "        print(\"Creating vector store...\")\n",
    "        self.vectorstore = Chroma.from_documents(\n",
    "            documents=self.chunks,\n",
    "            embedding=self.embeddings,\n",
    "            collection_name=\"langchain_rag\"\n",
    "        )\n",
    "        \n",
    "        # Setup LLM\n",
    "        print(f\"Initializing LLM: {llm_model}\")\n",
    "        self.llm = ChatOpenAI(\n",
    "            model=llm_model,\n",
    "            temperature=0.3,\n",
    "            api_key=os.environ[\"OPENAI_API_KEY\"]\n",
    "        )\n",
    "        \n",
    "        # Create QA chain\n",
    "        self.qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=self.llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=self.vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
    "            return_source_documents=True\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ RAG system ready! ({len(self.chunks)} chunks indexed)\\n\")\n",
    "    \n",
    "    def ask(self, question: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Ask a question and get an answer with sources.\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with answer and source information\n",
    "        \"\"\"\n",
    "        result = self.qa_chain.invoke({\"query\": question})\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": result['result'],\n",
    "            \"sources\": [\n",
    "                {\n",
    "                    \"page\": doc.metadata.get('page', 'N/A'),\n",
    "                    \"text\": doc.page_content[:150] + \"...\"\n",
    "                }\n",
    "                for doc in result['source_documents']\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def ask_multiple(self, questions: List[str]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Ask multiple questions at once.\n",
    "        \n",
    "        Args:\n",
    "            questions: List of questions\n",
    "            \n",
    "        Returns:\n",
    "            List of results\n",
    "        \"\"\"\n",
    "        return [self.ask(q) for q in questions]\n",
    "\n",
    "print(\"✅ LangChainRAG class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1dda09db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing LangChain RAG system...\n",
      "Loading PDF: llm_fundamentals.pdf\n",
      "Splitting into chunks (size=500, overlap=50)\n",
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "Creating vector store...\n",
      "Initializing LLM: gpt-4o-mini\n",
      "✅ RAG system ready! (37 chunks indexed)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test LangChainRAG class\n",
    "rag = LangChainRAG(\n",
    "    pdf_path=\"llm_fundamentals.pdf\",\n",
    "    llm_model=\"gpt-4o-mini\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7859fdb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are the main components of transformer architecture?\n",
      "\n",
      "Answer:\n",
      "The main components of transformer architecture include:\n",
      "\n",
      "1. **Encoder-Decoder Structure**: The transformer consists of an encoder that processes the input and a decoder that generates the output.\n",
      "2. **Self-Attention Mechanism**: This allows the model to weigh the importance of different words in a sentence when encoding or decoding.\n",
      "3. **Feedforward Neural Networks**: These are applied to the output of the self-attention mechanism to further process the information.\n",
      "4. **Positional Encoding**: This adds information about the position of tokens in the sequence to the embeddings, as transformers do not have a built-in sense of order.\n",
      "5. **Layer Normalization**: This is used to stabilize and accelerate the training of the model.\n",
      "6. **Residual Connections**: These connections help in training deep networks by allowing gradients to flow through the network more easily.\n",
      "\n",
      "These components work together to enable the transformer to effectively process and generate sequences of data.\n",
      "\n",
      "================================================================================\n",
      "Sources (3 chunks):\n",
      "\n",
      "1. Page 2:\n",
      "   @genieincodebottle \n",
      "Advanced Architectures \n",
      "1. Diffusion Models → Generate images/video by learning to reverse noise process (DALL-E, \n",
      "Midjourney, Sta...\n",
      "\n",
      "2. Page 2:\n",
      "   sequences \n",
      "5. Autoregressive Models → Generate sequences one token at a time (GPT family, Claude, \n",
      "Gemini etc) \n",
      "6. Flow-Based Models → Invertible neur...\n",
      "\n",
      "3. Page 1:\n",
      "   @genieincodebottle \n",
      "Core LLM Building Blocks \n",
      "1. Transformer Architecture → Core design (encoder-only like BERT, decoder-only like \n",
      "GPT/Claude/Gemini ...\n"
     ]
    }
   ],
   "source": [
    "# Ask questions\n",
    "result = rag.ask(\"What are the main components of transformer architecture?\")\n",
    "\n",
    "print(f\"Question: {result['question']}\\n\")\n",
    "print(f\"Answer:\\n{result['answer']}\\n\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Sources ({len(result['sources'])} chunks):\")\n",
    "for i, source in enumerate(result['sources'], 1):\n",
    "    print(f\"\\n{i}. Page {source['page']}:\")\n",
    "    print(f\"   {source['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "19d92487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Q: What is RLHF?\n",
      "A: RLHF stands for Reinforcement Learning from Human Feedback. It is a method used to align model outputs with human preferences by incorporating feedback from humans during the training process. This approach helps improve the quality and relevance of the model's responses based on what humans find desirable.\n",
      "Sources: Pages [3, 2, 7]\n",
      "\n",
      "================================================================================\n",
      "Q: Explain quantization\n",
      "A: Quantization is a process used in machine learning and deep learning to reduce the precision of the numbers used to represent model parameters and activations. This typically involves converting floating-point numbers (which can represent a wide range of values) into lower-precision formats, such as integers. The main goals of quantization are to decrease the model size, reduce memory bandwidth, and improve inference speed, all while attempting to maintain the model's accuracy.\n",
      "\n",
      "There are different types of quantization, such as post-training quantization, where a pre-trained model is quantized after training, and quantization-aware training (QAT), where the model is trained with quantization in mind to better retain accuracy. Quantization can help deploy models on resource-constrained devices, such as mobile phones or edge devices, where computational resources are limited.\n",
      "Sources: Pages [5, 2, 1]\n",
      "\n",
      "================================================================================\n",
      "Q: What are vector databases?\n",
      "A: I don't know.\n",
      "Sources: Pages [4, 2, 5]\n"
     ]
    }
   ],
   "source": [
    "# Multiple questions\n",
    "questions = [\n",
    "    \"What is RLHF?\",\n",
    "    \"Explain quantization\",\n",
    "    \"What are vector databases?\"\n",
    "]\n",
    "\n",
    "results = rag.ask_multiple(questions)\n",
    "\n",
    "for result in results:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Q: {result['question']}\")\n",
    "    print(f\"A: {result['answer']}\")\n",
    "    print(f\"Sources: Pages {[s['page'] for s in result['sources']]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb2ffdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLNotebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
