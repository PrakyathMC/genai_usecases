{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad4f5004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Loaded Successfully\n",
      " API Key Found: True\n",
      "Key starts with: sk-proj-3Q...\n",
      "✅ Environment loaded\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "print(\"Environment Loaded Successfully\")\n",
    "print(f\" API Key Found: {'OPENAI_API_KEY' in os.environ}\")\n",
    "\n",
    "if 'OPENAI_API_KEY' in os.environ:\n",
    "    print(f\"Key starts with: {os.environ['OPENAI_API_KEY'][:10]}...\")\n",
    "print(\"✅ Environment loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88379d99",
   "metadata": {},
   "source": [
    "# Core Langchain - Document Loaders, Text Splitters, Embedding, Vector Stroe, LLMs, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37578540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8 documents\n",
      "Content Preview: @genieincodebottle \n",
      "Instagram | GitHub | Medium | ...\n",
      "Metadata Preview: {'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2025-09-02T20:12:32+05:30', 'author': 'Rajesh Srivastava', 'moddate': '2025-09-02T20:12:32+05:30', 'source': 'llm_fundamentals.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "# Document Loading\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"llm_fundamentals.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents\")\n",
    "print(f\"Content Preview: {documents[0].page_content[:50]}...\")\n",
    "print(f\"Metadata Preview: {documents[0].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddaa58f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Split 8 pages into 37 chunks\n",
      "\n",
      "Sample chunk:\n",
      "5. Attention → Highlights the most relevant tokens in context \n",
      "6. Self-Attention → Each token attends to every other token for context \n",
      "7. Cross-Attention → Connect encoder and decoder (in encoder-decoder models) \n",
      "8. Multi-Head Attention → Several attention heads capture different patterns in parallel \n",
      "9. Feed-Forward Networks → Nonlinear layers that transform representations between \n",
      "attention blocks \n",
      "10. Residual Connections → Shortcut links that preserve signals and help gradient flow\n",
      "\n",
      "Metadata: {'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2025-09-02T20:12:32+05:30', 'author': 'Rajesh Srivastava', 'moddate': '2025-09-02T20:12:32+05:30', 'source': 'llm_fundamentals.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap = 50,\n",
    "    length_function = len,\n",
    "    separators = [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"✅ Split {len(documents)} pages into {len(chunks)} chunks\")\n",
    "print(f\"\\nSample chunk:\")\n",
    "print(chunks[5].page_content)\n",
    "print(f\"\\nMetadata: {chunks[5].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060669e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Model loaded successfully.\n",
      "Embedding Dimension: 384\n",
      "First 5 values: [0.045804012566804886, -0.08357568085193634, 0.008843314833939075, -0.02673397585749626, -0.05191471427679062]\n"
     ]
    }
   ],
   "source": [
    "#embedding\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name = \"all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "test_text = \"What is a large language model?\"\n",
    "test_embedding_vector = embeddings.embed_query(test_text)\n",
    "\n",
    "print(f\"Embedding Model loaded successfully.\")\n",
    "print(f\"Embedding Dimension: {len(test_embedding_vector)}\")\n",
    "print(f\"First 5 values: {test_embedding_vector[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1cf97bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Vector store created with Chroma\n",
      "✅ Vector store created with 111 chunks\n"
     ]
    }
   ],
   "source": [
    "# vector store\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents = chunks,\n",
    "    embedding = embeddings,\n",
    "    collection_name = \"llm_fundamentals_collection\"\n",
    ")\n",
    "\n",
    "print(\"✅ Vector store created with Chroma\")\n",
    "print(f\"✅ Vector store created with {vectorstore._collection.count()} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11d06cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is RAG?\n",
      "\n",
      "Result 1:\n",
      "17. ALiBi / Relative Positional Encoding → Alternative to RoPE for long contexts \n",
      "18. Linear / Performer Attention → Efficient attention variants for ...\n",
      "Source: Page 1\n",
      "\n",
      "Result 2:\n",
      "17. ALiBi / Relative Positional Encoding → Alternative to RoPE for long contexts \n",
      "18. Linear / Performer Attention → Efficient attention variants for ...\n",
      "Source: Page 1\n",
      "\n",
      "Result 3:\n",
      "17. ALiBi / Relative Positional Encoding → Alternative to RoPE for long contexts \n",
      "18. Linear / Performer Attention → Efficient attention variants for ...\n",
      "Source: Page 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test Similarity Search\n",
    "query = \"What is RAG?\"\n",
    "results = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"Result {i}:\")\n",
    "    print(f\"{doc.page_content[:150]}...\")\n",
    "    print(f\"Source: Page {doc.metadata['page']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2daf99bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Initialized\n",
      "Response: RAG stands for \"Retrieval-Augmented Generation.\" It's a method used in natural language processing and artificial intelligence to improve how machines understand and generate text.\n",
      "\n",
      "Here’s a simple breakdown:\n",
      "\n",
      "1. **Retrieval**: When a question or prompt is given, the system first searches a large database or set of documents to find relevant information. This is like looking up facts or details in a library before answering a question.\n",
      "\n",
      "2. **Augmentation**: The system then takes the information it found and uses it to help generate a more accurate and informed response. This step enhances the output by adding context and details that the model may not have remembered on its own.\n",
      "\n",
      "3. **Generation**: Finally, the system combines the retrieved information with its own language skills to create a coherent and relevant answer.\n",
      "\n",
      "In summary, RAG works by looking up information and using it to craft better answers, making the responses more accurate and informative.\n"
     ]
    }
   ],
   "source": [
    "# LLMs\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature = 0.7,\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"]\n",
    ")\n",
    "\n",
    "response = llm.invoke(\"Explain RAG in simple terms.\")\n",
    "\n",
    "print(\"LLM Initialized\")\n",
    "print(f\"Response: {response.content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39292ac7",
   "metadata": {},
   "source": [
    "## Now lets combine all components into a RAG system using RetrievalQA form (simplest method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1ab02b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RAG QA Chain created successfully.\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.chains.retrieval_qa.base import RetrievalQA\n",
    "\n",
    "# RAG QA Chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm = llm,\n",
    "    chain_type = \"stuff\", # stuff = put all context in prompt\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\":3}),\n",
    "    return_source_documents = True\n",
    ")\n",
    "\n",
    "print(\"✅ RAG QA Chain created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08810fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is LoRA and how is it used in LLMs?\n",
      "\n",
      "Answer:\n",
      "LoRA (Low-Rank Adaptation) is a method used in fine-tuning large language models (LLMs) by updating only small parts of the model instead of the entire model. It allows for efficient adaptation of these models to specific tasks or datasets, reducing the computational resources required for training. By focusing on low-rank updates, LoRA can maintain performance while significantly lowering the memory and processing power needed, making it feasible to fine-tune large models on more modest hardware.\n",
      "\n",
      "================================================================================\n",
      "Sources (3 chunks):\n",
      "\n",
      "1. Page 2:\n",
      "   9. QLoRA → LoRA + quantization, enabling fine-tuning of huge models on modest hardware \n",
      "10. PEFT → F...\n",
      "\n",
      "2. Page 2:\n",
      "   9. QLoRA → LoRA + quantization, enabling fine-tuning of huge models on modest hardware \n",
      "10. PEFT → F...\n",
      "\n",
      "3. Page 2:\n",
      "   9. QLoRA → LoRA + quantization, enabling fine-tuning of huge models on modest hardware \n",
      "10. PEFT → F...\n"
     ]
    }
   ],
   "source": [
    "# Test RAG QA Chain\n",
    "\n",
    "question = \"What is LoRA and how is it used in LLMs?\"\n",
    "result = qa_chain.invoke({\"query\": question})\n",
    "\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(f\"Answer:\\n{result['result']}\\n\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Sources ({len(result['source_documents'])} chunks):\")\n",
    "for i, doc in enumerate(result['source_documents'], 1):\n",
    "    print(f\"\\n{i}. Page {doc.metadata['page']}:\")\n",
    "    print(f\"   {doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d11f4862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RAG QA Chain with MMR created successfully.\n",
      "Question: What is LoRA and how is it used in LLMs?\n",
      "\n",
      "Answer:\n",
      "LoRA (Low-Rank Adaptation) is a method used in large language models (LLMs) that allows for efficient fine-tuning by updating only small parts of the model while keeping the majority of its parameters frozen. This approach helps to adapt large models to specific tasks without the need for extensive computational resources. LoRA achieves this by introducing low-rank matrices into the model's architecture, which enables it to learn task-specific adaptations efficiently.\n",
      "\n",
      "================================================================================\n",
      "Sources (3 chunks):\n",
      "\n",
      "1. Page 2:\n",
      "   9. QLoRA → LoRA + quantization, enabling fine-tuning of huge models on modest hardware \n",
      "10. PEFT → F...\n",
      "\n",
      "2. Page 6:\n",
      "   requirements \n",
      "11. Explainability / Interpretability Evaluation → Assess clarity and transparency of ...\n",
      "\n",
      "3. Page 0:\n",
      "   @genieincodebottle \n",
      "Instagram | GitHub | Medium | YouTube \n",
      "How to Be Better Than Most in GenAI \n",
      " \n",
      "Co...\n"
     ]
    }
   ],
   "source": [
    "# MMR avoids duplicate/similar results\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(\n",
    "        search_type=\"mmr\",  # Maximum Marginal Relevance\n",
    "        search_kwargs={\"k\": 3, \"fetch_k\": 10}\n",
    "    ),\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "print(\"✅ RAG QA Chain with MMR created successfully.\")\n",
    "# Test RAG QA Chain with MMR\n",
    "question = \"What is LoRA and how is it used in LLMs?\"\n",
    "result = qa_chain.invoke({\"query\": question})\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(f\"Answer:\\n{result['result']}\\n\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Sources ({len(result['source_documents'])} chunks):\")\n",
    "for i, doc in enumerate(result['source_documents'], 1):\n",
    "    print(f\"\\n{i}. Page {doc.metadata['page']}:\")\n",
    "    print(f\"   {doc.page_content[:100]}...\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd432e2",
   "metadata": {},
   "source": [
    "## USing LECL Method, building personal/custom chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "40756b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Custom RAG Chain created successfully.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate # used for custom prompt\n",
    "from langchain_core.output_parsers import StrOutputParser # used for custom output parsing\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "# custom prompt\n",
    "template = \"\"\"You are an AI assistant helping users understand LLM fundamentals.\n",
    "Answer the question based ONLY on the provided context. Cite page numbers when possible.\n",
    "\n",
    "Context: \n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# custom output parser. Helper function for custom output parsing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([\n",
    "        f\"[Page {doc.metadata['page']}]\\n{doc.page_content}\"\n",
    "        for doc in docs\n",
    "    ])\n",
    "\n",
    "# build chain using LECL\n",
    "rag_chain = (\n",
    "    {\"context\": vectorstore.as_retriever() | format_docs,  \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "\n",
    ")\n",
    "print(\"✅ Custom RAG Chain created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "69db41e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is attention mechanism?\n",
      "\n",
      "Answer:\n",
      "The attention mechanism highlights the most relevant tokens in context, allowing the model to focus on specific parts of the input data that are more important for the task at hand (Page 1).\n"
     ]
    }
   ],
   "source": [
    "# Use the custom chain\n",
    "question = \"What is attention mechanism?\"\n",
    "answer = rag_chain.invoke(question)\n",
    "\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(f\"Answer:\\n{answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e946663",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLNotebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
