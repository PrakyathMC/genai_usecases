{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad4f5004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Loaded Successfully\n",
      " API Key Found: True\n",
      "Key starts with: sk-proj-3Q...\n",
      "✅ Environment loaded\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "print(\"Environment Loaded Successfully\")\n",
    "print(f\" API Key Found: {'OPENAI_API_KEY' in os.environ}\")\n",
    "\n",
    "if 'OPENAI_API_KEY' in os.environ:\n",
    "    print(f\"Key starts with: {os.environ['OPENAI_API_KEY'][:10]}...\")\n",
    "print(\"✅ Environment loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88379d99",
   "metadata": {},
   "source": [
    "# Core Langchain - Document Loaders, Text Splitters, Embedding, Vector Stroe, LLMs, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37578540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8 documents\n",
      "Content Preview: @genieincodebottle \n",
      "Instagram | GitHub | Medium | ...\n",
      "Metadata Preview: {'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2025-09-02T20:12:32+05:30', 'author': 'Rajesh Srivastava', 'moddate': '2025-09-02T20:12:32+05:30', 'source': 'llm_fundamentals.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "# Document Loading\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"llm_fundamentals.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents\")\n",
    "print(f\"Content Preview: {documents[0].page_content[:50]}...\")\n",
    "print(f\"Metadata Preview: {documents[0].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddaa58f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Split 8 pages into 37 chunks\n",
      "\n",
      "Sample chunk:\n",
      "5. Attention → Highlights the most relevant tokens in context \n",
      "6. Self-Attention → Each token attends to every other token for context \n",
      "7. Cross-Attention → Connect encoder and decoder (in encoder-decoder models) \n",
      "8. Multi-Head Attention → Several attention heads capture different patterns in parallel \n",
      "9. Feed-Forward Networks → Nonlinear layers that transform representations between \n",
      "attention blocks \n",
      "10. Residual Connections → Shortcut links that preserve signals and help gradient flow\n",
      "\n",
      "Metadata: {'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2025-09-02T20:12:32+05:30', 'author': 'Rajesh Srivastava', 'moddate': '2025-09-02T20:12:32+05:30', 'source': 'llm_fundamentals.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap = 50,\n",
    "    length_function = len,\n",
    "    separators = [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"✅ Split {len(documents)} pages into {len(chunks)} chunks\")\n",
    "print(f\"\\nSample chunk:\")\n",
    "print(chunks[5].page_content)\n",
    "print(f\"\\nMetadata: {chunks[5].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060669e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Model loaded successfully.\n",
      "Embedding Dimension: 384\n",
      "First 5 values: [0.045804012566804886, -0.08357568085193634, 0.008843314833939075, -0.02673397585749626, -0.05191471427679062]\n"
     ]
    }
   ],
   "source": [
    "#embedding\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name = \"all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "test_text = \"What is a large language model?\"\n",
    "test_embedding_vector = embeddings.embed_query(test_text)\n",
    "\n",
    "print(f\"Embedding Model loaded successfully.\")\n",
    "print(f\"Embedding Dimension: {len(test_embedding_vector)}\")\n",
    "print(f\"First 5 values: {test_embedding_vector[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1cf97bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Vector store created with Chroma\n",
      "✅ Vector store created with 111 chunks\n"
     ]
    }
   ],
   "source": [
    "# vector store\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents = chunks,\n",
    "    embedding = embeddings,\n",
    "    collection_name = \"llm_fundamentals_collection\"\n",
    ")\n",
    "\n",
    "print(\"✅ Vector store created with Chroma\")\n",
    "print(f\"✅ Vector store created with {vectorstore._collection.count()} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11d06cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is RAG?\n",
      "\n",
      "Result 1:\n",
      "17. ALiBi / Relative Positional Encoding → Alternative to RoPE for long contexts \n",
      "18. Linear / Performer Attention → Efficient attention variants for ...\n",
      "Source: Page 1\n",
      "\n",
      "Result 2:\n",
      "17. ALiBi / Relative Positional Encoding → Alternative to RoPE for long contexts \n",
      "18. Linear / Performer Attention → Efficient attention variants for ...\n",
      "Source: Page 1\n",
      "\n",
      "Result 3:\n",
      "17. ALiBi / Relative Positional Encoding → Alternative to RoPE for long contexts \n",
      "18. Linear / Performer Attention → Efficient attention variants for ...\n",
      "Source: Page 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test Similarity Search\n",
    "query = \"What is RAG?\"\n",
    "results = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"Result {i}:\")\n",
    "    print(f\"{doc.page_content[:150]}...\")\n",
    "    print(f\"Source: Page {doc.metadata['page']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2daf99bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Initialized\n",
      "Response: RAG stands for \"Retrieval-Augmented Generation.\" It's a method used in natural language processing and artificial intelligence to improve how machines understand and generate text.\n",
      "\n",
      "Here’s a simple breakdown:\n",
      "\n",
      "1. **Retrieval**: When a question or prompt is given, the system first searches a large database or set of documents to find relevant information. This is like looking up facts or details in a library before answering a question.\n",
      "\n",
      "2. **Augmentation**: The system then takes the information it found and uses it to help generate a more accurate and informed response. This step enhances the output by adding context and details that the model may not have remembered on its own.\n",
      "\n",
      "3. **Generation**: Finally, the system combines the retrieved information with its own language skills to create a coherent and relevant answer.\n",
      "\n",
      "In summary, RAG works by looking up information and using it to craft better answers, making the responses more accurate and informative.\n"
     ]
    }
   ],
   "source": [
    "# LLMs\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature = 0.7,\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"]\n",
    ")\n",
    "\n",
    "response = llm.invoke(\"Explain RAG in simple terms.\")\n",
    "\n",
    "print(\"LLM Initialized\")\n",
    "print(f\"Response: {response.content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39292ac7",
   "metadata": {},
   "source": [
    "## Now lets combine all components into a RAG system using RetrievalQA form (simplest method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1ab02b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RAG QA Chain created successfully.\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.chains.retrieval_qa.base import RetrievalQA\n",
    "\n",
    "# RAG QA Chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm = llm,\n",
    "    chain_type = \"stuff\", # stuff = put all context in prompt\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\":3}),\n",
    "    return_source_documents = True\n",
    ")\n",
    "\n",
    "print(\"✅ RAG QA Chain created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08810fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is LoRA and how is it used in LLMs?\n",
      "\n",
      "Answer:\n",
      "LoRA (Low-Rank Adaptation) is a method used in fine-tuning large language models (LLMs) by updating only small parts of the model instead of the entire model. It allows for efficient adaptation of these models to specific tasks or datasets, reducing the computational resources required for training. By focusing on low-rank updates, LoRA can maintain performance while significantly lowering the memory and processing power needed, making it feasible to fine-tune large models on more modest hardware.\n",
      "\n",
      "================================================================================\n",
      "Sources (3 chunks):\n",
      "\n",
      "1. Page 2:\n",
      "   9. QLoRA → LoRA + quantization, enabling fine-tuning of huge models on modest hardware \n",
      "10. PEFT → F...\n",
      "\n",
      "2. Page 2:\n",
      "   9. QLoRA → LoRA + quantization, enabling fine-tuning of huge models on modest hardware \n",
      "10. PEFT → F...\n",
      "\n",
      "3. Page 2:\n",
      "   9. QLoRA → LoRA + quantization, enabling fine-tuning of huge models on modest hardware \n",
      "10. PEFT → F...\n"
     ]
    }
   ],
   "source": [
    "# Test RAG QA Chain\n",
    "\n",
    "question = \"What is LoRA and how is it used in LLMs?\"\n",
    "result = qa_chain.invoke({\"query\": question})\n",
    "\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(f\"Answer:\\n{result['result']}\\n\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Sources ({len(result['source_documents'])} chunks):\")\n",
    "for i, doc in enumerate(result['source_documents'], 1):\n",
    "    print(f\"\\n{i}. Page {doc.metadata['page']}:\")\n",
    "    print(f\"   {doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d11f4862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RAG QA Chain with MMR created successfully.\n",
      "Question: What is LoRA and how is it used in LLMs?\n",
      "\n",
      "Answer:\n",
      "LoRA (Low-Rank Adaptation) is a method used in large language models (LLMs) that allows for efficient fine-tuning by updating only small parts of the model while keeping the majority of its parameters frozen. This approach helps to adapt large models to specific tasks without the need for extensive computational resources. LoRA achieves this by introducing low-rank matrices into the model's architecture, which enables it to learn task-specific adaptations efficiently.\n",
      "\n",
      "================================================================================\n",
      "Sources (3 chunks):\n",
      "\n",
      "1. Page 2:\n",
      "   9. QLoRA → LoRA + quantization, enabling fine-tuning of huge models on modest hardware \n",
      "10. PEFT → F...\n",
      "\n",
      "2. Page 6:\n",
      "   requirements \n",
      "11. Explainability / Interpretability Evaluation → Assess clarity and transparency of ...\n",
      "\n",
      "3. Page 0:\n",
      "   @genieincodebottle \n",
      "Instagram | GitHub | Medium | YouTube \n",
      "How to Be Better Than Most in GenAI \n",
      " \n",
      "Co...\n"
     ]
    }
   ],
   "source": [
    "# MMR avoids duplicate/similar results\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(\n",
    "        search_type=\"mmr\",  # Maximum Marginal Relevance\n",
    "        search_kwargs={\"k\": 3, \"fetch_k\": 10}\n",
    "    ),\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "print(\"✅ RAG QA Chain with MMR created successfully.\")\n",
    "# Test RAG QA Chain with MMR\n",
    "question = \"What is LoRA and how is it used in LLMs?\"\n",
    "result = qa_chain.invoke({\"query\": question})\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(f\"Answer:\\n{result['result']}\\n\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Sources ({len(result['source_documents'])} chunks):\")\n",
    "for i, doc in enumerate(result['source_documents'], 1):\n",
    "    print(f\"\\n{i}. Page {doc.metadata['page']}:\")\n",
    "    print(f\"   {doc.page_content[:100]}...\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd432e2",
   "metadata": {},
   "source": [
    "## USing LECL Method, building personal/custom chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40756b99",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4232659719.py, line 28)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[28], line 28\u001b[1;36m\u001b[0m\n\u001b[1;33m    {\"context\": vectorstore.as_retriever(), | format_docs,  \"question\": RunnablePassthrough()}\u001b[0m\n\u001b[1;37m                                            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate # used for custom prompt\n",
    "from langchain_core.output_parsers import StrOutputParser # used for custom output parsing\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "# custom prompt\n",
    "template = \"\"\"You are an AI assistant helping users understand LLM fundamentals.\n",
    "Answer the question based ONLY on the provided context. Cite page numbers when possible.\n",
    "\n",
    "Context: \n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# custom output parser. Helper function for custom output parsing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([\n",
    "        f\"[Page {doc.metadata['page']}]\\n{doc.page_content}\"\n",
    "        for doc in docs\n",
    "    ])\n",
    "\n",
    "# build chain using LECL\n",
    "rag_chain = (\n",
    "    {\"context\": vectorstore.as_retriever() | format_docs,  \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "\n",
    ")\n",
    "print(\"✅ Custom RAG Chain created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69db41e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Input to ChatPromptTemplate is missing variables {'question'}.  Expected: ['context', 'question'] Received: ['context', 'Question']\\nNote: if you intended {question} to be part of the string and not a variable, please escape it with double curly braces like: '{{question}}'.\\nFor troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/INVALID_PROMPT_INPUT \"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Use the custom chain\u001b[39;00m\n\u001b[0;32m      2\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is attention mechanism?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43mrag_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00manswer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\lapde\\anaconda3\\envs\\MLNotebook\\lib\\site-packages\\langchain_core\\runnables\\base.py:3151\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3149\u001b[0m                 input_ \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, input_, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3150\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3151\u001b[0m                 input_ \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3152\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   3153\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\lapde\\anaconda3\\envs\\MLNotebook\\lib\\site-packages\\langchain_core\\prompts\\base.py:217\u001b[0m, in \u001b[0;36mBasePromptTemplate.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtags:\n\u001b[0;32m    216\u001b[0m     config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtags\n\u001b[1;32m--> 217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_format_prompt_with_error_handling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserialized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_serialized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lapde\\anaconda3\\envs\\MLNotebook\\lib\\site-packages\\langchain_core\\runnables\\base.py:2058\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[1;34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[0m\n\u001b[0;32m   2054\u001b[0m     child_config \u001b[38;5;241m=\u001b[39m patch_config(config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child())\n\u001b[0;32m   2055\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[0;32m   2056\u001b[0m         output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[0;32m   2057\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m-> 2058\u001b[0m             context\u001b[38;5;241m.\u001b[39mrun(\n\u001b[0;32m   2059\u001b[0m                 call_func_with_variable_args,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   2060\u001b[0m                 func,\n\u001b[0;32m   2061\u001b[0m                 input_,\n\u001b[0;32m   2062\u001b[0m                 config,\n\u001b[0;32m   2063\u001b[0m                 run_manager,\n\u001b[0;32m   2064\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2065\u001b[0m             ),\n\u001b[0;32m   2066\u001b[0m         )\n\u001b[0;32m   2067\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   2068\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32mc:\\Users\\lapde\\anaconda3\\envs\\MLNotebook\\lib\\site-packages\\langchain_core\\runnables\\config.py:435\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[0;32m    434\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[1;32m--> 435\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lapde\\anaconda3\\envs\\MLNotebook\\lib\\site-packages\\langchain_core\\prompts\\base.py:190\u001b[0m, in \u001b[0;36mBasePromptTemplate._format_prompt_with_error_handling\u001b[1;34m(self, inner_input)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_format_prompt_with_error_handling\u001b[39m(\u001b[38;5;28mself\u001b[39m, inner_input: \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PromptValue:\n\u001b[1;32m--> 190\u001b[0m     inner_input_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_prompt(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minner_input_)\n",
      "File \u001b[1;32mc:\\Users\\lapde\\anaconda3\\envs\\MLNotebook\\lib\\site-packages\\langchain_core\\prompts\\base.py:184\u001b[0m, in \u001b[0;36mBasePromptTemplate._validate_input\u001b[1;34m(self, inner_input)\u001b[0m\n\u001b[0;32m    178\u001b[0m     example_key \u001b[38;5;241m=\u001b[39m missing\u001b[38;5;241m.\u001b[39mpop()\n\u001b[0;32m    179\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    180\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mNote: if you intended \u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexample_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;124m to be part of the string\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    181\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and not a variable, please escape it with double curly braces like: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    182\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexample_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    183\u001b[0m     )\n\u001b[1;32m--> 184\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[0;32m    185\u001b[0m         create_message(message\u001b[38;5;241m=\u001b[39mmsg, error_code\u001b[38;5;241m=\u001b[39mErrorCode\u001b[38;5;241m.\u001b[39mINVALID_PROMPT_INPUT)\n\u001b[0;32m    186\u001b[0m     )\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inner_input_\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Input to ChatPromptTemplate is missing variables {'question'}.  Expected: ['context', 'question'] Received: ['context', 'Question']\\nNote: if you intended {question} to be part of the string and not a variable, please escape it with double curly braces like: '{{question}}'.\\nFor troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/INVALID_PROMPT_INPUT \""
     ]
    }
   ],
   "source": [
    "# Use the custom chain\n",
    "question = \"What is attention mechanism?\"\n",
    "answer = rag_chain.invoke(question)\n",
    "\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(f\"Answer:\\n{answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e946663",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLNotebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
