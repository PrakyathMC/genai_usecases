{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5baf01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Variable loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from typing import List, Tuple, Dict\n",
    "import numpy as np\n",
    "\n",
    "# load dotenv\n",
    "load_dotenv()\n",
    "print(\"Environment Variable loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd19be72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge base has 8 documents.\n",
      "\n",
      "Sample Example Document:\n",
      "Machine learning is a subset of artificial intelligence that enables computers to learn from data without explicit programming. It uses algorithms to identify patterns and make predictions.\n"
     ]
    }
   ],
   "source": [
    "#sample knowledge base\n",
    "knowledge_base = [ \"Machine learning is a subset of artificial intelligence that enables computers to learn from data without explicit programming. It uses algorithms to identify patterns and make predictions.\",\n",
    "    \n",
    "    \"Deep learning is a type of machine learning that uses neural networks with multiple layers. It's particularly effective for image recognition, natural language processing, and complex pattern recognition tasks.\",\n",
    "    \n",
    "    \"Natural Language Processing (NLP) is a field of AI that focuses on the interaction between computers and human language. It enables machines to understand, interpret, and generate human language.\",\n",
    "    \n",
    "    \"Embeddings are numerical representations of text that capture semantic meaning. Similar texts have similar embedding vectors, which enables semantic search and similarity comparison.\",\n",
    "    \n",
    "    \"RAG (Retrieval Augmented Generation) combines information retrieval with text generation. It retrieves relevant context from a knowledge base and uses it to generate more accurate and informed responses.\",\n",
    "    \n",
    "    \"OpenAI's GPT models are large language models trained on diverse internet text. They can perform various tasks like text generation, summarization, translation, and question answering.\",\n",
    "    \n",
    "    \"Vector databases store embeddings and enable fast similarity search. Popular options include Chroma, Pinecone, Weaviate, and FAISS. They're essential for production RAG systems.\",\n",
    "    \n",
    "    \"Fine-tuning is the process of adapting a pre-trained model to a specific task by training it on domain-specific data. It's useful when you need specialized behavior beyond what prompting can achieve.\"\n",
    "]\n",
    "\n",
    "print(f\"Knowledge base has {len(knowledge_base)} documents.\")\n",
    "print(f\"\\nSample Example Document:\\n{knowledge_base[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00d13212",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lapde\\anaconda3\\envs\\MLNotebook\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created embeddings with shape: (8, 384)\n",
      "Each document is respresented as a 384-dimensional vector.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "def create_embeddings(texts:List[str], model: str=\"all-MiniLM-L6-v2\") -> np.ndarray:\n",
    "    \"\"\"args:\n",
    "    texts: List of text data to be embedded\n",
    "    model: Model name from sentence transformers\n",
    "    \n",
    "    Returns:\n",
    "    NumPy array of embeddings (shape:[num_texts, embedding_dimension])\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer(model)\n",
    "    embeddings = model.encode(texts)\n",
    "    return embeddings\n",
    "\n",
    "# Call function to create embeddings\n",
    "kb_embeddings = create_embeddings(knowledge_base)\n",
    "\n",
    "print(f\"Created embeddings with shape: {kb_embeddings.shape}\")\n",
    "print(f\"Each document is respresented as a {kb_embeddings.shape[1]}-dimensional vector.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2de9df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the meaning of deep learning?\n",
      "\n",
      "Result 1 (similarity: 0.7687):\n",
      "Deep learning is a type of machine learning that uses neural networks with multiple layers. It's particularly effective for image recognition, natural language processing, and complex pattern recognition tasks.\n",
      "\n",
      "Result 2 (similarity: 0.5162):\n",
      "Machine learning is a subset of artificial intelligence that enables computers to learn from data without explicit programming. It uses algorithms to identify patterns and make predictions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def retrieve_relevant_docs(\n",
    "    query: str,\n",
    "    knowledge_base: List[str],\n",
    "    kb_embeddings: np.ndarray,\n",
    "    top_k: int=2) -> List[Tuple[str, float]]:\n",
    "\n",
    "    \"\"\"Args:\n",
    "    query: User's Question\n",
    "    knowledge_base: List of documet texts\n",
    "    kb_embeddings: NumPy array of knowledge base embeddings\n",
    "    top_k: Number of top relevant documents to retrieve\n",
    "    \"\"\"\n",
    "\n",
    "    # Create embeddings for query\n",
    "    query_embedding = create_embeddings([query])\n",
    "\n",
    "    # create similarity score between query and knowledge base\n",
    "    similarities = cosine_similarity(query_embedding, kb_embeddings)[0] \n",
    "\n",
    "    # get top_k indices \n",
    "    top_k_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "\n",
    "    # return document with similarity scores\n",
    "    results = [(knowledge_base[i], similarities[i]) for i in top_k_indices]\n",
    "    return results\n",
    "\n",
    "# call function to retrieve relevant documents\n",
    "query = \"What is the meaning of deep learning?\"\n",
    "relevant_docs = retrieve_relevant_docs(query, knowledge_base, kb_embeddings, top_k=2)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "for i, (doc, score) in enumerate(relevant_docs, 1):\n",
    "    print(f\"Result {i} (similarity: {score:.4f}):\")\n",
    "    print(f\"{doc}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e20d1a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is deep learning and what is it used for?\n",
      "\n",
      "Answer:\n",
      "Deep learning is a type of machine learning that utilizes neural networks with multiple layers. It is particularly effective for tasks such as image recognition, natural language processing, and complex pattern recognition.\n",
      "\n",
      "Tokens used: 205\n",
      "\n",
      "Sources used (similarity scores):\n",
      "1. [0.825] Deep learning is a type of machine learning that uses neural networks with multi...\n",
      "2. [0.507] Machine learning is a subset of artificial intelligence that enables computers t...\n",
      "3. [0.379] Natural Language Processing (NLP) is a field of AI that focuses on the interacti...\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "def generate_answer(\n",
    "    query: str,\n",
    "    context_docs: List[Tuple[str, float]],\n",
    "    model: str = \"gpt-4o-mini\") -> Dict[str, any]:\n",
    "\n",
    "    \"\"\"Args:\n",
    "    query: User's question\n",
    "    context_docs: Retrieved documents with similarity scores\n",
    "    model : OpenAI model name\n",
    "    \n",
    "    Returns: \n",
    "    Dictionary with answers and metadata\n",
    "    \"\"\"\n",
    "\n",
    "    # instantiate OpenAI client\n",
    "    client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "    # Prepare context by concatenating retrieved documents\n",
    "    context = \"\\n\\n\".join([doc for doc, _ in context_docs])\n",
    "\n",
    "    # Create prompt with context and query\n",
    "    system_prompt = \"\"\"You are a helpful AI assistant. Answer the user's question based on the provided context. \n",
    "If the context doesn't contain relevant information, say so rather than making up an answer.\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"Context: {context}\n",
    "\n",
    "    Question: {query}\n",
    "\n",
    "    Answer based on the above context: \"\"\"\n",
    "\n",
    "    # call openai chat completion\n",
    "    response = client.chat.completions.create(\n",
    "        model = model,\n",
    "        messages = [\n",
    "            {\"role\": \"system\" , \"content\": system_prompt},\n",
    "            {\"role\": \"user\" , \"content\": user_prompt}\n",
    "        ],\n",
    "\n",
    "        temperature = 0.6,\n",
    "        max_tokens = 300\n",
    "    )\n",
    "\n",
    "    return{\n",
    "        \"answer\": response.choices[0].message.content,\n",
    "        \"tokens_used\": response.usage.total_tokens,\n",
    "        \"sources\": [doc for doc, _ in context_docs],\n",
    "        \"similarity_scores\":[score for _, score in context_docs]\n",
    "    }\n",
    "\n",
    "# call function to generate answer\n",
    "query = \"What is deep learning and what is it used for?\"\n",
    "relevant_docs = retrieve_relevant_docs(query, knowledge_base, kb_embeddings, top_k=3)\n",
    "result = generate_answer(query, relevant_docs)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(f\"Answer:\\n{result['answer']}\\n\")\n",
    "print(f\"Tokens used: {result['tokens_used']}\")\n",
    "print(f\"\\nSources used (similarity scores):\")\n",
    "for i, (source, score) in enumerate(zip(result['sources'], result['similarity_scores']), 1):\n",
    "    print(f\"{i}. [{score:.3f}] {source[:80]}...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913fb5ef",
   "metadata": {},
   "source": [
    "# RAG with class instead of Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffe0d948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SimpleRAG class defined!\n"
     ]
    }
   ],
   "source": [
    "class BasicRAG:\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            embedding_model: str = \"all-MiniLM-L6-v2\",\n",
    "            llm_model: str = \"gpt-4o-mini\",\n",
    "            top_k: int=3\n",
    "    ):\n",
    "        \n",
    "        self.embedding_model = SentenceTransformer(embedding_model)\n",
    "        self.llm_model = llm_model\n",
    "        self.top_k = top_k\n",
    "        self.openai_client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "        # Store knowledge base and embeddings\n",
    "        self.documents = []\n",
    "        self.embeddings = np.ndarray = None\n",
    "\n",
    "    def add_documents(self, documents:List[str]):\n",
    "        \"\"\"Args:\n",
    "        texts: List of document texts to add to knowledge base\n",
    "        \"\"\"\n",
    "        self.documents = documents\n",
    "        print(f\"Embedding {len(documents)} documents!\")\n",
    "        self.embeddings = self.embedding_model.encode(documents)\n",
    "        print(f\"{len(documents)} documents embedded successfully.\")\n",
    "\n",
    "    def retrieve(self, query: str) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Retrieve most relevant documents for a query.\n",
    "        \n",
    "        Args:\n",
    "            query: User's question\n",
    "            \n",
    "        Returns:\n",
    "            List of (document, similarity_score) tuples\n",
    "        \"\"\"\n",
    "        if self.embedding is None:\n",
    "            raise ValueError(\"Knowledge base is empty. Add documents first., Call add_documents().\")\n",
    "        \n",
    "        # embed query\n",
    "        query_embedding = self.embedding_model.encode([query])\n",
    "\n",
    "        # compute similarities\n",
    "        similarities = cosine_similarity(query_embedding, self.embeddings)[0]\n",
    "\n",
    "        # get top_k indices\n",
    "        top_indices = np.argsort(similarities)[::-1][:self.top_k]\n",
    "\n",
    "        return [(self.documents[i], similarities[i]) for i in top_indices]\n",
    "    \n",
    "    def generate(self, query: str, context_docs: List[Tuple[str, float]]) -> Dict:\n",
    "        \"\"\"Generate answer using LLM based on query and context documents.\n",
    "        \n",
    "        Args:\n",
    "            query: User's question\n",
    "            context_docs: Retrieved documents with similarity scores \"\"\"\n",
    "        \n",
    "        # prepare and join context\n",
    "        context = \"\\n\\n\".join([doc for doc, _ in context_docs])\n",
    "\n",
    "        system_prompt = \"\"\"You are a helpful AI assistant. Answer questions based on the provided context.\n",
    "If the context doesn't contain enough information, acknowledge this limitation.\"\"\"\n",
    "\n",
    "        user_prompt = f\"\"\"Context: {context}\n",
    "Question: {query}\n",
    "\n",
    "Answer based on the above context: \"\"\"\n",
    "        \n",
    "        #call openai chat  to generate answer\n",
    "        response = self.openai_client.chat.completions.create(\n",
    "            model = self.llm_model,\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature = 0.7,\n",
    "            max_tokens = 300\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"answer\": response.choices[0].message.content,\n",
    "            \"tokens_used\": response.usage.total_tokens,\n",
    "            \"sources\": context_docs\n",
    "        }\n",
    "    \n",
    "    def query(self, question: str, return_sources: bool=True) -> Dict:\n",
    "        \"\"\"End-to-end query processing: retrieve relevant documents and generate answer.\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            return_sources: Whether to include source documents in the output \"\"\"\n",
    "        \n",
    "        # retrieve relevant documents\n",
    "        relevant_docs = self.retrieve(question)\n",
    "\n",
    "        # generate answer\n",
    "        result = self.generate(question, relevant_docs)\n",
    "\n",
    "        #format output/response\n",
    "        response = {\n",
    "            \"question\": question,\n",
    "            \"answer\": result[\"answer\"],\n",
    "            \"tokens_used\": result[\"tokens_used\"]\n",
    "\n",
    "        }\n",
    "\n",
    "        if return_sources:\n",
    "            response[\"sources\"] = [\n",
    "                {\"text\": doc, \"similarity\": float(score)}\n",
    "                for doc, score in result[\"sources\"]\n",
    "            ]\n",
    "        \n",
    "        return response\n",
    "\n",
    "print(\"✅ SimpleRAG class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cef2bc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SimpleRAG class defined!!\n"
     ]
    }
   ],
   "source": [
    "class SimpleRAG:\n",
    "    \"\"\"\n",
    "    A production-quality RAG system.\n",
    "    \n",
    "    This class encapsulates the entire RAG pipeline:\n",
    "    - Document storage and embedding\n",
    "    - Semantic search/retrieval\n",
    "    - Answer generation\n",
    "    \n",
    "    Why use a class?\n",
    "    - Maintains state (knowledge base, embeddings)\n",
    "    - Provides a clean API (add_documents, query)\n",
    "    - Easy to configure and reuse\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_model: str = \"all-MiniLM-L6-v2\",\n",
    "        llm_model: str = \"gpt-4o-mini\",\n",
    "        top_k: int = 3\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the RAG system.\n",
    "        \n",
    "        Args:\n",
    "            embedding_model: Sentence transformer model name\n",
    "            llm_model: OpenAI model for generation\n",
    "            top_k: Number of documents to retrieve\n",
    "        \"\"\"\n",
    "        self.embedding_model = SentenceTransformer(embedding_model)\n",
    "        self.llm_model = llm_model\n",
    "        self.top_k = top_k\n",
    "        self.openai_client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "        \n",
    "        # State: stores documents and their embeddings\n",
    "        self.documents: List[str] = []\n",
    "        self.embeddings: np.ndarray = None\n",
    "    \n",
    "    def add_documents(self, documents: List[str]) -> None:\n",
    "        \"\"\"\n",
    "        Add documents to the knowledge base.\n",
    "        \n",
    "        Args:\n",
    "            documents: List of text documents to add\n",
    "            \n",
    "        In production:\n",
    "            - This might load from database/files\n",
    "            - Could handle incremental updates\n",
    "            - Might include document metadata\n",
    "        \"\"\"\n",
    "        self.documents = documents\n",
    "        print(f\"Embedding {len(documents)} documents...\")\n",
    "        self.embeddings = self.embedding_model.encode(documents)\n",
    "        print(f\"✅ {len(documents)} documents indexed\")\n",
    "    \n",
    "    def retrieve(self, query: str) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Retrieve most relevant documents for a query.\n",
    "        \n",
    "        Args:\n",
    "            query: User's question\n",
    "            \n",
    "        Returns:\n",
    "            List of (document, similarity_score) tuples\n",
    "        \"\"\"\n",
    "        if self.embeddings is None:\n",
    "            raise ValueError(\"No documents added. Call add_documents() first.\")\n",
    "        \n",
    "        # Embed query\n",
    "        query_embedding = self.embedding_model.encode([query])\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = cosine_similarity(query_embedding, self.embeddings)[0]\n",
    "        \n",
    "        # Get top-k\n",
    "        top_indices = np.argsort(similarities)[::-1][:self.top_k]\n",
    "        \n",
    "        return [(self.documents[i], similarities[i]) for i in top_indices]\n",
    "    \n",
    "    def generate(self, query: str, context_docs: List[Tuple[str, float]]) -> Dict:\n",
    "        \"\"\"\n",
    "        Generate answer using retrieved context.\n",
    "        \n",
    "        Args:\n",
    "            query: User's question\n",
    "            context_docs: Retrieved documents with scores\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with answer and metadata\n",
    "        \"\"\"\n",
    "        # Build context\n",
    "        context = \"\\n\\n\".join([doc for doc, _ in context_docs])\n",
    "        \n",
    "        # Create prompt\n",
    "        system_prompt = \"\"\"You are a helpful AI assistant. Answer questions based on the provided context.\n",
    "If the context doesn't contain enough information, acknowledge this limitation.\"\"\"\n",
    "        \n",
    "        user_prompt = f\"\"\"Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        # Generate\n",
    "        response = self.openai_client.chat.completions.create(\n",
    "            model=self.llm_model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=300\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"answer\": response.choices[0].message.content,\n",
    "            \"tokens\": response.usage.total_tokens,\n",
    "            \"sources\": context_docs\n",
    "        }\n",
    "    \n",
    "    def query(self, question: str, return_sources: bool = True) -> Dict:\n",
    "        \"\"\"\n",
    "        Main method: Query the RAG system.\n",
    "        \n",
    "        This is the public API - simple to use!\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            return_sources: Whether to include source documents\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with answer and optional metadata\n",
    "        \"\"\"\n",
    "        # Step 1: Retrieve\n",
    "        relevant_docs = self.retrieve(question)\n",
    "        \n",
    "        # Step 2: Generate\n",
    "        result = self.generate(question, relevant_docs)\n",
    "        \n",
    "        # Format response\n",
    "        response = {\n",
    "            \"question\": question,\n",
    "            \"answer\": result[\"answer\"],\n",
    "            \"tokens_used\": result[\"tokens\"]\n",
    "        }\n",
    "        \n",
    "        if return_sources:\n",
    "            response[\"sources\"] = [\n",
    "                {\"text\": doc, \"similarity\": float(score)}\n",
    "                for doc, score in result[\"sources\"]\n",
    "            ]\n",
    "        \n",
    "        return response\n",
    "\n",
    "print(\"✅ SimpleRAG class defined!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bda5e60",
   "metadata": {},
   "source": [
    "## Using the defined Rag Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b49df95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 8 documents...\n",
      "✅ 8 documents indexed\n",
      "Question: What is RAG and why is it useful?.\n",
      "\n",
      "Answer:\n",
      "RAG (Retrieval Augmented Generation) is a methodology that combines information retrieval with text generation. It works by retrieving relevant context from a knowledge base and using that information to generate more accurate and informed responses. This is useful because it allows the generation of responses that are not only coherent but also grounded in factual information, enhancing the reliability and relevance of the generated content. By leveraging external knowledge, RAG systems can provide more comprehensive and contextually appropriate answers, making them particularly valuable in applications like customer support, content creation, and question-answering systems.\n",
      "\n",
      "Tokens used: 272\n",
      "\n",
      "Sources:\n",
      "1. [Similarity: 0.621]\n",
      "   RAG (Retrieval Augmented Generation) combines information retrieval with text generation. It retriev...\n",
      "\n",
      "2. [Similarity: 0.448]\n",
      "   Vector databases store embeddings and enable fast similarity search. Popular options include Chroma,...\n",
      "\n",
      "3. [Similarity: 0.156]\n",
      "   Fine-tuning is the process of adapting a pre-trained model to a specific task by training it on doma...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize RAG system\n",
    "rag = SimpleRAG(\n",
    "    embedding_model = \"all-MiniLM-L6-v2\",\n",
    "    llm_model = \"gpt-4o-mini\",\n",
    "    top_k = 3\n",
    ")\n",
    "\n",
    "# add documents to RAG knowledge base\n",
    "rag.add_documents(knowledge_base)\n",
    "\n",
    "# query RAG system\n",
    "result = rag.query(\"What is RAG and why is it useful?.\")\n",
    "\n",
    "print(f\"Question: {result['question']}\\n\")\n",
    "print(f\"Answer:\\n{result['answer']}\\n\")\n",
    "print(f\"Tokens used: {result['tokens_used']}\\n\")\n",
    "print(\"Sources:\")\n",
    "for i, source in enumerate(result['sources'], 1):\n",
    "    print(f\"{i}. [Similarity: {source['similarity']:.3f}]\")\n",
    "    print(f\"   {source['text'][:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89344e82",
   "metadata": {},
   "source": [
    "# RAG with different LLM providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14058d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MultiProviderRAG class defined\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "from anthropic import Anthropic\n",
    "class MultiProviderRAG(SimpleRAG):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        provider: str=\"openai\",\n",
    "        embedding_model: str = \"all-MiniLM-L6-v2\",\n",
    "        llm_model: str = None,\n",
    "        top_k: int = 3):\n",
    "\n",
    "        # set default model per provider\n",
    "        default_models = {\n",
    "            \"openai\": \"gpt-4o-mini\",\n",
    "            \"google\": \"gemini-2.5-flash\",\n",
    "            \"anthropic\": \"claude-3-5-haiku-20241022\"\n",
    "        }\n",
    "\n",
    "        self.provider = provider\n",
    "        llm_model = llm_model or default_models[provider]\n",
    "\n",
    "        # Initialize the parent class with the specified parameters\n",
    "        super().__init__(\n",
    "            embedding_model,\n",
    "            llm_model,\n",
    "            top_k \n",
    "        )\n",
    "\n",
    "        if provider == \"gemini\":\n",
    "            self.gemini_client = genai.Client(api_key=os.environ[\"GOOGLE_GENAI_API_KEY\"])\n",
    "        if provider == \"anthropic\":\n",
    "            self.anthropic_client = Anthropic(api_key=os.environ[\"ANTHROPIC_API_KEY\"])\n",
    "\n",
    "        def generate(self, query: str, context_docs: List[Tuple[str, float]]) -> Dict:\n",
    "            \"\"\"\n",
    "        Generate answer using the configured provider.\n",
    "        \n",
    "        This overrides the parent method to support multiple providers.\n",
    "        \"\"\"\n",
    "            # prepare the context\n",
    "            context = \"\\n\\n\".join([doc for doc, _ in context_docs])\n",
    "\n",
    "            if self.provider == \"openai\":\n",
    "                return self.generate_openai(query, context)\n",
    "            if self.provider == \"gemini\":\n",
    "                return self.generate_gemini(query, context)\n",
    "            if self.provider == \"anthropic\":\n",
    "                return self.generate_claude(query, context)\n",
    "            \n",
    "        def generate_openai(self, query: str, context: str) -> Dict:\n",
    "            response = self.openai_client.chat.completions.create(\n",
    "                model = self.llm_model,\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"context\": \"Answer based on context.\"},\n",
    "                    {\"role\": \"user\", \"context\": f\"Context:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"}\n",
    "                ],\n",
    "                temperature = 0.7,\n",
    "                max_tokens = 300\n",
    "            )\n",
    "            return {\n",
    "                \"answer\": response.choices[0].message.content,\n",
    "                \"tokens\": response.usage.total_tokens,\n",
    "                \"sources\": context\n",
    "            }\n",
    "            \n",
    "        def generate_gemini(self, query: str, context: str) -> Dict:\n",
    "            prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer based on the context:\"\n",
    "\n",
    "            response = self.gemini_client.models.generate_content(\n",
    "                model = self.llm_model,\n",
    "                contents = prompt,\n",
    "                config = types.GenerateContentConfig(\n",
    "                    temperature = 0.7,\n",
    "                    max_output_tokens = 300\n",
    "                )\n",
    "            )\n",
    "            return {\n",
    "                \"answer\": response.text,\n",
    "                \"tokens\": response.usage.metadata.total_token_count,\n",
    "                \"sources\": context\n",
    "            }\n",
    "        \n",
    "        def _generate_claude(self, query: str, context: str) -> Dict:\n",
    "            \"\"\"Generate using Claude.\"\"\"\n",
    "            prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer based on the context above:\"\n",
    "        \n",
    "            response = self.claude_client.messages.create(\n",
    "                model=self.llm_model,\n",
    "                max_tokens=300,\n",
    "                temperature=0.7,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            return {\n",
    "\n",
    "                \"answer\": response.content[0].text,\n",
    "                \"tokens\": response.usage.input_tokens + response.usage.output_tokens,\n",
    "                \"sources\": context\n",
    "            }\n",
    "print(\"✅ MultiProviderRAG class defined\")         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "257e5cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Testing with OPENAI\n",
      "================================================================================\n",
      "Embedding 8 documents...\n",
      "✅ 8 documents indexed\n",
      "\n",
      "Answer:\n",
      "Deep learning is a type of machine learning that utilizes neural networks with multiple layers to process data. It is particularly effective in tasks such as image recognition, natural language processing, and complex pattern recognition. By using these multi-layered networks, deep learning can automatically learn and extract features from large amounts of data, leading to improved accuracy and performance in various applications.\n",
      "\n",
      "Tokens used: 223\n",
      "\n",
      "================================================================================\n",
      "Testing with GOOGLE\n",
      "================================================================================\n",
      "Embedding 8 documents...\n",
      "✅ 8 documents indexed\n",
      "Error with google: Error code: 404 - {'error': {'message': 'The model `gemini-2.5-flash` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
      "Make sure you have the API key set in your .env file\n",
      "\n",
      "================================================================================\n",
      "Testing with ANTHROPIC\n",
      "================================================================================\n",
      "Error with anthropic: 'ANTHROPIC_API_KEY'\n",
      "Make sure you have the API key set in your .env file\n"
     ]
    }
   ],
   "source": [
    "# compare and test different providers\n",
    "test_questions = \"What is deep learning and how is it used?\"\n",
    "\n",
    "providers = [\"openai\", \"google\", \"anthropic\"]\n",
    "\n",
    "for provider in providers:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Testing with {provider.upper()}\")\n",
    "    print('='*80)\n",
    "\n",
    "    try:\n",
    "        # Create RAG instance for the provider\n",
    "        rag = MultiProviderRAG(\n",
    "            provider=provider,\n",
    "            top_k=3\n",
    "        )\n",
    "\n",
    "        rag.add_documents(knowledge_base)\n",
    "        result = rag.query(test_questions)\n",
    "\n",
    "        print(f\"\\nAnswer:\\n{result['answer']}\")\n",
    "        print(f\"\\nTokens used: {result['tokens_used']}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error with {provider}: {e}\")\n",
    "        print(\"Make sure you have the API key set in your .env file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c5286b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLNotebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
