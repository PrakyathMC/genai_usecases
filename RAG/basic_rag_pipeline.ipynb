{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5baf01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Variable loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from typing import List, Tuple, Dict\n",
    "import numpy as np\n",
    "\n",
    "# load dotenv\n",
    "load_dotenv()\n",
    "print(\"Environment Variable loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd19be72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge base has 8 documents.\n",
      "\n",
      "Sample Example Document:\n",
      "Machine learning is a subset of artificial intelligence that enables computers to learn from data without explicit programming. It uses algorithms to identify patterns and make predictions.\n"
     ]
    }
   ],
   "source": [
    "#sample knowledge base\n",
    "knowledge_base = [ \"Machine learning is a subset of artificial intelligence that enables computers to learn from data without explicit programming. It uses algorithms to identify patterns and make predictions.\",\n",
    "    \n",
    "    \"Deep learning is a type of machine learning that uses neural networks with multiple layers. It's particularly effective for image recognition, natural language processing, and complex pattern recognition tasks.\",\n",
    "    \n",
    "    \"Natural Language Processing (NLP) is a field of AI that focuses on the interaction between computers and human language. It enables machines to understand, interpret, and generate human language.\",\n",
    "    \n",
    "    \"Embeddings are numerical representations of text that capture semantic meaning. Similar texts have similar embedding vectors, which enables semantic search and similarity comparison.\",\n",
    "    \n",
    "    \"RAG (Retrieval Augmented Generation) combines information retrieval with text generation. It retrieves relevant context from a knowledge base and uses it to generate more accurate and informed responses.\",\n",
    "    \n",
    "    \"OpenAI's GPT models are large language models trained on diverse internet text. They can perform various tasks like text generation, summarization, translation, and question answering.\",\n",
    "    \n",
    "    \"Vector databases store embeddings and enable fast similarity search. Popular options include Chroma, Pinecone, Weaviate, and FAISS. They're essential for production RAG systems.\",\n",
    "    \n",
    "    \"Fine-tuning is the process of adapting a pre-trained model to a specific task by training it on domain-specific data. It's useful when you need specialized behavior beyond what prompting can achieve.\"\n",
    "]\n",
    "\n",
    "print(f\"Knowledge base has {len(knowledge_base)} documents.\")\n",
    "print(f\"\\nSample Example Document:\\n{knowledge_base[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00d13212",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lapde\\anaconda3\\envs\\MLNotebook\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created embeddings with shape: (8, 384)\n",
      "Each document is respresented as a 384-dimensional vector.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "def create_embeddings(texts:List[str], model: str=\"all-MiniLM-L6-v2\") -> np.ndarray:\n",
    "    \"\"\"args:\n",
    "    texts: List of text data to be embedded\n",
    "    model: Model name from sentence transformers\n",
    "    \n",
    "    Returns:\n",
    "    NumPy array of embeddings (shape:[num_texts, embedding_dimension])\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer(model)\n",
    "    embeddings = model.encode(texts)\n",
    "    return embeddings\n",
    "\n",
    "# Call function to create embeddings\n",
    "kb_embeddings = create_embeddings(knowledge_base)\n",
    "\n",
    "print(f\"Created embeddings with shape: {kb_embeddings.shape}\")\n",
    "print(f\"Each document is respresented as a {kb_embeddings.shape[1]}-dimensional vector.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2de9df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the meaning of deep learning?\n",
      "\n",
      "Result 1 (similarity: 0.7687):\n",
      "Deep learning is a type of machine learning that uses neural networks with multiple layers. It's particularly effective for image recognition, natural language processing, and complex pattern recognition tasks.\n",
      "\n",
      "Result 2 (similarity: 0.5162):\n",
      "Machine learning is a subset of artificial intelligence that enables computers to learn from data without explicit programming. It uses algorithms to identify patterns and make predictions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def retrieve_relevant_docs(\n",
    "    query: str,\n",
    "    knowledge_base: List[str],\n",
    "    kb_embeddings: np.ndarray,\n",
    "    top_k: int=2) -> List[Tuple[str, float]]:\n",
    "\n",
    "    \"\"\"Args:\n",
    "    query: User's Question\n",
    "    knowledge_base: List of documet texts\n",
    "    kb_embeddings: NumPy array of knowledge base embeddings\n",
    "    top_k: Number of top relevant documents to retrieve\n",
    "    \"\"\"\n",
    "\n",
    "    # Create embeddings for query\n",
    "    query_embedding = create_embeddings([query])\n",
    "\n",
    "    # create similarity score between query and knowledge base\n",
    "    similarities = cosine_similarity(query_embedding, kb_embeddings)[0] \n",
    "\n",
    "    # get top_k indices \n",
    "    top_k_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "\n",
    "    # return document with similarity scores\n",
    "    results = [(knowledge_base[i], similarities[i]) for i in top_k_indices]\n",
    "    return results\n",
    "\n",
    "# call function to retrieve relevant documents\n",
    "query = \"What is the meaning of deep learning?\"\n",
    "relevant_docs = retrieve_relevant_docs(query, knowledge_base, kb_embeddings, top_k=2)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "for i, (doc, score) in enumerate(relevant_docs, 1):\n",
    "    print(f\"Result {i} (similarity: {score:.4f}):\")\n",
    "    print(f\"{doc}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e20d1a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is deep learning and what is it used for?\n",
      "\n",
      "Answer:\n",
      "Deep learning is a type of machine learning that utilizes neural networks with multiple layers. It is particularly effective for tasks such as image recognition, natural language processing, and complex pattern recognition.\n",
      "\n",
      "Tokens used: 205\n",
      "\n",
      "Sources used (similarity scores):\n",
      "1. [0.825] Deep learning is a type of machine learning that uses neural networks with multi...\n",
      "2. [0.507] Machine learning is a subset of artificial intelligence that enables computers t...\n",
      "3. [0.379] Natural Language Processing (NLP) is a field of AI that focuses on the interacti...\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "def generate_answer(\n",
    "    query: str,\n",
    "    context_docs: List[Tuple[str, float]],\n",
    "    model: str = \"gpt-4o-mini\") -> Dict[str, any]:\n",
    "\n",
    "    \"\"\"Args:\n",
    "    query: User's question\n",
    "    context_docs: Retrieved documents with similarity scores\n",
    "    model : OpenAI model name\n",
    "    \n",
    "    Returns: \n",
    "    Dictionary with answers and metadata\n",
    "    \"\"\"\n",
    "\n",
    "    # instantiate OpenAI client\n",
    "    client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "    # Prepare context by concatenating retrieved documents\n",
    "    context = \"\\n\\n\".join([doc for doc, _ in context_docs])\n",
    "\n",
    "    # Create prompt with context and query\n",
    "    system_prompt = \"\"\"You are a helpful AI assistant. Answer the user's question based on the provided context. \n",
    "If the context doesn't contain relevant information, say so rather than making up an answer.\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"Context: {context}\n",
    "\n",
    "    Question: {query}\n",
    "\n",
    "    Answer based on the above context: \"\"\"\n",
    "\n",
    "    # call openai chat completion\n",
    "    response = client.chat.completions.create(\n",
    "        model = model,\n",
    "        messages = [\n",
    "            {\"role\": \"system\" , \"content\": system_prompt},\n",
    "            {\"role\": \"user\" , \"content\": user_prompt}\n",
    "        ],\n",
    "\n",
    "        temperature = 0.6,\n",
    "        max_tokens = 300\n",
    "    )\n",
    "\n",
    "    return{\n",
    "        \"answer\": response.choices[0].message.content,\n",
    "        \"tokens_used\": response.usage.total_tokens,\n",
    "        \"sources\": [doc for doc, _ in context_docs],\n",
    "        \"similarity_scores\":[score for _, score in context_docs]\n",
    "    }\n",
    "\n",
    "# call function to generate answer\n",
    "query = \"What is deep learning and what is it used for?\"\n",
    "relevant_docs = retrieve_relevant_docs(query, knowledge_base, kb_embeddings, top_k=3)\n",
    "result = generate_answer(query, relevant_docs)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(f\"Answer:\\n{result['answer']}\\n\")\n",
    "print(f\"Tokens used: {result['tokens_used']}\")\n",
    "print(f\"\\nSources used (similarity scores):\")\n",
    "for i, (source, score) in enumerate(zip(result['sources'], result['similarity_scores']), 1):\n",
    "    print(f\"{i}. [{score:.3f}] {source[:80]}...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913fb5ef",
   "metadata": {},
   "source": [
    "# RAG with class instead of Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffe0d948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SimpleRAG class defined!\n"
     ]
    }
   ],
   "source": [
    "class BasicRAG:\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            embedding_model: str = \"all-MiniLM-L6-v2\",\n",
    "            llm_model: str = \"gpt-4o-mini\",\n",
    "            top_k: int=3\n",
    "    ):\n",
    "        \n",
    "        self.embedding_model = SentenceTransformer(embedding_model)\n",
    "        self.llm_model = llm_model\n",
    "        self.top_k = top_k\n",
    "        self.openai_client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "        # Store knowledge base and embeddings\n",
    "        self.documents = []\n",
    "        self.embeddings = np.ndarray = None\n",
    "\n",
    "    def add_documents(self, documents:List[str]):\n",
    "        \"\"\"Args:\n",
    "        texts: List of document texts to add to knowledge base\n",
    "        \"\"\"\n",
    "        self.documents = documents\n",
    "        print(f\"Embedding {len(documents)} documents!\")\n",
    "        self.embeddings = self.embedding_model.encode(documents)\n",
    "        print(f\"{len(documents)} documents embedded successfully.\")\n",
    "\n",
    "    def retrieve(self, query: str) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Retrieve most relevant documents for a query.\n",
    "        \n",
    "        Args:\n",
    "            query: User's question\n",
    "            \n",
    "        Returns:\n",
    "            List of (document, similarity_score) tuples\n",
    "        \"\"\"\n",
    "        if self.embedding is None:\n",
    "            raise ValueError(\"Knowledge base is empty. Add documents first., Call add_documents().\")\n",
    "        \n",
    "        # embed query\n",
    "        query_embedding = self.embedding_model.encode([query])\n",
    "\n",
    "        # compute similarities\n",
    "        similarities = cosine_similarity(query_embedding, self.embeddings)[0]\n",
    "\n",
    "        # get top_k indices\n",
    "        top_indices = np.argsort(similarities)[::-1][:self.top_k]\n",
    "\n",
    "        return [(self.documents[i], similarities[i]) for i in top_indices]\n",
    "    \n",
    "    def generate(self, query: str, context_docs: List[Tuple[str, float]]) -> Dict:\n",
    "        \"\"\"Generate answer using LLM based on query and context documents.\n",
    "        \n",
    "        Args:\n",
    "            query: User's question\n",
    "            context_docs: Retrieved documents with similarity scores \"\"\"\n",
    "        \n",
    "        # prepare and join context\n",
    "        context = \"\\n\\n\".join([doc for doc, _ in context_docs])\n",
    "\n",
    "        system_prompt = \"\"\"You are a helpful AI assistant. Answer questions based on the provided context.\n",
    "If the context doesn't contain enough information, acknowledge this limitation.\"\"\"\n",
    "\n",
    "        user_prompt = f\"\"\"Context: {context}\n",
    "Question: {query}\n",
    "\n",
    "Answer based on the above context: \"\"\"\n",
    "        \n",
    "        #call openai chat  to generate answer\n",
    "        response = self.openai_client.chat.completions.create(\n",
    "            model = self.llm_model,\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature = 0.7,\n",
    "            max_tokens = 300\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"answer\": response.choices[0].message.content,\n",
    "            \"tokens_used\": response.usage.total_tokens,\n",
    "            \"sources\": context_docs\n",
    "        }\n",
    "    \n",
    "    def query(self, question: str, return_sources: bool=True) -> Dict:\n",
    "        \"\"\"End-to-end query processing: retrieve relevant documents and generate answer.\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            return_sources: Whether to include source documents in the output \"\"\"\n",
    "        \n",
    "        # retrieve relevant documents\n",
    "        relevant_docs = self.retrieve(question)\n",
    "\n",
    "        # generate answer\n",
    "        result = self.generate(question, relevant_docs)\n",
    "\n",
    "        #format output/response\n",
    "        response = {\n",
    "            \"question\": question,\n",
    "            \"answer\": result[\"answer\"],\n",
    "            \"tokens_used\": result[\"tokens_used\"]\n",
    "\n",
    "        }\n",
    "\n",
    "        if return_sources:\n",
    "            response[\"sources\"] = [\n",
    "                {\"text\": doc, \"similarity\": float(score)}\n",
    "                for doc, score in result[\"sources\"]\n",
    "            ]\n",
    "        \n",
    "        return response\n",
    "\n",
    "print(\"✅ SimpleRAG class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cef2bc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SimpleRAG class defined!!\n"
     ]
    }
   ],
   "source": [
    "class SimpleRAG:\n",
    "    \"\"\"\n",
    "    A production-quality RAG system.\n",
    "    \n",
    "    This class encapsulates the entire RAG pipeline:\n",
    "    - Document storage and embedding\n",
    "    - Semantic search/retrieval\n",
    "    - Answer generation\n",
    "    \n",
    "    Why use a class?\n",
    "    - Maintains state (knowledge base, embeddings)\n",
    "    - Provides a clean API (add_documents, query)\n",
    "    - Easy to configure and reuse\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_model: str = \"all-MiniLM-L6-v2\",\n",
    "        llm_model: str = \"gpt-4o-mini\",\n",
    "        top_k: int = 3\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the RAG system.\n",
    "        \n",
    "        Args:\n",
    "            embedding_model: Sentence transformer model name\n",
    "            llm_model: OpenAI model for generation\n",
    "            top_k: Number of documents to retrieve\n",
    "        \"\"\"\n",
    "        self.embedding_model = SentenceTransformer(embedding_model)\n",
    "        self.llm_model = llm_model\n",
    "        self.top_k = top_k\n",
    "        self.openai_client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "        \n",
    "        # State: stores documents and their embeddings\n",
    "        self.documents: List[str] = []\n",
    "        self.embeddings: np.ndarray = None\n",
    "    \n",
    "    def add_documents(self, documents: List[str]) -> None:\n",
    "        \"\"\"\n",
    "        Add documents to the knowledge base.\n",
    "        \n",
    "        Args:\n",
    "            documents: List of text documents to add\n",
    "            \n",
    "        In production:\n",
    "            - This might load from database/files\n",
    "            - Could handle incremental updates\n",
    "            - Might include document metadata\n",
    "        \"\"\"\n",
    "        self.documents = documents\n",
    "        print(f\"Embedding {len(documents)} documents...\")\n",
    "        self.embeddings = self.embedding_model.encode(documents)\n",
    "        print(f\"✅ {len(documents)} documents indexed\")\n",
    "    \n",
    "    def retrieve(self, query: str) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Retrieve most relevant documents for a query.\n",
    "        \n",
    "        Args:\n",
    "            query: User's question\n",
    "            \n",
    "        Returns:\n",
    "            List of (document, similarity_score) tuples\n",
    "        \"\"\"\n",
    "        if self.embeddings is None:\n",
    "            raise ValueError(\"No documents added. Call add_documents() first.\")\n",
    "        \n",
    "        # Embed query\n",
    "        query_embedding = self.embedding_model.encode([query])\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = cosine_similarity(query_embedding, self.embeddings)[0]\n",
    "        \n",
    "        # Get top-k\n",
    "        top_indices = np.argsort(similarities)[::-1][:self.top_k]\n",
    "        \n",
    "        return [(self.documents[i], similarities[i]) for i in top_indices]\n",
    "    \n",
    "    def generate(self, query: str, context_docs: List[Tuple[str, float]]) -> Dict:\n",
    "        \"\"\"\n",
    "        Generate answer using retrieved context.\n",
    "        \n",
    "        Args:\n",
    "            query: User's question\n",
    "            context_docs: Retrieved documents with scores\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with answer and metadata\n",
    "        \"\"\"\n",
    "        # Build context\n",
    "        context = \"\\n\\n\".join([doc for doc, _ in context_docs])\n",
    "        \n",
    "        # Create prompt\n",
    "        system_prompt = \"\"\"You are a helpful AI assistant. Answer questions based on the provided context.\n",
    "If the context doesn't contain enough information, acknowledge this limitation.\"\"\"\n",
    "        \n",
    "        user_prompt = f\"\"\"Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        # Generate\n",
    "        response = self.openai_client.chat.completions.create(\n",
    "            model=self.llm_model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=300\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"answer\": response.choices[0].message.content,\n",
    "            \"tokens\": response.usage.total_tokens,\n",
    "            \"sources\": context_docs\n",
    "        }\n",
    "    \n",
    "    def query(self, question: str, return_sources: bool = True) -> Dict:\n",
    "        \"\"\"\n",
    "        Main method: Query the RAG system.\n",
    "        \n",
    "        This is the public API - simple to use!\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            return_sources: Whether to include source documents\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with answer and optional metadata\n",
    "        \"\"\"\n",
    "        # Step 1: Retrieve\n",
    "        relevant_docs = self.retrieve(question)\n",
    "        \n",
    "        # Step 2: Generate\n",
    "        result = self.generate(question, relevant_docs)\n",
    "        \n",
    "        # Format response\n",
    "        response = {\n",
    "            \"question\": question,\n",
    "            \"answer\": result[\"answer\"],\n",
    "            \"tokens_used\": result[\"tokens\"]\n",
    "        }\n",
    "        \n",
    "        if return_sources:\n",
    "            response[\"sources\"] = [\n",
    "                {\"text\": doc, \"similarity\": float(score)}\n",
    "                for doc, score in result[\"sources\"]\n",
    "            ]\n",
    "        \n",
    "        return response\n",
    "\n",
    "print(\"✅ SimpleRAG class defined!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bda5e60",
   "metadata": {},
   "source": [
    "## Using the defined Rag Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b49df95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 8 documents...\n",
      "✅ 8 documents indexed\n",
      "Question: What is RAG and why is it useful?.\n",
      "\n",
      "Answer:\n",
      "RAG (Retrieval Augmented Generation) is a methodology that combines information retrieval with text generation. It works by retrieving relevant context from a knowledge base and using that information to generate more accurate and informed responses. This is useful because it allows the generation of responses that are not only coherent but also grounded in factual information, enhancing the reliability and relevance of the generated content. By leveraging external knowledge, RAG systems can provide more comprehensive and contextually appropriate answers, making them particularly valuable in applications like customer support, content creation, and question-answering systems.\n",
      "\n",
      "Tokens used: 272\n",
      "\n",
      "Sources:\n",
      "1. [Similarity: 0.621]\n",
      "   RAG (Retrieval Augmented Generation) combines information retrieval with text generation. It retriev...\n",
      "\n",
      "2. [Similarity: 0.448]\n",
      "   Vector databases store embeddings and enable fast similarity search. Popular options include Chroma,...\n",
      "\n",
      "3. [Similarity: 0.156]\n",
      "   Fine-tuning is the process of adapting a pre-trained model to a specific task by training it on doma...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize RAG system\n",
    "rag = SimpleRAG(\n",
    "    embedding_model = \"all-MiniLM-L6-v2\",\n",
    "    llm_model = \"gpt-4o-mini\",\n",
    "    top_k = 3\n",
    ")\n",
    "\n",
    "# add documents to RAG knowledge base\n",
    "rag.add_documents(knowledge_base)\n",
    "\n",
    "# query RAG system\n",
    "result = rag.query(\"What is RAG and why is it useful?.\")\n",
    "\n",
    "print(f\"Question: {result['question']}\\n\")\n",
    "print(f\"Answer:\\n{result['answer']}\\n\")\n",
    "print(f\"Tokens used: {result['tokens_used']}\\n\")\n",
    "print(\"Sources:\")\n",
    "for i, source in enumerate(result['sources'], 1):\n",
    "    print(f\"{i}. [Similarity: {source['similarity']:.3f}]\")\n",
    "    print(f\"   {source['text'][:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89344e82",
   "metadata": {},
   "source": [
    "# RAG with different LLM providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14058d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MultiProviderRAG class defined\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "from anthropic import Anthropic\n",
    "class MultiProviderRAG(SimpleRAG):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        provider: str=\"openai\",\n",
    "        embedding_model: str = \"all-MiniLM-L6-v2\",\n",
    "        llm_model: str = None,\n",
    "        top_k: int = 3):\n",
    "\n",
    "        # set default model per provider\n",
    "        default_models = {\n",
    "            \"openai\": \"gpt-4o-mini\",\n",
    "            \"google\": \"gemini-2.5-flash\",\n",
    "            \"anthropic\": \"claude-3-5-haiku-20241022\"\n",
    "        }\n",
    "\n",
    "        self.provider = provider\n",
    "        llm_model = llm_model or default_models[provider]\n",
    "\n",
    "        # Initialize the parent class with the specified parameters\n",
    "        super().__init__(\n",
    "            embedding_model,\n",
    "            llm_model,\n",
    "            top_k \n",
    "        )\n",
    "\n",
    "        if provider == \"gemini\":\n",
    "            self.gemini_client = genai.Client(api_key=os.environ[\"GOOGLE_GENAI_API_KEY\"])\n",
    "        if provider == \"anthropic\":\n",
    "            self.anthropic_client = Anthropic(api_key=os.environ[\"ANTHROPIC_API_KEY\"])\n",
    "\n",
    "        def generate(self, query: str, context_docs: List[Tuple[str, float]]) -> Dict:\n",
    "            \"\"\"\n",
    "        Generate answer using the configured provider.\n",
    "        \n",
    "        This overrides the parent method to support multiple providers.\n",
    "        \"\"\"\n",
    "            # prepare the context\n",
    "            context = \"\\n\\n\".join([doc for doc, _ in context_docs])\n",
    "\n",
    "            if self.provider == \"openai\":\n",
    "                return self.generate_openai(query, context)\n",
    "            if self.provider == \"gemini\":\n",
    "                return self.generate_gemini(query, context)\n",
    "            if self.provider == \"anthropic\":\n",
    "                return self.generate_claude(query, context)\n",
    "            \n",
    "        def generate_openai(self, query: str, context: str) -> Dict:\n",
    "            response = self.openai_client.chat.completions.create(\n",
    "                model = self.llm_model,\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"context\": \"Answer based on context.\"},\n",
    "                    {\"role\": \"user\", \"context\": f\"Context:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"}\n",
    "                ],\n",
    "                temperature = 0.7,\n",
    "                max_tokens = 300\n",
    "            )\n",
    "            return {\n",
    "                \"answer\": response.choices[0].message.content,\n",
    "                \"tokens\": response.usage.total_tokens,\n",
    "                \"sources\": context\n",
    "            }\n",
    "            \n",
    "        def generate_gemini(self, query: str, context: str) -> Dict:\n",
    "            prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer based on the context:\"\n",
    "\n",
    "            response = self.gemini_client.models.generate_content(\n",
    "                model = self.llm_model,\n",
    "                contents = prompt,\n",
    "                config = types.GenerateContentConfig(\n",
    "                    temperature = 0.7,\n",
    "                    max_output_tokens = 300\n",
    "                )\n",
    "            )\n",
    "            return {\n",
    "                \"answer\": response.text,\n",
    "                \"tokens\": response.usage.metadata.total_token_count,\n",
    "                \"sources\": context\n",
    "            }\n",
    "        \n",
    "        def _generate_claude(self, query: str, context: str) -> Dict:\n",
    "            \"\"\"Generate using Claude.\"\"\"\n",
    "            prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer based on the context above:\"\n",
    "        \n",
    "            response = self.claude_client.messages.create(\n",
    "                model=self.llm_model,\n",
    "                max_tokens=300,\n",
    "                temperature=0.7,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            return {\n",
    "\n",
    "                \"answer\": response.content[0].text,\n",
    "                \"tokens\": response.usage.input_tokens + response.usage.output_tokens,\n",
    "                \"sources\": context\n",
    "            }\n",
    "print(\"✅ MultiProviderRAG class defined\")         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "257e5cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Testing with OPENAI\n",
      "================================================================================\n",
      "Embedding 8 documents...\n",
      "✅ 8 documents indexed\n",
      "\n",
      "Answer:\n",
      "Deep learning is a type of machine learning that utilizes neural networks with multiple layers to process data. It is particularly effective in tasks such as image recognition, natural language processing, and complex pattern recognition. By using these multi-layered networks, deep learning can automatically learn and extract features from large amounts of data, leading to improved accuracy and performance in various applications.\n",
      "\n",
      "Tokens used: 223\n",
      "\n",
      "================================================================================\n",
      "Testing with GOOGLE\n",
      "================================================================================\n",
      "Embedding 8 documents...\n",
      "✅ 8 documents indexed\n",
      "Error with google: Error code: 404 - {'error': {'message': 'The model `gemini-2.5-flash` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
      "Make sure you have the API key set in your .env file\n",
      "\n",
      "================================================================================\n",
      "Testing with ANTHROPIC\n",
      "================================================================================\n",
      "Error with anthropic: 'ANTHROPIC_API_KEY'\n",
      "Make sure you have the API key set in your .env file\n"
     ]
    }
   ],
   "source": [
    "# compare and test different providers\n",
    "test_questions = \"What is deep learning and how is it used?\"\n",
    "\n",
    "providers = [\"openai\", \"google\", \"anthropic\"]\n",
    "\n",
    "for provider in providers:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Testing with {provider.upper()}\")\n",
    "    print('='*80)\n",
    "\n",
    "    try:\n",
    "        # Create RAG instance for the provider\n",
    "        rag = MultiProviderRAG(\n",
    "            provider=provider,\n",
    "            top_k=3\n",
    "        )\n",
    "\n",
    "        rag.add_documents(knowledge_base)\n",
    "        result = rag.query(test_questions)\n",
    "\n",
    "        print(f\"\\nAnswer:\\n{result['answer']}\")\n",
    "        print(f\"\\nTokens used: {result['tokens_used']}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error with {provider}: {e}\")\n",
    "        print(\"Make sure you have the API key set in your .env file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c5286b",
   "metadata": {},
   "source": [
    "# Full scale basic RAG project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4afbad12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Variable loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from typing import List, Tuple, Dict\n",
    "import numpy as np\n",
    "\n",
    "# load dotenv\n",
    "load_dotenv()\n",
    "print(\"Environment Variable loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48196e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8 pages from llm_fundamentals.pdf\n",
      "\n",
      "Sample from page 1 (first 200 chars):\n",
      "@genieincodebottle \n",
      "Instagram | GitHub | Medium | YouTube \n",
      "How to Be Better Than Most in GenAI \n",
      " \n",
      "Contents \n",
      " \n",
      "Core LLM Building Blocks .................................................................\n",
      "\n",
      "Metadata: {'source': 'llm_fundamentals.pdf', 'page': 1, 'total_pages': 8}\n"
     ]
    }
   ],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "def load_pdf(pdf_path: str) -> List[Dict[str, any]]:\n",
    "    reader = PdfReader(pdf_path)\n",
    "    documents = [] # list to hold page texts and metadata\n",
    "\n",
    "    for page_num, page in enumerate(reader.pages, start=1):\n",
    "        text = page.extract_text()\n",
    "\n",
    "        # store text and metadata\n",
    "        documents.append({\n",
    "            \"text\": text,\n",
    "            \"metadata\": {\n",
    "                \"source\": pdf_path,\n",
    "                \"page\": page_num,\n",
    "                \"total_pages\": len(reader.pages)\n",
    "            },\n",
    "\n",
    "        })\n",
    "    return documents\n",
    "\n",
    "# load sample PDF\n",
    "pdf_path = \"llm_fundamentals.pdf\"\n",
    "pages = load_pdf(pdf_path)\n",
    "\n",
    "print(f\"Loaded {len(pages)} pages from {pdf_path}\")\n",
    "print(f\"\\nSample from page 1 (first 200 chars):\")\n",
    "print(pages[0]['text'][:200])\n",
    "print(f\"\\nMetadata: {pages[0]['metadata']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "867e05c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created 39 chunks from 8 pages\n",
      "\n",
      "Sample chunk:\n",
      "sequence order information to token embeddings \n",
      "5. Attention → Highlights the most relevant tokens in context \n",
      "6. Self-Attention → Each token attends to every other token for context \n",
      "7. Cross-Attention → Connect encoder and decoder (in encoder-decoder models) \n",
      "8. Multi-Head Attention → Several attention heads capture different patterns in parallel \n",
      "9. Feed-Forward Networks → Nonlinear layers that transform representations between \n",
      "attention blocks \n",
      "10.\n",
      "\n",
      "Its metadata: {'source': 'llm_fundamentals.pdf', 'page': 2, 'total_pages': 8, 'chunk_index': 1}\n"
     ]
    }
   ],
   "source": [
    "def chunk_text(\n",
    "    text: str,\n",
    "    chunk_size: int=500,\n",
    "    chunk_overlap: int=50\n",
    ") -> List[str]:\n",
    "    \n",
    "    chunks = [] # list to hold text chunks\n",
    "    start = 0 # starting index for chunking\n",
    "\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size # ending index for chunk\n",
    "        chunk = text[start:end]\n",
    "\n",
    "        # try to break at sentence boundary\n",
    "        if end < len(text):\n",
    "            last_period = max(\n",
    "                chunk.rfind('.'),\n",
    "                chunk.rfind('!'),\n",
    "                chunk.rfind('?')\n",
    "            )\n",
    "        \n",
    "            if last_period > chunk_size * 0.5: # if found a sentence boundary\n",
    "                chunk = text[start:start + last_period + 1]\n",
    "                end = start + last_period + 1\n",
    "        \n",
    "        chunks.append(chunk.strip())\n",
    "        \n",
    "        # Move start position (with overlap)\n",
    "        start = end - chunk_overlap\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Process all pages into chunks\n",
    "all_chunks = []\n",
    "chunk_metadata = []\n",
    "\n",
    "for page_doc in pages:\n",
    "    page_text = page_doc['text']\n",
    "    page_meta = page_doc['metadata']\n",
    "    \n",
    "    # Chunk this page\n",
    "    page_chunks = chunk_text(page_text, chunk_size=500, chunk_overlap=50)\n",
    "    \n",
    "    for chunk_idx, chunk in enumerate(page_chunks):\n",
    "        all_chunks.append(chunk)\n",
    "        # Keep track of where this chunk came from\n",
    "        chunk_metadata.append({\n",
    "            **page_meta,\n",
    "            \"chunk_index\": chunk_idx\n",
    "        })\n",
    "\n",
    "print(f\"✅ Created {len(all_chunks)} chunks from {len(pages)} pages\")\n",
    "print(f\"\\nSample chunk:\")\n",
    "print(all_chunks[5])\n",
    "print(f\"\\nIts metadata: {chunk_metadata[5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52aced31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Chroma collection created.\n",
      "Collection name: llm_fundamentals\n"
     ]
    }
   ],
   "source": [
    "import chromadb \n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# Initialize Chroma client\n",
    "client = chromadb.Client()\n",
    "\n",
    "# Create embedding function\n",
    "embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name = \"all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# Create Chroma collection, this will store our document chunks\n",
    "collection = client.get_or_create_collection(\n",
    "    name = \"llm_fundamentals\", # collection name\n",
    "    embedding_function = embedding_function,\n",
    "    metadata = {\"description\": \"Chunks from LLM Fundamentals PDF\"}\n",
    ")\n",
    "\n",
    "print(\"✅ Chroma collection created.\")\n",
    "print(f\"Collection name: {collection.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f883e481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 39 chunks to Chroma collection.\n",
      "Total documents in collection: 39\n"
     ]
    }
   ],
   "source": [
    "# Add chunks to Chroma collection\n",
    "# This will automatically create embeddings using the defined embedding function\n",
    "\n",
    "unique_ids = ids = [f\"chunk_{i}\" for i in range(len(all_chunks))] # Generate unique IDs for each chunk\n",
    "\n",
    "collection.add(\n",
    "    documents = all_chunks,\n",
    "    metadatas = chunk_metadata,\n",
    "    ids = ids\n",
    ")\n",
    "\n",
    "print(f\"Added {len(all_chunks)} chunks to Chroma collection.\")\n",
    "print(f\"Total documents in collection: {collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "349d8834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ PDFQuestionAnswering class defined\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "class PDFQuestionAnswering:\n",
    "    \"\"\"\n",
    "    Production-ready RAG system for PDF Question Answering.\n",
    "    \n",
    "    Why a class?\n",
    "        - Manages ChromaDB connection (state)\n",
    "        - Handles LLM client (state)\n",
    "        - Provides clean API for querying\n",
    "        - Easy to extend and test\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        collection_name: str = \"llm_fundamentals\",\n",
    "        llm_model: str = \"gpt-4o-mini\",\n",
    "        top_k: int = 3\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the QA system.\n",
    "        \n",
    "        Args:\n",
    "            collection_name: ChromaDB collection to use\n",
    "            llm_model: OpenAI model for generation\n",
    "            top_k: Number of chunks to retrieve\n",
    "        \"\"\"\n",
    "        # Setup ChromaDB\n",
    "        self.client = chromadb.Client()\n",
    "        self.embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "            model_name=\"all-MiniLM-L6-v2\"\n",
    "        )\n",
    "        self.collection = self.client.get_or_create_collection(\n",
    "            name=collection_name,\n",
    "            embedding_function=self.embedding_function\n",
    "        )\n",
    "        \n",
    "        # Setup LLM\n",
    "        self.llm_model = llm_model\n",
    "        self.openai_client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "        \n",
    "        # Config\n",
    "        self.top_k = top_k\n",
    "        \n",
    "        print(f\"✅ QA System initialized\")\n",
    "        print(f\"   Collection: {collection_name} ({self.collection.count()} chunks)\")\n",
    "        print(f\"   LLM: {llm_model}\")\n",
    "    \n",
    "    def retrieve(self, question: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Step 6: Retrieve relevant chunks from ChromaDB.\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            \n",
    "        Returns:\n",
    "            List of retrieved chunks with metadata\n",
    "        \"\"\"\n",
    "        results = self.collection.query(\n",
    "            query_texts=[question],\n",
    "            n_results=self.top_k\n",
    "        )\n",
    "        \n",
    "        # Format results\n",
    "        retrieved = []\n",
    "        for i in range(len(results['documents'][0])):\n",
    "            retrieved.append({\n",
    "                \"text\": results['documents'][0][i],\n",
    "                \"metadata\": results['metadatas'][0][i],\n",
    "                \"distance\": results['distances'][0][i]\n",
    "            })\n",
    "        \n",
    "        return retrieved\n",
    "    \n",
    "    def generate_answer(self, question: str, context_chunks: List[Dict]) -> str:\n",
    "        \"\"\"\n",
    "        Step 7: Generate answer using LLM with retrieved context.\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            context_chunks: Retrieved chunks from ChromaDB\n",
    "            \n",
    "        Returns:\n",
    "            Generated answer\n",
    "        \"\"\"\n",
    "        # Build context from chunks\n",
    "        context = \"\\n\\n\".join([\n",
    "            f\"[Page {chunk['metadata']['page']}]\\n{chunk['text']}\"\n",
    "            for chunk in context_chunks\n",
    "        ])\n",
    "        \n",
    "        # Create prompt\n",
    "        system_prompt = \"\"\"You are an AI assistant helping users understand LLM fundamentals.\n",
    "Answer questions based ONLY on the provided context from the PDF.\n",
    "If the context doesn't contain the answer, say \"I don't have enough information in the provided context.\"\n",
    "Always cite the page number when giving information.\"\"\"\n",
    "        \n",
    "        user_prompt = f\"\"\"Context from LLM Fundamentals PDF:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        # Generate\n",
    "        response = self.openai_client.chat.completions.create(\n",
    "            model=self.llm_model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.3,  # Lower = more factual\n",
    "            max_tokens=400\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    def ask(self, question: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Step 5-8: Complete pipeline - ask a question and get an answer.\n",
    "        \n",
    "        This is the main method users call!\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with answer, sources, and metadata\n",
    "        \"\"\"\n",
    "        # Step 6: Retrieve\n",
    "        retrieved_chunks = self.retrieve(question)\n",
    "        \n",
    "        # Step 7: Generate\n",
    "        answer = self.generate_answer(question, retrieved_chunks)\n",
    "        \n",
    "        # Step 8: Return with sources\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"sources\": [\n",
    "                {\n",
    "                    \"page\": chunk['metadata']['page'],\n",
    "                    \"text\": chunk['text'][:150] + \"...\",  # Preview\n",
    "                    \"relevance\": 1 - chunk['distance']  # Convert distance to similarity\n",
    "                }\n",
    "                for chunk in retrieved_chunks\n",
    "            ]\n",
    "        }\n",
    "\n",
    "print(\"✅ PDFQuestionAnswering class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "76525c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ QA System initialized\n",
      "   Collection: llm_fundamentals (39 chunks)\n",
      "   LLM: gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "# Initialize the PDF QA system\n",
    "pdf_qa = PDFQuestionAnswering(\n",
    "    collection_name = \"llm_fundamentals\",\n",
    "    llm_model = \"gpt-4o-mini\",\n",
    "    top_k = 3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "17bf9ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is RAG and how does it work?\n",
      "\n",
      "Answer:\n",
      "RAG stands for Retrieval-Augmented Generation. It combines LLMs with external knowledge sources to provide up-to-date answers. This approach allows the model to access and utilize current information from these external sources to enhance its responses (Page 4).\n",
      "\n",
      "================================================================================\n",
      "Sources:\n",
      "\n",
      "1. Page 4 (Relevance: 0.198)\n",
      "   n for speed + \n",
      "accuracy \n",
      "Knowledge & Retrieval \n",
      "1. RAG → Combine LLMs with external knowledge sources for up-to-date answers \n",
      "2. Vector Databases → St...\n",
      "\n",
      "2. Page 2 (Relevance: 0.197)\n",
      "   Turns logits into a probability distribution \n",
      "15. Sampling from Probabilities → Chooses the next token based on probability weights \n",
      "16. RoPE → Rotary...\n",
      "\n",
      "3. Page 6 (Relevance: 0.123)\n",
      "   ow well a model predicts text (core LM metric) \n",
      "2. BLEU / ROUGE / BERTScore → Compare generated text to reference quality \n",
      "3. Benchmark Suites → Stand...\n"
     ]
    }
   ],
   "source": [
    "# testing the PDF QA system\n",
    "result = pdf_qa.ask(\"What is RAG and how does it work?\")\n",
    "print(f\"Question: {result['question']}\\n\")\n",
    "print(f\"Answer:\\n{result['answer']}\\n\")\n",
    "print(\"=\"*80)\n",
    "print(\"Sources:\")\n",
    "for i, source in enumerate(result['sources'], 1):\n",
    "    print(f\"\\n{i}. Page {source['page']} (Relevance: {source['relevance']:.3f})\")\n",
    "    print(f\"   {source['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0593fba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is LoRA and why is it useful?\n",
      "\n",
      "Answer:\n",
      "LoRA stands for Low-Rank Adaptation, and it is a method used for fine-tuning large language models. It is useful because it allows for the adaptation of models by updating only small parts of the model, which makes the fine-tuning process more efficient in terms of memory and computational resources. This is particularly beneficial for fine-tuning huge models on modest hardware, especially when combined with quantization techniques like QLoRA (Page 3).\n",
      "\n",
      "================================================================================\n",
      "Sources:\n",
      "\n",
      "1. Page 3\n",
      "\n",
      "2. Page 7\n",
      "\n",
      "3. Page 1\n"
     ]
    }
   ],
   "source": [
    "result = pdf_qa.ask(\"What is LoRA and why is it useful?\")\n",
    "\n",
    "\n",
    "print(f\"Question: {result['question']}\\n\")\n",
    "print(f\"Answer:\\n{result['answer']}\\n\")\n",
    "print(\"=\"*80)\n",
    "print(\"Sources:\")\n",
    "for i, source in enumerate(result['sources'], 1):\n",
    "    print(f\"\\n{i}. Page {source['page']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0df7e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLNotebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
