{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5baf01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Variable loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from typing import List, Tuple, Dict\n",
    "import numpy as np\n",
    "\n",
    "# load dotenv\n",
    "load_dotenv()\n",
    "print(\"Environment Variable loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd19be72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge base has 8 documents.\n",
      "\n",
      "Sample Example Document:\n",
      "Machine learning is a subset of artificial intelligence that enables computers to learn from data without explicit programming. It uses algorithms to identify patterns and make predictions.\n"
     ]
    }
   ],
   "source": [
    "#sample knowledge base\n",
    "knowledge_base = [ \"Machine learning is a subset of artificial intelligence that enables computers to learn from data without explicit programming. It uses algorithms to identify patterns and make predictions.\",\n",
    "    \n",
    "    \"Deep learning is a type of machine learning that uses neural networks with multiple layers. It's particularly effective for image recognition, natural language processing, and complex pattern recognition tasks.\",\n",
    "    \n",
    "    \"Natural Language Processing (NLP) is a field of AI that focuses on the interaction between computers and human language. It enables machines to understand, interpret, and generate human language.\",\n",
    "    \n",
    "    \"Embeddings are numerical representations of text that capture semantic meaning. Similar texts have similar embedding vectors, which enables semantic search and similarity comparison.\",\n",
    "    \n",
    "    \"RAG (Retrieval Augmented Generation) combines information retrieval with text generation. It retrieves relevant context from a knowledge base and uses it to generate more accurate and informed responses.\",\n",
    "    \n",
    "    \"OpenAI's GPT models are large language models trained on diverse internet text. They can perform various tasks like text generation, summarization, translation, and question answering.\",\n",
    "    \n",
    "    \"Vector databases store embeddings and enable fast similarity search. Popular options include Chroma, Pinecone, Weaviate, and FAISS. They're essential for production RAG systems.\",\n",
    "    \n",
    "    \"Fine-tuning is the process of adapting a pre-trained model to a specific task by training it on domain-specific data. It's useful when you need specialized behavior beyond what prompting can achieve.\"\n",
    "]\n",
    "\n",
    "print(f\"Knowledge base has {len(knowledge_base)} documents.\")\n",
    "print(f\"\\nSample Example Document:\\n{knowledge_base[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00d13212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created embeddings with shape: (8, 384)\n",
      "Each document is respresented as a 384-dimensional vector.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "def create_embeddings(texts:List[str], model: str=\"all-MiniLM-L6-v2\") -> np.ndarray:\n",
    "    \"\"\"args:\n",
    "    texts: List of text data to be embedded\n",
    "    model: Model name from sentence transformers\n",
    "    \n",
    "    Returns:\n",
    "    NumPy array of embeddings (shape:[num_texts, embedding_dimension])\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer(model)\n",
    "    embeddings = model.encode(texts)\n",
    "    return embeddings\n",
    "\n",
    "# Call function to create embeddings\n",
    "kb_embeddings = create_embeddings(knowledge_base)\n",
    "\n",
    "print(f\"Created embeddings with shape: {kb_embeddings.shape}\")\n",
    "print(f\"Each document is respresented as a {kb_embeddings.shape[1]}-dimensional vector.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2de9df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the meaning of deep learning?\n",
      "\n",
      "Result 1 (similarity: 0.7687):\n",
      "Deep learning is a type of machine learning that uses neural networks with multiple layers. It's particularly effective for image recognition, natural language processing, and complex pattern recognition tasks.\n",
      "\n",
      "Result 2 (similarity: 0.5162):\n",
      "Machine learning is a subset of artificial intelligence that enables computers to learn from data without explicit programming. It uses algorithms to identify patterns and make predictions.\n",
      "\n",
      "Result 3 (similarity: 0.3774):\n",
      "Natural Language Processing (NLP) is a field of AI that focuses on the interaction between computers and human language. It enables machines to understand, interpret, and generate human language.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def retrieve_relevant_docs(\n",
    "    query: str,\n",
    "    knowledge_base: List[str],\n",
    "    kb_embeddings: np.ndarray,\n",
    "    top_k: int=3) -> List[Tuple[str, float]]:\n",
    "\n",
    "    \"\"\"Args:\n",
    "    query: User's Question\n",
    "    knowledge_base: List of documet texts\n",
    "    kb_embeddings: NumPy array of knowledge base embeddings\n",
    "    top_k: Number of top relevant documents to retrieve\n",
    "    \"\"\"\n",
    "\n",
    "    # Create embeddings for query\n",
    "    query_embedding = create_embeddings([query])\n",
    "\n",
    "    # create similarity score between query and knowledge base\n",
    "    similarities = cosine_similarity(query_embedding, kb_embeddings)[0] \n",
    "\n",
    "    # get top_k indices \n",
    "    top_k_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "\n",
    "    # return document with similarity scores\n",
    "    results = [(knowledge_base[i], similarities[i]) for i in top_k_indices]\n",
    "    return results\n",
    "\n",
    "# call function to retrieve relevant documents\n",
    "query = \"What is the meaning of deep learning?\"\n",
    "relevant_docs = retrieve_relevant_docs(query, knowledge_base, kb_embeddings, top_k=3)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "for i, (doc, score) in enumerate(relevant_docs, 1):\n",
    "    print(f\"Result {i} (similarity: {score:.4f}):\")\n",
    "    print(f\"{doc}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20d1a14",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: \"Deep learning is a type of machine learning that uses neural networks with multiple layers. It's particularly effective for image recognition, natural language processing, and complex pattern recognition tasks.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 55\u001b[0m\n\u001b[0;32m     53\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is deep learning and what is it used for?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     54\u001b[0m relevant_docs \u001b[38;5;241m=\u001b[39m retrieve_relevant_docs(query, knowledge_base, kb_embeddings, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m---> 55\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelevant_docs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuery: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[21], line 49\u001b[0m, in \u001b[0;36mgenerate_answer\u001b[1;34m(query, context_docs, model)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# call openai chat completion\u001b[39;00m\n\u001b[0;32m     34\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m     35\u001b[0m     model \u001b[38;5;241m=\u001b[39m model,\n\u001b[0;32m     36\u001b[0m     messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m     max_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m\n\u001b[0;32m     43\u001b[0m )\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m{\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m: response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent,\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens_used\u001b[39m\u001b[38;5;124m\"\u001b[39m: response\u001b[38;5;241m.\u001b[39musage\u001b[38;5;241m.\u001b[39mtotal_tokens,\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msources\u001b[39m\u001b[38;5;124m\"\u001b[39m: [doc \u001b[38;5;28;01mfor\u001b[39;00m doc, _ \u001b[38;5;129;01min\u001b[39;00m context_docs],\n\u001b[1;32m---> 49\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m:[\u001b[38;5;28mfloat\u001b[39m(score) \u001b[38;5;28;01mfor\u001b[39;00m score, _ \u001b[38;5;129;01min\u001b[39;00m context_docs]\n\u001b[0;32m     50\u001b[0m }\n",
      "Cell \u001b[1;32mIn[21], line 49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# call openai chat completion\u001b[39;00m\n\u001b[0;32m     34\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m     35\u001b[0m     model \u001b[38;5;241m=\u001b[39m model,\n\u001b[0;32m     36\u001b[0m     messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m     max_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m\n\u001b[0;32m     43\u001b[0m )\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m{\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m: response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent,\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens_used\u001b[39m\u001b[38;5;124m\"\u001b[39m: response\u001b[38;5;241m.\u001b[39musage\u001b[38;5;241m.\u001b[39mtotal_tokens,\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msources\u001b[39m\u001b[38;5;124m\"\u001b[39m: [doc \u001b[38;5;28;01mfor\u001b[39;00m doc, _ \u001b[38;5;129;01min\u001b[39;00m context_docs],\n\u001b[1;32m---> 49\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m:[\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mscore\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m score, _ \u001b[38;5;129;01min\u001b[39;00m context_docs]\n\u001b[0;32m     50\u001b[0m }\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: \"Deep learning is a type of machine learning that uses neural networks with multiple layers. It's particularly effective for image recognition, natural language processing, and complex pattern recognition tasks.\""
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "def generate_answer(\n",
    "    query: str,\n",
    "    context_docs: List[Tuple[str, float]],\n",
    "    model: str = \"gpt-4o-mini\") -> Dict[str, any]:\n",
    "\n",
    "    \"\"\"Args:\n",
    "    query: User's question\n",
    "    context_docs: Retrieved documents with similarity scores\n",
    "    model : OpenAI model name\n",
    "    \n",
    "    Returns: \n",
    "    Dictionary with answers and metadata\n",
    "    \"\"\"\n",
    "\n",
    "    # instantiate OpenAI client\n",
    "    client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "    # Prepare context by concatenating retrieved documents\n",
    "    context = \"\\n\\n\".join([doc for doc, _ in context_docs])\n",
    "\n",
    "    # Create prompt with context and query\n",
    "    system_prompt = \"\"\"You are a helpful AI assistant. Answer the user's question based on the provided context. \n",
    "If the context doesn't contain relevant information, say so rather than making up an answer.\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"Context: {context}\n",
    "\n",
    "    Question: {query}\n",
    "\n",
    "    Answer based on the above context: \"\"\"\n",
    "\n",
    "    # call openai chat completion\n",
    "    response = client.chat.completions.create(\n",
    "        model = model,\n",
    "        messages = [\n",
    "            {\"role\": \"system\" , \"content\": system_prompt},\n",
    "            {\"role\": \"user\" , \"content\": user_prompt}\n",
    "        ],\n",
    "\n",
    "        temperature = 0.6,\n",
    "        max_tokens = 300\n",
    "    )\n",
    "\n",
    "    return{\n",
    "        \"answer\": response.choices[0].message.content,\n",
    "        \"tokens_used\": response.usage.total_tokens,\n",
    "        \"sources\": [doc for doc, _ in context_docs],\n",
    "        \"similarity_scores\":[score for _, score in context_docs]\n",
    "    }\n",
    "\n",
    "# call function to generate answer\n",
    "query = \"What is deep learning and what is it used for?\"\n",
    "relevant_docs = retrieve_relevant_docs(query, knowledge_base, kb_embeddings, top_k=3)\n",
    "result = generate_answer(query, relevant_docs)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(f\"Answer:\\n{result['answer']}\\n\")\n",
    "print(f\"Tokens used: {result['tokens_used']}\")\n",
    "print(f\"\\nSources used (similarity scores):\")\n",
    "for i, (source, score) in enumerate(zip(result['sources'], result['similarity_scores']), 1):\n",
    "    print(f\"{i}. [{score:.3f}] {source[:80]}...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913fb5ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLNotebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
