{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG (Retrieval Augmented Generation) - From Scratch\n",
    "\n",
    "This notebook connects:\n",
    "1. **Data** â†’ Prepare and chunk documents\n",
    "2. **Embeddings** â†’ Convert text to vectors (Day 1)\n",
    "3. **LLMs** â†’ Generate intelligent responses (Day 2)\n",
    "4. **RAG** â†’ Combine them to answer questions using your own data\n",
    "\n",
    "## What is RAG?\n",
    "\n",
    "**Problem:** LLMs don't know about:\n",
    "- Your company's internal documents\n",
    "- Recent events (after training cutoff)\n",
    "- Your personal data\n",
    "\n",
    "**Solution:** RAG = Retrieval + Generation\n",
    "1. **Retrieve** relevant documents using embeddings (semantic search)\n",
    "2. **Augment** the prompt with retrieved context\n",
    "3. **Generate** response using LLM with context\n",
    "\n",
    "## Production Code Practices\n",
    "\n",
    "We'll use:\n",
    "- **Functions** - Reusable, testable code blocks\n",
    "- **Classes** - When we need to maintain state (like a RAG system)\n",
    "- **Type hints** - Makes code more readable and catches errors\n",
    "- **Docstrings** - Explains what functions do\n",
    "\n",
    "**Why?** In real companies, code needs to be maintainable, reusable, and understandable by teams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install openai google-genai anthropic sentence-transformers chromadb python-dotenv numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Environment loaded\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"âœ… Environment loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Understanding RAG Components\n",
    "\n",
    "Let's build RAG step-by-step, starting with simple examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Sample Data (Your Knowledge Base)\n",
    "\n",
    "In production, this could be:\n",
    "- Company documents (PDFs, docs)\n",
    "- Customer support tickets\n",
    "- Product documentation\n",
    "- Database records\n",
    "\n",
    "For learning, we'll use simple text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge base has 8 documents\n",
      "\n",
      "Example document:\n",
      "Machine learning is a subset of artificial intelligence that enables computers to learn from data without explicit programming. It uses algorithms to identify patterns and make predictions.\n"
     ]
    }
   ],
   "source": [
    "# Sample knowledge base about AI/ML topics\n",
    "knowledge_base = [\n",
    "    \"Machine learning is a subset of artificial intelligence that enables computers to learn from data without explicit programming. It uses algorithms to identify patterns and make predictions.\",\n",
    "    \n",
    "    \"Deep learning is a type of machine learning that uses neural networks with multiple layers. It's particularly effective for image recognition, natural language processing, and complex pattern recognition tasks.\",\n",
    "    \n",
    "    \"Natural Language Processing (NLP) is a field of AI that focuses on the interaction between computers and human language. It enables machines to understand, interpret, and generate human language.\",\n",
    "    \n",
    "    \"Embeddings are numerical representations of text that capture semantic meaning. Similar texts have similar embedding vectors, which enables semantic search and similarity comparison.\",\n",
    "    \n",
    "    \"RAG (Retrieval Augmented Generation) combines information retrieval with text generation. It retrieves relevant context from a knowledge base and uses it to generate more accurate and informed responses.\",\n",
    "    \n",
    "    \"OpenAI's GPT models are large language models trained on diverse internet text. They can perform various tasks like text generation, summarization, translation, and question answering.\",\n",
    "    \n",
    "    \"Vector databases store embeddings and enable fast similarity search. Popular options include Chroma, Pinecone, Weaviate, and FAISS. They're essential for production RAG systems.\",\n",
    "    \n",
    "    \"Fine-tuning is the process of adapting a pre-trained model to a specific task by training it on domain-specific data. It's useful when you need specialized behavior beyond what prompting can achieve.\"\n",
    "]\n",
    "\n",
    "print(f\"Knowledge base has {len(knowledge_base)} documents\")\n",
    "print(f\"\\nExample document:\\n{knowledge_base[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Embeddings\n",
    "\n",
    "**Why a function?** We'll need to embed both:\n",
    "- The knowledge base (once, at setup)\n",
    "- User queries (every time they ask a question)\n",
    "\n",
    "Using a function avoids code duplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created embeddings with shape: (8, 384)\n",
      "Each document is represented as a 384-dimensional vector\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def create_embeddings(texts: List[str], model_name: str = \"all-MiniLM-L6-v2\") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert texts to embedding vectors.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of text strings to embed\n",
    "        model_name: Name of the sentence transformer model\n",
    "        \n",
    "    Returns:\n",
    "        NumPy array of embeddings (shape: [num_texts, embedding_dim])\n",
    "        \n",
    "    Why this is useful:\n",
    "        - Reusable for both knowledge base and queries\n",
    "        - Easy to test independently\n",
    "        - Can swap embedding models easily\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embeddings = model.encode(texts)\n",
    "    return embeddings\n",
    "\n",
    "# Create embeddings for our knowledge base\n",
    "kb_embeddings = create_embeddings(knowledge_base)\n",
    "\n",
    "print(f\"Created embeddings with shape: {kb_embeddings.shape}\")\n",
    "print(f\"Each document is represented as a {kb_embeddings.shape[1]}-dimensional vector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Semantic Search (Retrieval)\n",
    "\n",
    "Find the most relevant documents for a query using cosine similarity.\n",
    "\n",
    "**Why a function?** This is the core retrieval logic that we'll use repeatedly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is deep learning?\n",
      "\n",
      "Result 1 (similarity: 0.8377):\n",
      "Deep learning is a type of machine learning that uses neural networks with multiple layers. It's particularly effective for image recognition, natural language processing, and complex pattern recognition tasks.\n",
      "\n",
      "Result 2 (similarity: 0.5488):\n",
      "Machine learning is a subset of artificial intelligence that enables computers to learn from data without explicit programming. It uses algorithms to identify patterns and make predictions.\n",
      "\n",
      "Result 3 (similarity: 0.3874):\n",
      "Natural Language Processing (NLP) is a field of AI that focuses on the interaction between computers and human language. It enables machines to understand, interpret, and generate human language.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def retrieve_relevant_docs(\n",
    "    query: str,\n",
    "    knowledge_base: List[str],\n",
    "    kb_embeddings: np.ndarray,\n",
    "    top_k: int = 3\n",
    ") -> List[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Retrieve the most relevant documents for a query.\n",
    "    \n",
    "    Args:\n",
    "        query: User's question\n",
    "        knowledge_base: List of document texts\n",
    "        kb_embeddings: Pre-computed embeddings of knowledge base\n",
    "        top_k: Number of documents to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        List of (document_text, similarity_score) tuples\n",
    "        \n",
    "    How it works:\n",
    "        1. Embed the query\n",
    "        2. Calculate similarity with all documents\n",
    "        3. Return top-k most similar\n",
    "    \"\"\"\n",
    "    # Embed the query\n",
    "    query_embedding = create_embeddings([query])\n",
    "    \n",
    "    # Calculate similarity scores\n",
    "    similarities = cosine_similarity(query_embedding, kb_embeddings)[0]\n",
    "    \n",
    "    # Get top-k indices\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]  # Sort from lowest to highest: [1, 3, 2, 0, ...] (positions) # [::-1] â†’ Reverse to highest first: \n",
    "    \n",
    "    # Return documents with scores\n",
    "    results = [(knowledge_base[i], similarities[i]) for i in top_indices]\n",
    "    return results\n",
    "\n",
    "# Test retrieval\n",
    "query = \"What is deep learning?\"\n",
    "relevant_docs = retrieve_relevant_docs(query, knowledge_base, kb_embeddings, top_k=3)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "for i, (doc, score) in enumerate(relevant_docs, 1):\n",
    "    print(f\"Result {i} (similarity: {score:.4f}):\")\n",
    "    print(f\"{doc}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Generate Answer with LLM\n",
    "\n",
    "Now we use the retrieved context to generate an informed answer.\n",
    "\n",
    "**Why a function?** Clean separation of concerns - retrieval vs generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "def generate_answer(\n",
    "    query: str,\n",
    "    context_docs: List[Tuple[str, float]],\n",
    "    model: str = \"gpt-4o-mini\"\n",
    ") -> Dict[str, any]:\n",
    "    \"\"\"\n",
    "    Generate an answer using retrieved context.\n",
    "    \n",
    "    Args:\n",
    "        query: User's question\n",
    "        context_docs: Retrieved documents with similarity scores\n",
    "        model: OpenAI model to use\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with answer and metadata\n",
    "        \n",
    "    Production tip:\n",
    "        Return metadata (tokens, sources) for debugging and cost tracking\n",
    "    \"\"\"\n",
    "    client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "    \n",
    "    # Build context from retrieved documents\n",
    "    context = \"\\n\\n\".join([doc for doc, _ in context_docs])\n",
    "    \n",
    "    # Create prompt with context\n",
    "    system_prompt = \"\"\"You are a helpful AI assistant. Answer the user's question based on the provided context. \n",
    "If the context doesn't contain relevant information, say so rather than making up an answer.\"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer based on the context above:\"\"\"\n",
    "    \n",
    "    # Generate response\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        max_tokens=300\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"answer\": response.choices[0].message.content,\n",
    "        \"tokens_used\": response.usage.total_tokens,\n",
    "        \"sources\": [doc for doc, _ in context_docs],\n",
    "        \"similarity_scores\": [score for _, score in context_docs]\n",
    "    }\n",
    "\n",
    "# Test RAG pipeline\n",
    "query = \"What is deep learning and what is it good for?\"\n",
    "relevant_docs = retrieve_relevant_docs(query, knowledge_base, kb_embeddings, top_k=2)\n",
    "result = generate_answer(query, relevant_docs)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(f\"Answer:\\n{result['answer']}\\n\")\n",
    "print(f\"Tokens used: {result['tokens_used']}\")\n",
    "print(f\"\\nSources used (similarity scores):\")\n",
    "for i, (source, score) in enumerate(zip(result['sources'], result['similarity_scores']), 1):\n",
    "    print(f\"{i}. [{score:.3f}] {source[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Production-Ready RAG System (Class-Based)\n",
    "\n",
    "## Why Use a Class?\n",
    "\n",
    "**Problem with functions:** We keep passing the same data around:\n",
    "- `knowledge_base`\n",
    "- `kb_embeddings`\n",
    "- Model configurations\n",
    "\n",
    "**Solution:** A class keeps related data and functions together.\n",
    "\n",
    "**Benefits:**\n",
    "- **State management** - Store knowledge base, embeddings once\n",
    "- **Cleaner code** - No need to pass same parameters repeatedly\n",
    "- **Reusable** - Create multiple RAG systems with different configs\n",
    "- **Production-ready** - Easy to test, maintain, and extend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRAG:\n",
    "    \"\"\"\n",
    "    A production-quality RAG system.\n",
    "    \n",
    "    This class encapsulates the entire RAG pipeline:\n",
    "    - Document storage and embedding\n",
    "    - Semantic search/retrieval\n",
    "    - Answer generation\n",
    "    \n",
    "    Why use a class?\n",
    "    - Maintains state (knowledge base, embeddings)\n",
    "    - Provides a clean API (add_documents, query)\n",
    "    - Easy to configure and reuse\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_model: str = \"all-MiniLM-L6-v2\",\n",
    "        llm_model: str = \"gpt-4o-mini\",\n",
    "        top_k: int = 3\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the RAG system.\n",
    "        \n",
    "        Args:\n",
    "            embedding_model: Sentence transformer model name\n",
    "            llm_model: OpenAI model for generation\n",
    "            top_k: Number of documents to retrieve\n",
    "        \"\"\"\n",
    "        self.embedding_model = SentenceTransformer(embedding_model)\n",
    "        self.llm_model = llm_model\n",
    "        self.top_k = top_k\n",
    "        self.openai_client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "        \n",
    "        # State: stores documents and their embeddings\n",
    "        self.documents: List[str] = []\n",
    "        self.embeddings: np.ndarray = None\n",
    "    \n",
    "    def add_documents(self, documents: List[str]) -> None:\n",
    "        \"\"\"\n",
    "        Add documents to the knowledge base.\n",
    "        \n",
    "        Args:\n",
    "            documents: List of text documents to add\n",
    "            \n",
    "        In production:\n",
    "            - This might load from database/files\n",
    "            - Could handle incremental updates\n",
    "            - Might include document metadata\n",
    "        \"\"\"\n",
    "        self.documents = documents\n",
    "        print(f\"Embedding {len(documents)} documents...\")\n",
    "        self.embeddings = self.embedding_model.encode(documents)\n",
    "        print(f\"âœ… {len(documents)} documents indexed\")\n",
    "    \n",
    "    def retrieve(self, query: str) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Retrieve most relevant documents for a query.\n",
    "        \n",
    "        Args:\n",
    "            query: User's question\n",
    "            \n",
    "        Returns:\n",
    "            List of (document, similarity_score) tuples\n",
    "        \"\"\"\n",
    "        if self.embeddings is None:\n",
    "            raise ValueError(\"No documents added. Call add_documents() first.\")\n",
    "        \n",
    "        # Embed query\n",
    "        query_embedding = self.embedding_model.encode([query])\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = cosine_similarity(query_embedding, self.embeddings)[0]\n",
    "        \n",
    "        # Get top-k\n",
    "        top_indices = np.argsort(similarities)[::-1][:self.top_k]\n",
    "        \n",
    "        return [(self.documents[i], similarities[i]) for i in top_indices]\n",
    "    \n",
    "    def generate(self, query: str, context_docs: List[Tuple[str, float]]) -> Dict:\n",
    "        \"\"\"\n",
    "        Generate answer using retrieved context.\n",
    "        \n",
    "        Args:\n",
    "            query: User's question\n",
    "            context_docs: Retrieved documents with scores\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with answer and metadata\n",
    "        \"\"\"\n",
    "        # Build context\n",
    "        context = \"\\n\\n\".join([doc for doc, _ in context_docs])\n",
    "        \n",
    "        # Create prompt\n",
    "        system_prompt = \"\"\"You are a helpful AI assistant. Answer questions based on the provided context.\n",
    "If the context doesn't contain enough information, acknowledge this limitation.\"\"\"\n",
    "        \n",
    "        user_prompt = f\"\"\"Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        # Generate\n",
    "        response = self.openai_client.chat.completions.create(\n",
    "            model=self.llm_model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=300\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"answer\": response.choices[0].message.content,\n",
    "            \"tokens\": response.usage.total_tokens,\n",
    "            \"sources\": context_docs\n",
    "        }\n",
    "    \n",
    "    def query(self, question: str, return_sources: bool = True) -> Dict:\n",
    "        \"\"\"\n",
    "        Main method: Query the RAG system.\n",
    "        \n",
    "        This is the public API - simple to use!\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            return_sources: Whether to include source documents\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with answer and optional metadata\n",
    "        \"\"\"\n",
    "        # Step 1: Retrieve\n",
    "        relevant_docs = self.retrieve(question)\n",
    "        \n",
    "        # Step 2: Generate\n",
    "        result = self.generate(question, relevant_docs)\n",
    "        \n",
    "        # Format response\n",
    "        response = {\n",
    "            \"question\": question,\n",
    "            \"answer\": result[\"answer\"],\n",
    "            \"tokens_used\": result[\"tokens\"]\n",
    "        }\n",
    "        \n",
    "        if return_sources:\n",
    "            response[\"sources\"] = [\n",
    "                {\"text\": doc, \"similarity\": float(score)}\n",
    "                for doc, score in result[\"sources\"]\n",
    "            ]\n",
    "        \n",
    "        return response\n",
    "\n",
    "print(\"âœ… SimpleRAG class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the RAG System\n",
    "\n",
    "See how clean the API is now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RAG system\n",
    "rag = SimpleRAG(\n",
    "    embedding_model=\"all-MiniLM-L6-v2\",\n",
    "    llm_model=\"gpt-4o-mini\",\n",
    "    top_k=3\n",
    ")\n",
    "\n",
    "# Add knowledge base\n",
    "rag.add_documents(knowledge_base)\n",
    "\n",
    "# Query the system\n",
    "result = rag.query(\"What is RAG and why is it useful?\")\n",
    "\n",
    "print(f\"Question: {result['question']}\\n\")\n",
    "print(f\"Answer:\\n{result['answer']}\\n\")\n",
    "print(f\"Tokens used: {result['tokens_used']}\\n\")\n",
    "print(\"Sources:\")\n",
    "for i, source in enumerate(result['sources'], 1):\n",
    "    print(f\"{i}. [Similarity: {source['similarity']:.3f}]\")\n",
    "    print(f\"   {source['text'][:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Multiple Queries Demo\n",
    "\n",
    "Let's test different types of questions to see how RAG performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"What is the difference between machine learning and deep learning?\",\n",
    "    \"What are embeddings used for?\",\n",
    "    \"Which vector databases are mentioned?\",\n",
    "    \"What is fine-tuning?\"\n",
    "]\n",
    "\n",
    "for i, question in enumerate(questions, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Question {i}: {question}\")\n",
    "    print('='*80)\n",
    "    \n",
    "    result = rag.query(question, return_sources=False)\n",
    "    print(f\"\\nAnswer:\\n{result['answer']}\")\n",
    "    print(f\"\\nTokens: {result['tokens_used']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: RAG with Different LLM Providers\n",
    "\n",
    "Let's extend our class to support multiple LLM providers (OpenAI, Gemini, Claude)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "from anthropic import Anthropic\n",
    "\n",
    "class MultiProviderRAG(SimpleRAG):\n",
    "    \"\"\"\n",
    "    Extended RAG system supporting multiple LLM providers.\n",
    "    \n",
    "    Why extend the class?\n",
    "    - Reuses all retrieval logic from SimpleRAG\n",
    "    - Only adds multi-provider support\n",
    "    - Demonstrates Object-Oriented Programming (inheritance)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        provider: str = \"openai\",  # \"openai\", \"gemini\", or \"claude\"\n",
    "        embedding_model: str = \"all-MiniLM-L6-v2\",\n",
    "        llm_model: str = None,\n",
    "        top_k: int = 3\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize multi-provider RAG.\n",
    "        \n",
    "        Args:\n",
    "            provider: Which LLM provider to use\n",
    "            embedding_model: Sentence transformer model\n",
    "            llm_model: Provider-specific model (auto-set if None)\n",
    "            top_k: Number of documents to retrieve\n",
    "        \"\"\"\n",
    "        # Set default models per provider\n",
    "        default_models = {\n",
    "            \"openai\": \"gpt-4o-mini\",\n",
    "            \"gemini\": \"gemini-2.5-flash\",\n",
    "            \"claude\": \"claude-3-5-haiku-20241022\"\n",
    "        }\n",
    "        \n",
    "        self.provider = provider\n",
    "        llm_model = llm_model or default_models[provider]\n",
    "        \n",
    "        # Initialize parent class\n",
    "        super().__init__(embedding_model, llm_model, top_k)\n",
    "        \n",
    "        # Initialize provider-specific clients\n",
    "        if provider == \"gemini\":\n",
    "            self.gemini_client = genai.Client(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "        elif provider == \"claude\":\n",
    "            self.claude_client = Anthropic(api_key=os.environ[\"ANTHROPIC_API_KEY\"])\n",
    "    \n",
    "    def generate(self, query: str, context_docs: List[Tuple[str, float]]) -> Dict:\n",
    "        \"\"\"\n",
    "        Generate answer using the configured provider.\n",
    "        \n",
    "        This overrides the parent method to support multiple providers.\n",
    "        \"\"\"\n",
    "        context = \"\\n\\n\".join([doc for doc, _ in context_docs])\n",
    "        \n",
    "        if self.provider == \"openai\":\n",
    "            return self._generate_openai(query, context)\n",
    "        elif self.provider == \"gemini\":\n",
    "            return self._generate_gemini(query, context)\n",
    "        elif self.provider == \"claude\":\n",
    "            return self._generate_claude(query, context)\n",
    "    \n",
    "    def _generate_openai(self, query: str, context: str) -> Dict:\n",
    "        \"\"\"Generate using OpenAI.\"\"\"\n",
    "        response = self.openai_client.chat.completions.create(\n",
    "            model=self.llm_model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Answer based on the provided context.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"}\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=300\n",
    "        )\n",
    "        return {\n",
    "            \"answer\": response.choices[0].message.content,\n",
    "            \"tokens\": response.usage.total_tokens,\n",
    "            \"sources\": context\n",
    "        }\n",
    "    \n",
    "    def _generate_gemini(self, query: str, context: str) -> Dict:\n",
    "        \"\"\"Generate using Gemini.\"\"\"\n",
    "        prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer based on the context:\"\n",
    "        \n",
    "        response = self.gemini_client.models.generate_content(\n",
    "            model=self.llm_model,\n",
    "            contents=prompt,\n",
    "            config=types.GenerateContentConfig(\n",
    "                temperature=0.7,\n",
    "                max_output_tokens=300\n",
    "            )\n",
    "        )\n",
    "        return {\n",
    "            \"answer\": response.text,\n",
    "            \"tokens\": response.usage_metadata.total_token_count,\n",
    "            \"sources\": context\n",
    "        }\n",
    "    \n",
    "    def _generate_claude(self, query: str, context: str) -> Dict:\n",
    "        \"\"\"Generate using Claude.\"\"\"\n",
    "        prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer based on the context above:\"\n",
    "        \n",
    "        response = self.claude_client.messages.create(\n",
    "            model=self.llm_model,\n",
    "            max_tokens=300,\n",
    "            temperature=0.7,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        return {\n",
    "            \"answer\": response.content[0].text,\n",
    "            \"tokens\": response.usage.input_tokens + response.usage.output_tokens,\n",
    "            \"sources\": context\n",
    "        }\n",
    "\n",
    "print(\"âœ… MultiProviderRAG class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Different LLM Providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test question\n",
    "test_question = \"What is deep learning and what are its applications?\"\n",
    "\n",
    "providers = [\"openai\", \"gemini\", \"claude\"]\n",
    "\n",
    "for provider in providers:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Testing with {provider.upper()}\")\n",
    "    print('='*80)\n",
    "    \n",
    "    try:\n",
    "        # Create RAG system for this provider\n",
    "        rag = MultiProviderRAG(provider=provider, top_k=2)\n",
    "        rag.add_documents(knowledge_base)\n",
    "        \n",
    "        # Query\n",
    "        result = rag.query(test_question, return_sources=False)\n",
    "        \n",
    "        print(f\"\\nAnswer:\\n{result['answer']}\")\n",
    "        print(f\"\\nTokens used: {result['tokens_used']}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error with {provider}: {e}\")\n",
    "        print(\"Make sure you have the API key set in your .env file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: RAG with Vector Database (ChromaDB)\n",
    "\n",
    "## Why Vector Databases?\n",
    "\n",
    "Our in-memory approach works for small datasets, but in production:\n",
    "- **Scalability**: Millions of documents\n",
    "- **Persistence**: Data survives restarts\n",
    "- **Speed**: Optimized similarity search\n",
    "- **Features**: Filtering, metadata, updates\n",
    "\n",
    "ChromaDB is perfect for learning - simple API, runs locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "class ProductionRAG:\n",
    "    \"\"\"\n",
    "    Production-ready RAG using ChromaDB.\n",
    "    \n",
    "    Why this is better for production:\n",
    "    - Persistent storage\n",
    "    - Handles large datasets efficiently\n",
    "    - Built-in embedding generation\n",
    "    - Metadata filtering\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        collection_name: str = \"knowledge_base\",\n",
    "        llm_provider: str = \"openai\",\n",
    "        llm_model: str = \"gpt-4o-mini\",\n",
    "        top_k: int = 3\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize production RAG system.\n",
    "        \n",
    "        Args:\n",
    "            collection_name: Name for the vector collection\n",
    "            llm_provider: LLM provider (openai, gemini, claude)\n",
    "            llm_model: Model name\n",
    "            top_k: Number of results to retrieve\n",
    "        \"\"\"\n",
    "        # Initialize ChromaDB\n",
    "        self.client = chromadb.Client()\n",
    "        \n",
    "        # Use sentence transformers for embeddings\n",
    "        self.embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "            model_name=\"all-MiniLM-L6-v2\"\n",
    "        )\n",
    "        \n",
    "        # Create or get collection\n",
    "        self.collection = self.client.get_or_create_collection(\n",
    "            name=collection_name,\n",
    "            embedding_function=self.embedding_function\n",
    "        )\n",
    "        \n",
    "        self.llm_provider = llm_provider\n",
    "        self.llm_model = llm_model\n",
    "        self.top_k = top_k\n",
    "        \n",
    "        # Initialize LLM client\n",
    "        self.openai_client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "    \n",
    "    def add_documents(\n",
    "        self,\n",
    "        documents: List[str],\n",
    "        metadatas: List[Dict] = None,\n",
    "        ids: List[str] = None\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Add documents to the vector database.\n",
    "        \n",
    "        Args:\n",
    "            documents: List of text documents\n",
    "            metadatas: Optional metadata for each document\n",
    "            ids: Optional IDs (auto-generated if not provided)\n",
    "        \"\"\"\n",
    "        if ids is None:\n",
    "            ids = [f\"doc_{i}\" for i in range(len(documents))]\n",
    "        \n",
    "        if metadatas is None:\n",
    "            metadatas = [{\"source\": \"knowledge_base\"} for _ in documents]\n",
    "        \n",
    "        self.collection.add(\n",
    "            documents=documents,\n",
    "            metadatas=metadatas,\n",
    "            ids=ids\n",
    "        )\n",
    "        print(f\"âœ… Added {len(documents)} documents to ChromaDB\")\n",
    "    \n",
    "    def query(self, question: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Query the RAG system.\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with answer and metadata\n",
    "        \"\"\"\n",
    "        # Retrieve from ChromaDB\n",
    "        results = self.collection.query(\n",
    "            query_texts=[question],\n",
    "            n_results=self.top_k\n",
    "        )\n",
    "        \n",
    "        # Extract documents and distances\n",
    "        documents = results['documents'][0]\n",
    "        distances = results['distances'][0]\n",
    "        \n",
    "        # Build context\n",
    "        context = \"\\n\\n\".join(documents)\n",
    "        \n",
    "        # Generate answer\n",
    "        prompt = f\"\"\"Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer based on the context above:\"\"\"\n",
    "        \n",
    "        response = self.openai_client.chat.completions.create(\n",
    "            model=self.llm_model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Answer based on the provided context.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=300\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": response.choices[0].message.content,\n",
    "            \"tokens_used\": response.usage.total_tokens,\n",
    "            \"sources\": [\n",
    "                {\"text\": doc, \"distance\": dist}\n",
    "                for doc, dist in zip(documents, distances)\n",
    "            ]\n",
    "        }\n",
    "\n",
    "print(\"âœ… ProductionRAG class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create production RAG system\n",
    "prod_rag = ProductionRAG(\n",
    "    collection_name=\"ai_knowledge\",\n",
    "    llm_model=\"gpt-4o-mini\",\n",
    "    top_k=3\n",
    ")\n",
    "\n",
    "# Add documents with metadata\n",
    "metadatas = [{\"topic\": \"AI\", \"index\": i} for i in range(len(knowledge_base))]\n",
    "prod_rag.add_documents(knowledge_base, metadatas=metadatas)\n",
    "\n",
    "# Query\n",
    "result = prod_rag.query(\"What are vector databases and why are they important for RAG?\")\n",
    "\n",
    "print(f\"Question: {result['question']}\\n\")\n",
    "print(f\"Answer:\\n{result['answer']}\\n\")\n",
    "print(f\"Tokens used: {result['tokens_used']}\\n\")\n",
    "print(\"Sources (lower distance = more relevant):\")\n",
    "for i, source in enumerate(result['sources'], 1):\n",
    "    print(f\"{i}. [Distance: {source['distance']:.3f}]\")\n",
    "    print(f\"   {source['text'][:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary: Your Complete RAG Journey\n",
    "\n",
    "## What You Learned\n",
    "\n",
    "### 1. **The RAG Pipeline**\n",
    "```\n",
    "Data â†’ Embeddings â†’ Vector Store â†’ Retrieval â†’ LLM â†’ Answer\n",
    "```\n",
    "\n",
    "### 2. **Production Code Practices**\n",
    "\n",
    "**Functions:**\n",
    "- âœ… Reusable code blocks\n",
    "- âœ… Easy to test\n",
    "- âœ… Clear inputs/outputs\n",
    "\n",
    "**Classes:**\n",
    "- âœ… Manage state (knowledge base, embeddings)\n",
    "- âœ… Clean API (`rag.query()`)\n",
    "- âœ… Extensible (inheritance for multi-provider)\n",
    "\n",
    "**Type Hints & Docstrings:**\n",
    "- âœ… Self-documenting code\n",
    "- âœ… Catches errors early\n",
    "- âœ… Better IDE support\n",
    "\n",
    "### 3. **From Simple to Production**\n",
    "\n",
    "| Approach | Best For | Limitations |\n",
    "|----------|----------|-------------|\n",
    "| **Functions** | Learning, prototypes | Passing data repeatedly |\n",
    "| **SimpleRAG Class** | Small projects | In-memory only |\n",
    "| **MultiProviderRAG** | Flexibility | Still in-memory |\n",
    "| **ProductionRAG** | Real applications | Requires vector DB |\n",
    "\n",
    "### 4. **Key Concepts**\n",
    "\n",
    "- **Semantic Search**: Find relevant info using meaning, not keywords\n",
    "- **Context Window**: How much text the LLM can process\n",
    "- **Top-K Retrieval**: Get K most relevant documents\n",
    "- **Cosine Similarity**: Measure how similar two vectors are (0-1)\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. âœ… **Practice with your own data**\n",
    "   - PDFs, documentation, notes\n",
    "   - Use `PyPDF2` or `langchain` loaders\n",
    "\n",
    "2. âœ… **Advanced RAG Techniques**\n",
    "   - Chunking strategies\n",
    "   - Hybrid search (keyword + semantic)\n",
    "   - Re-ranking retrieved documents\n",
    "   - Parent-child chunking\n",
    "\n",
    "3. âœ… **Production Enhancements**\n",
    "   - Error handling\n",
    "   - Logging and monitoring\n",
    "   - Caching for repeated queries\n",
    "   - Rate limiting\n",
    "\n",
    "4. âœ… **Learn Advanced Tools**\n",
    "   - LangChain (RAG framework)\n",
    "   - LlamaIndex (data framework)\n",
    "   - Vector databases (Pinecone, Weaviate)\n",
    "\n",
    "## Your AI Engineering Path\n",
    "\n",
    "```\n",
    "âœ… Day 1: Embeddings\n",
    "âœ… Day 2: LLM APIs  \n",
    "âœ… Day 3: RAG\n",
    "ðŸ”œ Advanced Prompting\n",
    "ðŸ”œ Agents & Function Calling\n",
    "ðŸ”œ Fine-tuning\n",
    "ðŸ”œ Production Deployment\n",
    "```\n",
    "\n",
    "You're building real AI engineering skills! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLNotebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
